<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>notconfusing.com - hacking</title><link href="https://notconfusing.com/" rel="alternate"></link><link href="https://notconfusing.com/feeds/hacking.atom.xml" rel="self"></link><id>https://notconfusing.com/</id><updated>2016-11-15T21:17:00-08:00</updated><subtitle>The world might be complex, but it's not confusing</subtitle><entry><title>So you want to upload an image to the cloud with Node.js</title><link href="https://notconfusing.com/so-you-want-to-upload-an-image-to-the-cloud-with-node-js.html" rel="alternate"></link><published>2016-11-15T21:17:00-08:00</published><updated>2016-11-15T21:17:00-08:00</updated><author><name>max</name></author><id>tag:notconfusing.com,2016-11-15:/so-you-want-to-upload-an-image-to-the-cloud-with-node-js.html</id><summary type="html">&lt;h2&gt;So you want to upload an image to the cloud with Node.js?&lt;/h2&gt;
&lt;p&gt;Maybe you want a small raspberry pi webcam to take timelapse footage and send it to a server every hour because of its small harddrive. Maybe you want to build a social network swapping images of Lizard …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;So you want to upload an image to the cloud with Node.js?&lt;/h2&gt;
&lt;p&gt;Maybe you want a small raspberry pi webcam to take timelapse footage and send it to a server every hour because of its small harddrive. Maybe you want to build a social network swapping images of Lizard People, and your sever can t handle all the image traffic. Maybe you want to back-up your irreplaceable collection of dead-sea scroll fragments -- it's irreplaceable. You  might want to keep around images or files for many different reasons, and having them publicly accessible in the cloud is better than trying to manage them yourself, for storage and network reasons. This is a tutorial about how to do that. In this tutorial we will be building and inspecting a use case for a web-app I've been working on in which users can post images of items they want to trade.&lt;/p&gt;
&lt;p&gt;Just focusing on the backend, we'll be assuming there's already webpage with a file upload user-interface. What we'll be doing is creating the API endpoint that images can be uploaded to, but will then turn-around and ask a cloud-strorage provider to hold the image for us. We'll also make a square thumbnail version of the image and upload those too. Once both of these images (the full-size and the thumbnail) are in the cloud, we'll return to the web-client a URL of where the images can be found in the cloud.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Architectural Overview" class="aligncenter size-full wp-image-4110" src="https://notconfusing.com/images/uploads/2016/11/Untitled-drawing1.png" style="width:958px !important"&gt;&lt;/p&gt;
&lt;h2&gt;Choices, choices&lt;/h2&gt;
&lt;p&gt;Let's take a moment to justify the technologies we are using. The first is node.js. There's a Ph.D. student who works at my university researching programming languages who starts all of his presentations with the question: "why on earth does anyone write in node.js?" It's funny the first few times you hear it, and if you're like me who fell in love with python, you might be reticent. But there is some justification here, although some of our processing needs to happen synchronously, but a lot doesn't, so nodeJS's asynchronous-by-default nature will be useful.&lt;/p&gt;
&lt;p&gt;The next question we have to answer is, which cloud storage provider to use. I investigated a couple of candidates, and eventually settled on using Microsoft's Azure, because it came with some excellent sample applications and code that made it easy to get off the ground in an hour. It's not the only one out there though, I also thought about Amazon S3 the popular giant - but its popular enough already. I looked at Google Cloud Services, which does come with node.js wrappers which is nice, but I didn't see any sample applications so it wasn't as dev-friendly as it could have been. I also wanted to consider companies that weren't corporations of sizes I can't even fathom, so I poked around Rackspace's promising offerings, but their documentation was written entirely in cURL exmaples, and we're trying to play this game on easy mode. Lastly, Cloudinary seems like a good service which is more purpose-build for images, not just general storage, and so has a built-in thumbnail API. I wish my colleague had told me about Cloudinary before I'd written this tutorial not after, it may have come out differently.&lt;/p&gt;
&lt;p&gt;One last thing to note is how much these services cost, what I noticed is that their pricing structures were more similar than different. \$0.003 per GB of traffic matters to Netflix, not to us for these purposes.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Amazon S3               The popular leader in cloud storage. Yawn.
  Google Cloud Services   Node.js library, no example application
  Rackspace               Thoughtful company, documentation is cURL only, no node.js library.
  Azure                    Node.js library and example applications.
  Cloudinary               Potentially interesting entrant, which image-focused features.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Set up&lt;/h2&gt;
&lt;p&gt;For this toturial you'll need a Microsoft Azure account. They'll give you \$200 free credit to start off, so go sign-up at &lt;a href="http://portal.azure.com"&gt;http://portal.azure.com/&lt;/a&gt; without entering a credit card, and setup your "Blob" storage.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Make a new Storage Account" class="size-full wp-image-4112" src="https://notconfusing.com/images/uploads/2016/11/azure1.png" style="width:726px !important"&gt; Make a new Storage Account&lt;/p&gt;
&lt;p&gt;Remember these credentials for your application. When everything is set up you'll see your container like this:&lt;/p&gt;
&lt;p&gt;&lt;img alt="There can be many containers, which contain blobs. Each blob has a deterministic URL. Note, the Key icon which contains your secret key needed for later on." class="size-large wp-image-4113" src="https://notconfusing.com/images/uploads/2016/11/azure3.png" style="width:474px !important"&gt; There can be many containers, which contain blobs. Each blob has a deterministic URL. Note, the Key icon which contains your secret key needed for later on.&lt;/p&gt;
&lt;p&gt;Now we'll move on to getting our node.js enviorment set up. With the magic of &lt;code&gt;npm&lt;/code&gt; this is remarkably easy. In your project folder do&lt;/p&gt;
&lt;p&gt;``` {.EnlighterJSRAW enlighter-language="shell"}
npm install azure-storage --save&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;As&lt;/span&gt; &lt;span class="n"&gt;I&lt;/span&gt; &lt;span class="n"&gt;mentioned&lt;/span&gt; &lt;span class="n"&gt;Azure&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="n"&gt;storage&lt;/span&gt; &lt;span class="n"&gt;solution&lt;/span&gt; &lt;span class="n"&gt;not&lt;/span&gt; &lt;span class="n"&gt;only&lt;/span&gt; &lt;span class="n"&gt;has&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;js&lt;/span&gt; &lt;span class="n"&gt;library&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;but&lt;/span&gt; &lt;span class="n"&gt;also&lt;/span&gt; &lt;span class="n"&gt;comes&lt;/span&gt; &lt;span class="n"&gt;with&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;nice&lt;/span&gt; &lt;span class="n"&gt;simple&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="n"&gt;application&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="nl"&gt;https&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="c1"&gt;//github.com/Azure/azure-sdk-for-node/blob/c98fd8ff9310f688a7ff6a227015876cd1f722ab/examples/ASM/samples/continuationsample.js) which I&amp;#39;ll embellish and go over in more detail.&lt;/span&gt;

&lt;span class="n"&gt;Breaking&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;Code&lt;/span&gt;
&lt;span class="o"&gt;-----------------&lt;/span&gt;

&lt;span class="n"&gt;We&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;re&lt;/span&gt; &lt;span class="n"&gt;ready&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;look&lt;/span&gt; &lt;span class="n"&gt;at&lt;/span&gt; &lt;span class="n"&gt;some&lt;/span&gt; &lt;span class="n"&gt;code&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;What&lt;/span&gt; &lt;span class="n"&gt;you&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;re&lt;/span&gt; &lt;span class="n"&gt;about&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;see&lt;/span&gt; &lt;span class="n"&gt;is&lt;/span&gt; &lt;span class="n"&gt;inside&lt;/span&gt; &lt;span class="n"&gt;an&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;express&lt;/span&gt; &lt;span class="n"&gt;application&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="nl"&gt;http&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="c1"&gt;//expressjs.com/) which I won&amp;#39;t cover now. If you&amp;#39;re not familiar with express it&amp;#39;s quite easy to pick up, but for now pay attention to the broad strokes.&lt;/span&gt;

&lt;span class="err"&gt;```&lt;/span&gt; &lt;span class="p"&gt;{.&lt;/span&gt;&lt;span class="n"&gt;EnlighterJSRAW&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="n"&gt;var&lt;/span&gt; &lt;span class="n"&gt;azure&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;require&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;azure&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;storage&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;var&lt;/span&gt; &lt;span class="n"&gt;formidable&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;require&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;formidable&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="c1"&gt;//for parsing data that comes with our images&lt;/span&gt;
&lt;span class="n"&gt;var&lt;/span&gt; &lt;span class="n"&gt;lwip&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;require&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;lwip&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="c1"&gt;//to make our thumbnails&lt;/span&gt;
&lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;storageAccount&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;youenteredthisatazureportal&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;
&lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;storageAccessKey&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;alongrandomstringyougetfromazure&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;
&lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;azureEndpoint&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="nl"&gt;https&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="c1"&gt;//OURDOMAIN.blob.core.windows.net/&amp;#39;&lt;/span&gt;
&lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;containerName&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;youenteredthisatazureportal&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="n"&gt;var&lt;/span&gt; &lt;span class="n"&gt;blobClient&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;azure&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;createBlobService&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;storageAccount&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;storageAccessKey&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We now have the the blobClient which is an Object that has a method which will accept our image and upload it, keep this in mind. Next we'll plumb in our API to respond to HTTP POST requests at &lt;code&gt;/uploadhandler&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;``` {.EnlighterJSRAW enlighter-language="null"}
router.post('/uploadhandler', function (req, res) {
  var form = new formidable.IncomingForm();
  var itemId = new Date().getTime()
  form.parse(req, function (err, fields, files) {
      var options = {
        contentType: 'image/jpeg',
        metadata: {fileName: itemId}
        }
      var imgPath = files.image.path
      var machineId = fields.machineId
      var userName = fields.userName
      makeThumb(imgPath, itemId, machineId, userName, options, res, uploadThumb);
      uploadFull(imgPath, itemId, options)
  });
});&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;In&lt;/span&gt; &lt;span class="n"&gt;this&lt;/span&gt; &lt;span class="n"&gt;example&lt;/span&gt; &lt;span class="n"&gt;we&lt;/span&gt; &lt;span class="n"&gt;are&lt;/span&gt; &lt;span class="n"&gt;expecting&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="ss"&gt;`form`&lt;/span&gt;  &lt;span class="n"&gt;which&lt;/span&gt; &lt;span class="n"&gt;contains&lt;/span&gt; &lt;span class="n"&gt;our&lt;/span&gt; &lt;span class="n"&gt;image&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="ss"&gt;`files.image`&lt;/span&gt; &lt;span class="n"&gt;attribute&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;and&lt;/span&gt; &lt;span class="n"&gt;some&lt;/span&gt; &lt;span class="n"&gt;other&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="k"&gt;like&lt;/span&gt; &lt;span class="ss"&gt;`machineId`&lt;/span&gt; &lt;span class="k"&gt;and&lt;/span&gt; &lt;span class="ss"&gt;`userName`&lt;/span&gt; &lt;span class="n"&gt;which&lt;/span&gt; &lt;span class="n"&gt;came&lt;/span&gt; &lt;span class="n"&gt;along&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;ride&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="n"&gt;fields&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;form&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;After&lt;/span&gt; &lt;span class="n"&gt;that&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;s all parsed, we are going to make a thumb, and we&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;re&lt;/span&gt; &lt;span class="n"&gt;sending&lt;/span&gt; &lt;span class="n"&gt;it&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;callback&lt;/span&gt; &lt;span class="ss"&gt;`uploadThumb`&lt;/span&gt; &lt;span class="k"&gt;to&lt;/span&gt; &lt;span class="n"&gt;run&lt;/span&gt; &lt;span class="n"&gt;afterwards&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="ss"&gt;`uploadFull`&lt;/span&gt; &lt;span class="n"&gt;doesn&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;t need any preprocessing so we can send it on it&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="n"&gt;merry&lt;/span&gt; &lt;span class="n"&gt;way&lt;/span&gt; &lt;span class="k"&gt;right&lt;/span&gt; &lt;span class="n"&gt;now&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;giving&lt;/span&gt; &lt;span class="n"&gt;it&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;image&lt;/span&gt; &lt;span class="n"&gt;path&lt;/span&gt; &lt;span class="k"&gt;and&lt;/span&gt; &lt;span class="n"&gt;an&lt;/span&gt; &lt;span class="ss"&gt;`itemId`&lt;/span&gt; &lt;span class="n"&gt;which&lt;/span&gt; &lt;span class="n"&gt;basically&lt;/span&gt; &lt;span class="n"&gt;serves&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="k"&gt;unique&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;enough&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;now&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;our&lt;/span&gt; &lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;We&lt;/span&gt; &lt;span class="n"&gt;also&lt;/span&gt; &lt;span class="n"&gt;are&lt;/span&gt; &lt;span class="n"&gt;giving&lt;/span&gt; &lt;span class="n"&gt;Azure&lt;/span&gt; &lt;span class="n"&gt;some&lt;/span&gt; &lt;span class="n"&gt;clues&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="k"&gt;to&lt;/span&gt; &lt;span class="n"&gt;what&lt;/span&gt; &lt;span class="n"&gt;this&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="k"&gt;is&lt;/span&gt; &lt;span class="n"&gt;that&lt;/span&gt; &lt;span class="n"&gt;we&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;re sending it in the  `options` object. For simplicity we&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;re&lt;/span&gt; &lt;span class="n"&gt;assuming&lt;/span&gt; &lt;span class="n"&gt;its&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;jpeg&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;

&lt;span class="n"&gt;I&lt;/span&gt; &lt;span class="n"&gt;bet&lt;/span&gt; &lt;span class="n"&gt;you&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;re wondering what those upload functions do?&lt;/span&gt;

&lt;span class="s1"&gt;``` {.EnlighterJSRAW enlighter-language=&amp;quot;null&amp;quot;}&lt;/span&gt;
&lt;span class="s1"&gt;var uploadFull = function(imgPath, itemId, options){&lt;/span&gt;
&lt;span class="s1"&gt;  var fileName = itemId + &amp;#39;&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jpg&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;
&lt;span class="s1"&gt;  blobClient.createBlockBlobFromLocalFile(containerName, fileName, imgPath, options, function (error) {&lt;/span&gt;
&lt;span class="s1"&gt;    if (error != null) {&lt;/span&gt;
&lt;span class="s1"&gt;      console.log(&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;Azure&lt;/span&gt; &lt;span class="n"&gt;Full&lt;/span&gt; &lt;span class="n"&gt;Error&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;, error)&lt;/span&gt;
&lt;span class="s1"&gt;    } else {&lt;/span&gt;
&lt;span class="s1"&gt;      console.log(&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;Azure&lt;/span&gt; &lt;span class="n"&gt;Full&lt;/span&gt; &lt;span class="n"&gt;Success&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="err"&gt;}&lt;/span&gt;
  &lt;span class="err"&gt;}&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="err"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;code&gt;uploadFull&lt;/code&gt; is going take our image file and plug it into the blobClient's &lt;code&gt;createBlockBlobFromLocalFile&lt;/code&gt; method. It takes a callback to know how to handle errors. Actually this a weak point of Azure that I'll cover later, about why what arguments passed to this callback are insufficient. But lets take a moment here some magic happened. blobClient is abstracting away a lot of HTTP-based complexity for us--thanks Azure SDK.&lt;/p&gt;
&lt;p&gt;Now let's see how to make make a square thumbnail out of our image, and at the same time uncover a gripe I have about azure's SDK.&lt;/p&gt;
&lt;p&gt;``` {.EnlighterJSRAW enlighter-language="null"}
var makeThumb = function(imgPath, itemId, machineId, userName, options, res, callback){
  lwip.open(imgPath, 'jpg', function(err, image){
    image.cover(320, 320, function(err, image){
      image.toBuffer('jpg', function(err, buffer){
        callback(buffer, itemId, machineId, userName, options, res)
      });
    });
  });
};&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;We&lt;/span&gt; &lt;span class="nt"&gt;use&lt;/span&gt; &lt;span class="nt"&gt;the&lt;/span&gt; &lt;span class="nt"&gt;aptly&lt;/span&gt; &lt;span class="nt"&gt;named&lt;/span&gt; &lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;LightWeight Image Processing&amp;quot;&lt;/span&gt; &lt;span class="nb"&gt;library&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;https&lt;/span&gt;&lt;span class="o"&gt;://&lt;/span&gt;&lt;span class="nt"&gt;github&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;com&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;EyalAr&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;lwip&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;to&lt;/span&gt; &lt;span class="nt"&gt;make&lt;/span&gt; &lt;span class="nt"&gt;a&lt;/span&gt; &lt;span class="nt"&gt;320x320&lt;/span&gt; &lt;span class="nt"&gt;pixel&lt;/span&gt; &lt;span class="nt"&gt;thumbnail&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="nt"&gt;There&lt;/span&gt; &lt;span class="nt"&gt;is&lt;/span&gt; &lt;span class="nt"&gt;actually&lt;/span&gt; &lt;span class="nt"&gt;a&lt;/span&gt; &lt;span class="nt"&gt;single&lt;/span&gt; &lt;span class="nt"&gt;call&lt;/span&gt; &lt;span class="nt"&gt;that&lt;/span&gt; &lt;span class="nt"&gt;will&lt;/span&gt; &lt;span class="nt"&gt;do&lt;/span&gt; &lt;span class="nt"&gt;this&lt;/span&gt; &lt;span class="nt"&gt;in&lt;/span&gt; &lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="nt"&gt;image&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;cover&lt;/span&gt;&lt;span class="err"&gt;`&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="nt"&gt;But&lt;/span&gt; &lt;span class="nt"&gt;notice&lt;/span&gt; &lt;span class="nt"&gt;that&lt;/span&gt; &lt;span class="nt"&gt;we&lt;/span&gt; &lt;span class="nt"&gt;passed&lt;/span&gt; &lt;span class="nt"&gt;in&lt;/span&gt; &lt;span class="nt"&gt;a&lt;/span&gt; &lt;span class="nt"&gt;string&lt;/span&gt; &lt;span class="nt"&gt;representing&lt;/span&gt; &lt;span class="nt"&gt;a&lt;/span&gt; &lt;span class="nt"&gt;path&lt;/span&gt; &lt;span class="nt"&gt;and&lt;/span&gt; &lt;span class="nt"&gt;what&lt;/span&gt; &lt;span class="nt"&gt;we&lt;/span&gt; &lt;span class="nt"&gt;got&lt;/span&gt; &lt;span class="nt"&gt;back&lt;/span&gt; &lt;span class="nt"&gt;was&lt;/span&gt; &lt;span class="nt"&gt;a&lt;/span&gt; &lt;span class="nt"&gt;buffer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="nt"&gt;We&lt;/span&gt; &lt;span class="nt"&gt;could&lt;/span&gt; &lt;span class="nt"&gt;have&lt;/span&gt; &lt;span class="nt"&gt;saved&lt;/span&gt; &lt;span class="nt"&gt;the&lt;/span&gt; &lt;span class="nt"&gt;image&lt;/span&gt; &lt;span class="nt"&gt;as&lt;/span&gt; &lt;span class="nt"&gt;a&lt;/span&gt; &lt;span class="nt"&gt;file&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;but&lt;/span&gt; &lt;span class="nt"&gt;what&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;s the point in cloud upload if you&amp;#39;&lt;/span&gt;&lt;span class="nt"&gt;re&lt;/span&gt; &lt;span class="nt"&gt;going&lt;/span&gt; &lt;span class="nt"&gt;to&lt;/span&gt; &lt;span class="nt"&gt;save&lt;/span&gt; &lt;span class="nt"&gt;the&lt;/span&gt; &lt;span class="nt"&gt;files&lt;/span&gt; &lt;span class="nt"&gt;locally&lt;/span&gt;&lt;span class="o"&gt;?&lt;/span&gt;

&lt;span class="nt"&gt;At&lt;/span&gt; &lt;span class="nt"&gt;this&lt;/span&gt; &lt;span class="nt"&gt;point&lt;/span&gt; &lt;span class="nt"&gt;we&lt;/span&gt; &lt;span class="nt"&gt;have&lt;/span&gt; &lt;span class="nt"&gt;an&lt;/span&gt; &lt;span class="nt"&gt;image&lt;/span&gt; &lt;span class="nt"&gt;as&lt;/span&gt; &lt;span class="nt"&gt;a&lt;/span&gt; &lt;span class="nt"&gt;buffer&lt;/span&gt; &lt;span class="nt"&gt;and&lt;/span&gt; &lt;span class="nt"&gt;we&lt;/span&gt; &lt;span class="nt"&gt;want&lt;/span&gt; &lt;span class="nt"&gt;to&lt;/span&gt; &lt;span class="nt"&gt;push&lt;/span&gt; &lt;span class="nt"&gt;this&lt;/span&gt; &lt;span class="nt"&gt;to&lt;/span&gt; &lt;span class="nt"&gt;Azure&lt;/span&gt; &lt;span class="nt"&gt;too&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;

&lt;span class="err"&gt;```&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="err"&gt;.EnlighterJSRAW&lt;/span&gt; &lt;span class="err"&gt;enlighter-language=&amp;quot;null&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="nt"&gt;var&lt;/span&gt; &lt;span class="nt"&gt;uploadThumb&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;function&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;buffer&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;itemId&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;machineId&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;userName&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;options&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;res&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="err"&gt;var&lt;/span&gt; &lt;span class="err"&gt;fileName&lt;/span&gt; &lt;span class="err"&gt;=&lt;/span&gt; &lt;span class="err"&gt;itemId&lt;/span&gt; &lt;span class="err"&gt;+&lt;/span&gt; &lt;span class="err"&gt;&amp;#39;_thumb.jpg&amp;#39;&lt;/span&gt;
  &lt;span class="err"&gt;blobClient.createBlockBlobFromText(containerName,&lt;/span&gt; &lt;span class="err"&gt;fileName,&lt;/span&gt; &lt;span class="err"&gt;buffer,&lt;/span&gt; &lt;span class="err"&gt;options,&lt;/span&gt; &lt;span class="err"&gt;function&lt;/span&gt; &lt;span class="err"&gt;(error)&lt;/span&gt; &lt;span class="err"&gt;{&lt;/span&gt;
    &lt;span class="err"&gt;if&lt;/span&gt; &lt;span class="err"&gt;(error&lt;/span&gt; &lt;span class="err"&gt;!=&lt;/span&gt; &lt;span class="err"&gt;null)&lt;/span&gt; &lt;span class="err"&gt;{&lt;/span&gt;
      &lt;span class="err"&gt;console.log(&amp;#39;Azure&lt;/span&gt; &lt;span class="err"&gt;Thumb&lt;/span&gt; &lt;span class="n"&gt;Error&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;, error)&lt;/span&gt;
&lt;span class="s1"&gt;      res.send({sucess:false})&lt;/span&gt;
&lt;span class="s1"&gt;    } else {&lt;/span&gt;
&lt;span class="s1"&gt;      console.log(&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;Azure&lt;/span&gt; &lt;span class="n"&gt;Thumb&lt;/span&gt; &lt;span class="n"&gt;Success&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;)&lt;/span&gt;
&lt;span class="s1"&gt;      anItem = {itemid: itemId,&lt;/span&gt;
&lt;span class="s1"&gt;               machineid:machineId,&lt;/span&gt;
&lt;span class="s1"&gt;               label:&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;Unlabeled&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
               &lt;span class="n"&gt;username&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;userName&lt;/span&gt;
               &lt;span class="n"&gt;thumbURL&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;azureEndpoint&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;fileName&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
      &lt;span class="nt"&gt;res&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nc"&gt;send&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;success&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;anItem&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
    &lt;span class="err"&gt;}&lt;/span&gt;
  &lt;span class="err"&gt;}&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
&lt;span class="err"&gt;}&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Luckily blobClient has another method &lt;code&gt;createBlockBlobFromText&lt;/code&gt; which can handle this buffer and work its magic again. Finally if we get success back from the thumbnail upload, we can tell that &lt;code&gt;res&lt;/code&gt; response object we've been passing around to send some metadata back to the client, so they can know what happened to their image upload. They can use the &lt;code&gt;thumbURL&lt;/code&gt; attribute to know where to look for a thumbnail online.  And we're done. QED.&lt;/p&gt;
&lt;h2&gt;Discussion&lt;/h2&gt;
&lt;p&gt;Azure's SDK made our image uploading easy, but with close analysis there is a some room for improvement. The first thing to notice, which is mentioned earlier is that the callback that the blobClient methods require is one that takes &lt;code&gt;err&lt;/code&gt; as its only argument. I would feel better about my life is they would also send back some data about where the blob got stored. It's true that you can piece it together because it &lt;em&gt;should&lt;/em&gt; be at &lt;code&gt;azureEndpoint+fileName&lt;/code&gt;, but I would have more (ahem) piece of mind if it returned this URL to me.&lt;/p&gt;
&lt;p&gt;The real annoyance is that blobClient has two separate methods for dealing with file paths and text buffers. One method that could either infer these two cases, or provide a specifier as an optional argument makes more sense to me. If it were so, it would allow us more easily to write higher-level functions using the blobClient for many different operations.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I hope you learned to node, to upload, and to Azure for great good.  This sketch walked us through making an account with Azure and then linking in the Azure SDK with Express. They have many good &lt;a href="https://docs.microsoft.com/en-us/azure/storage/storage-nodejs-how-to-use-blob-storage"&gt;resources themselves at Microsoft Developer Network&lt;/a&gt;. Without confusion, signing off.&lt;/p&gt;</content><category term="hacking"></category></entry><entry><title>3 Ways To Access Wikidata Data Until It Can Be Done Properly</title><link href="https://notconfusing.com/3-ways-to-access-wikidata-data-early.html" rel="alternate"></link><published>2016-05-02T02:40:00-07:00</published><updated>2016-05-02T02:40:00-07:00</updated><author><name>max</name></author><id>tag:notconfusing.com,2016-05-02:/3-ways-to-access-wikidata-data-early.html</id><summary type="html">&lt;p&gt;Note: This post is quite old. In fact Wikidata can now be accessed "properly" via the &lt;a href="https://query.wikidata.org/"&gt;Wikidata Query Service (WDQS).&lt;/a&gt; However the techniques outlined below still have their advantages.&lt;/p&gt;
&lt;p&gt;The inaugural &lt;a href="https://meta.wikimedia.org/wiki/Research:Labs2/Hackathons/November_9th,_2013"&gt;Wiki Research Hackathon&lt;/a&gt; went very well, and I'm affirmed that I feel best when I'm conducting Wiki Research. I …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Note: This post is quite old. In fact Wikidata can now be accessed "properly" via the &lt;a href="https://query.wikidata.org/"&gt;Wikidata Query Service (WDQS).&lt;/a&gt; However the techniques outlined below still have their advantages.&lt;/p&gt;
&lt;p&gt;The inaugural &lt;a href="https://meta.wikimedia.org/wiki/Research:Labs2/Hackathons/November_9th,_2013"&gt;Wiki Research Hackathon&lt;/a&gt; went very well, and I'm affirmed that I feel best when I'm conducting Wiki Research. I was asked to give one of the tech talks of the day about accessing Wikidata data programmatically. Here is an outline of the talk&lt;/p&gt;
&lt;h3&gt;Purpose:&lt;/h3&gt;
&lt;p&gt;We'll be viewing Wikidata as file in its own right for research, not as it's canonical use case of being used in various Wikipedias.&lt;/p&gt;
&lt;h3&gt;Native format:&lt;/h3&gt;
&lt;p&gt;Wikidata is a mostly standard Mediawiki instance except that pages don't store "Wikitext", they store JSON blobs. (If you want to understand more about this abstraction, see then &lt;a href="http://www.mediawiki.org/wiki/Manual:ContentHandler"&gt;ContentHandler&lt;/a&gt;).&lt;/p&gt;
&lt;h3&gt;Structure of a Wikidata Item:&lt;/h3&gt;
&lt;p&gt;Main entry point of any Wikidata item is a JSON dictionary, that has this form:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;{“&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="err"&gt;”&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="k"&gt;by&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="k"&gt;language&lt;/span&gt; &lt;span class="k"&gt;dictionary&lt;/span&gt;

&lt;span class="err"&gt;“&lt;/span&gt;&lt;span class="n"&gt;descriptions&lt;/span&gt;&lt;span class="err"&gt;”&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="k"&gt;by&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="k"&gt;language&lt;/span&gt; &lt;span class="k"&gt;dictionary&lt;/span&gt;

&lt;span class="err"&gt;“&lt;/span&gt;&lt;span class="n"&gt;aliases&lt;/span&gt;&lt;span class="err"&gt;”&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="k"&gt;by&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="k"&gt;language&lt;/span&gt; &lt;span class="k"&gt;dictionary&lt;/span&gt;

&lt;span class="err"&gt;“&lt;/span&gt;&lt;span class="n"&gt;claims&lt;/span&gt;&lt;span class="err"&gt;”&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;list&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="n"&gt;property&lt;/span&gt; &lt;span class="k"&gt;and&lt;/span&gt; &lt;span class="k"&gt;values&lt;/span&gt;

&lt;span class="err"&gt;“&lt;/span&gt;&lt;span class="n"&gt;sitelinks&lt;/span&gt;&lt;span class="err"&gt;”&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="k"&gt;by&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="k"&gt;language&lt;/span&gt; &lt;span class="k"&gt;dictionary&lt;/span&gt;&lt;span class="err"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;3 Ways To Access Wikidata:&lt;/h3&gt;
&lt;p&gt;Whether your more comfortable in object-oriented python, parsing large text files, or munging linked data, there is something for you.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Live, from the API, using &lt;a href="http://www.mediawiki.org/wiki/Manual:Pywikibot/Wikidata"&gt;pywikibot&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;Read / write&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Offline, dumps, using Wikidata Toolkit (wdtk)&lt;ul&gt;
&lt;li&gt;Newer, shinier, performant Java&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.mediawiki.org/wiki/Wikidata_Toolkit"&gt;https://www.mediawiki.org/wiki/Wikidata_Toolkit&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Deprecated, slower, unmaintained python "Wikidata Analyzer"&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/mkroetzsch/wda"&gt;https://github.com/mkroetzsch/wda&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Read only&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;As linked data, entities, via content negotiation&lt;ul&gt;
&lt;li&gt;Read only&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Using Pywikibot:&lt;/h3&gt;
&lt;p&gt;With pywikibot you get almost full support of the API.&lt;br&gt;
New classes in the “core” branch&lt;br&gt;
class WikibasePage(Page):&lt;br&gt;
class ItemPage(WikibasePage):&lt;br&gt;
class PropertyPage(WikibasePage):&lt;br&gt;
class Claim(PropertyPage):&lt;br&gt;
Using Pywikibot&lt;br&gt;
Classic pywikibot pagegenerators work.&lt;br&gt;
#make a generator for all the pages with a property&lt;br&gt;
en_wikipedia = pywikibot.Site('en', 'wikipedia')&lt;br&gt;
wikidata = en_wikipedia.data_repository()&lt;br&gt;
property_page = pywikibot.ItemPage(wikidata, 'Property:P21')&lt;br&gt;
pages_with_property = property_page.getReferences()&lt;/p&gt;
&lt;h4&gt;Pywikibot example:&lt;/h4&gt;
&lt;p&gt;I've been harvesting Infobox Book across many languages and writing the corresponding properties to Wikidata &lt;a href="https://github.com/notconfusing/harvest_infobox_book"&gt;https://github.com/notconfusing/harvest_infobox_book&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Using wda&lt;/h3&gt;
&lt;p&gt;[Update: WDA is deprecated and replaced by Wikidata Toolkit, which I explain how to use with code examples &lt;a href="http://notconfusing.com/sex-ratios-in-wikidata-part-iii/" title="Sex Ratios in Wikidata Part III"&gt;in this blog post.&lt;/a&gt;]{style="font-size: 14pt;"}&lt;/p&gt;
&lt;p&gt;WDA, WikiData Analytics, downloads the official dump and analyzes it offline. Cleverly it uses nightly incremental dumps after about a 10GB first download. It's also written in python, mainly by Markus Kroetzsch.After downloading there is a parser that writes a file called kb.txt. kb.txt stores plaintext triples, one per line giving you something like this.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Q21&lt;/span&gt; &lt;span class="n"&gt;link&lt;/span&gt; &lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="n"&gt;trwiki&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;İngiltere&lt;/span&gt;&lt;span class="err"&gt;}&lt;/span&gt; &lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="n"&gt;Q21&lt;/span&gt; &lt;span class="n"&gt;link&lt;/span&gt; &lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="n"&gt;hewiki&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="err"&gt;אנגליה}&lt;/span&gt; &lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="n"&gt;Q21&lt;/span&gt; &lt;span class="k"&gt;alias&lt;/span&gt; &lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="n"&gt;en&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;ENG&lt;/span&gt;&lt;span class="err"&gt;}&lt;/span&gt; &lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="n"&gt;Q21&lt;/span&gt; &lt;span class="k"&gt;alias&lt;/span&gt; &lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="k"&gt;min&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;Inggirih&lt;/span&gt;&lt;span class="err"&gt;}&lt;/span&gt; &lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="n"&gt;Q21&lt;/span&gt; &lt;span class="k"&gt;alias&lt;/span&gt; &lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="n"&gt;sgs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;England&lt;/span&gt;&lt;span class="err"&gt;}&lt;/span&gt; &lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="n"&gt;Q21&lt;/span&gt; &lt;span class="n"&gt;P31&lt;/span&gt; &lt;span class="n"&gt;Q1763527&lt;/span&gt; &lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="n"&gt;Q21&lt;/span&gt; &lt;span class="n"&gt;P47&lt;/span&gt; &lt;span class="n"&gt;Q22&lt;/span&gt; &lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="n"&gt;Q21&lt;/span&gt; &lt;span class="n"&gt;P47&lt;/span&gt; &lt;span class="n"&gt;Q25&lt;/span&gt; &lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="n"&gt;Q21&lt;/span&gt; &lt;span class="n"&gt;P41&lt;/span&gt; &lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="n"&gt;Flag&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="n"&gt;England&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;svg&lt;/span&gt;&lt;span class="err"&gt;}&lt;/span&gt; &lt;span class="p"&gt;.&lt;/span&gt;

&lt;span class="n"&gt;I&lt;/span&gt; &lt;span class="n"&gt;used&lt;/span&gt; &lt;span class="n"&gt;wda&lt;/span&gt; &lt;span class="k"&gt;to&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="n"&gt;my&lt;/span&gt; &lt;span class="n"&gt;analysis&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;most&lt;/span&gt; &lt;span class="k"&gt;unique&lt;/span&gt; &lt;span class="n"&gt;Wikipedias&lt;/span&gt; &lt;span class="n"&gt;according&lt;/span&gt; &lt;span class="k"&gt;to&lt;/span&gt; &lt;span class="n"&gt;Wikidata&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Content Negotiation:&lt;/h3&gt;
&lt;p&gt;You can also access Wikidata as linked data. The build path is:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c"&gt;https://wikidata.org/entity/&amp;lt;QID&amp;gt;.&amp;lt;format&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;where your choices of format are&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;nt&lt;/span&gt;
&lt;span class="err"&gt;rdf&lt;/span&gt;
&lt;span class="err"&gt;ttl&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Content Negotiaton Example&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nl"&gt;https&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="n"&gt;www&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;wikidata&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;org&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;wiki&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nl"&gt;Special&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;EntityData&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;Q42046&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ttl&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;@prefix&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nl"&gt;entity&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nl"&gt;http&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="n"&gt;www&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;wikidata&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;org&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;entity&lt;/span&gt;&lt;span class="o"&gt;/&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;@prefix&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nl"&gt;wikibase&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nl"&gt;http&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="n"&gt;www&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;wikidata&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;org&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;ontology&lt;/span&gt;&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;@prefix&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nl"&gt;rdfs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nl"&gt;http&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="n"&gt;www&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;w3&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;org&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;2000&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;01&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;rdf&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="k"&gt;schema&lt;/span&gt;&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;@prefix&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nl"&gt;skos&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nl"&gt;http&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="n"&gt;www&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;w3&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;org&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;2004&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;02&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;skos&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;core&lt;/span&gt;&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;@prefix&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;schema&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nl"&gt;http&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="k"&gt;schema&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;org&lt;/span&gt;&lt;span class="o"&gt;/&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;@prefix&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;data&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nl"&gt;http&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="n"&gt;www&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;wikidata&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;org&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;wiki&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nl"&gt;Special&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;EntityData&lt;/span&gt;&lt;span class="o"&gt;/&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;@prefix&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nl"&gt;cc&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nl"&gt;http&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="n"&gt;creativecommons&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;org&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;ns&lt;/span&gt;&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;@prefix&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nl"&gt;xsd&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nl"&gt;http&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="n"&gt;www&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;w3&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;org&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;2001&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;XMLSchema&lt;/span&gt;&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="nl"&gt;entity&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;Q42046&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nl"&gt;wikibase&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;Item&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nl"&gt;rdfs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;鬣狗科&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;@zh&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;Hienowate&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;@pl&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;Hiena&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;@eu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;Hyaenidae&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;@es&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;Hiëna&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;@af&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;Dubuk&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;@ms&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;Hiénafélék&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;@hu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;Fisi&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;@sw&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;Hüäänlased&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;@et&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;হায়েনা&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;@bn&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;Hiena&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;@sq&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;Hyaenidae&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;@br&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;Ύαινα&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;@el&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;So until &lt;a href="http://en.wikipedia.org/wiki/Wikidata#Phase_3"&gt;Phase III&lt;/a&gt; there are still some usable options to explore Wikidata for research purposes. However we can still dream of future robust query system. In that dream I like to think of a query system capable of answering "does there exists is a sequence of properties that connects these two Wikidata items?"&lt;/p&gt;</content><category term="hacking"></category></entry><entry><title>WIGI Update: Results from Usability Testing</title><link href="https://notconfusing.com/wigi-update-results-from-usability-testing.html" rel="alternate"></link><published>2015-12-06T18:25:00-08:00</published><updated>2015-12-06T18:25:00-08:00</updated><author><name>max</name></author><id>tag:notconfusing.com,2015-12-06:/wigi-update-results-from-usability-testing.html</id><summary type="html">&lt;p&gt;Since the beta version of  &lt;a href="http://wigi.wmflabs.org/"&gt;wigi.wmflabs.org&lt;/a&gt;, our site dedicated to the biography gender gap on WIkipedia is progressing on the technical side, we decided  conducting usability study on to help improve interaction aspects. We conducted a usability study to find out more. It was fantastic to here people …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Since the beta version of  &lt;a href="http://wigi.wmflabs.org/"&gt;wigi.wmflabs.org&lt;/a&gt;, our site dedicated to the biography gender gap on WIkipedia is progressing on the technical side, we decided  conducting usability study on to help improve interaction aspects. We conducted a usability study to find out more. It was fantastic to here people say ”this is the tool we've been waiting for," and we also want to address issues that lead people to describe their experience as "burdensome".  Thanks to &lt;a href="https://meta.wikimedia.org/wiki/User:Masssly"&gt;Masssly&lt;/a&gt; for compiling the report. I repost his summary here, and the &lt;a href="https://commons.wikimedia.org/wiki/File:WIGI_Usability_Study_Report.pdf"&gt;full report can be read on wikimedia commons&lt;/a&gt;:&lt;/p&gt;
&lt;h2&gt;Executive Summary&lt;/h2&gt;
&lt;p&gt;During the week of November 17 - December 1, 2015, WIGI was tested among participants pulled from the Wikimedia community, loosely described as anyone who edits Wikipedia-the-encyclopedia, or is a potential reader of any of its language versions. The study was carried out via separate video calls with each participant. Nine (9) participants in all took part in the study.&lt;br&gt;
The main objective was to get feedback from the Wikimedia community about how the WIGI presents data, how it demonstrates what can be done with the data and how it makes open data-sets available for download. The study assessed participants’ perception of the content along with their understanding of the user interface and went ahead to evaluate the study participants’ reading enjoyment and overall user experience.&lt;/p&gt;
&lt;p&gt;In general, &lt;strong&gt;67% of participants found the website web site to be uncluttered, straightforward and tidy.&lt;/strong&gt; Whereas&lt;br&gt;
&lt;strong&gt;60% of the participants held that it was fun and easy to use&lt;/strong&gt;, others thought that they spend too much time trying&lt;br&gt;
to figure it out; &lt;strong&gt;one respondent described their experience as “burdensome”&lt;/strong&gt;. All Five (5) participants who were video interviewed indicated that WIGI will help them further explore the gender gap in Wikipedia biographies. In their delight, one participant said that &lt;strong&gt;“I [now] have a deeper understanding of of the diversity and will make strong&lt;/strong&gt;&lt;br&gt;
&lt;strong&gt;arguments backed by statistics”&lt;/strong&gt;. Another participant stated that &lt;strong&gt;”this is the tool we've been waiting for."&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Key problems identified in the study include:&lt;br&gt;
• Difficulty to distinguish between Recent Changes and All Time displays&lt;br&gt;
• Inflexible Recent Changes&lt;br&gt;
• Lack of connection between between variables on the different pages&lt;br&gt;
• Confusion over interpretation of charts&lt;br&gt;
• Difficulty to gauge extent of measure (eg. percentages without figures, and vice versa)&lt;br&gt;
• Use of presumptuous language and maths symbols&lt;br&gt;
• Lack of data probing questions&lt;br&gt;
Read &lt;a href="https://commons.wikimedia.org/wiki/File:WIGI_Usability_Study_Report.pdf"&gt;the full report&lt;/a&gt; for more detail, get back to us &lt;a href="http://twitter.com/notconfusing"&gt;on twitter&lt;/a&gt;, or &lt;a href="https://groups.google.com/forum/#!forum/wigi-project"&gt;email&lt;/a&gt;, if you have any more feedback.&lt;/p&gt;</content><category term="hacking"></category></entry><entry><title>Data Management: The Plan</title><link href="https://notconfusing.com/data-management-the-plan.html" rel="alternate"></link><published>2015-11-16T18:14:00-08:00</published><updated>2015-11-16T18:14:00-08:00</updated><author><name>max</name></author><id>tag:notconfusing.com,2015-11-16:/data-management-the-plan.html</id><summary type="html">&lt;p&gt;Perhaps because it's not something I would have done on my own, thanks to the prodding of &lt;a href="https://twitter.com/evomri"&gt;Daniel Mietchen&lt;/a&gt;, I  have created a data management plan for my &lt;a href="http://notconfusing.com/am-i-doing-my-phd-in-the-open/"&gt;open-PhD adventure&lt;/a&gt;. What is a &lt;a href="https://en.wikipedia.org/wiki/Data_management_plan"&gt;data management plan&lt;/a&gt; (DMP), you might ask? Now that I'm up to speed, I can tell you …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Perhaps because it's not something I would have done on my own, thanks to the prodding of &lt;a href="https://twitter.com/evomri"&gt;Daniel Mietchen&lt;/a&gt;, I  have created a data management plan for my &lt;a href="http://notconfusing.com/am-i-doing-my-phd-in-the-open/"&gt;open-PhD adventure&lt;/a&gt;. What is a &lt;a href="https://en.wikipedia.org/wiki/Data_management_plan"&gt;data management plan&lt;/a&gt; (DMP), you might ask? Now that I'm up to speed, I can tell you that it's a document in which you set out the parameters for how you will create, share, and store the outcomes of a project. It's also the sort of thing you go through in order to pose detail questions to yourself and make rigorous your otherwise slightly sloppy thinking.&lt;/p&gt;
&lt;p&gt;Questions of the license to use for data produced were quite easy for me: I'm dedicated to using as open a license as possible while requiring attribution. Other topics however gave me pause in considering how I will handle the spew of data exhaust I produce all the time. For instance, documentation; how will I keep track of what those bits represent?&lt;/p&gt;
&lt;p&gt;There is a large amount of hubris to sidestep with documentation. Thinking that I could keep a grand folder structure, or complete list seems like myopic optimism. I think there is some wisdom to draw from the casual obvservation that even as organizational tools online improve most coordination still gets done by plaintext emails. That's why I have no plan to keep a megalist sort of card-catalogue of documentation, but to include it as files alongside the datae. I will aim for IPython/Jupyterized inline documentation of data handling when possible, but resorting to standard readme.md files in the directory otherwise. That is, I'll be relying on search as a organisational principle, so the key will be making the barrier to searchable documentation as low as possible - like writing quick files.&lt;/p&gt;
&lt;p&gt;On the question of archiving; how will I keep the data around for a long time? This posed a very difficult question for me because I wasn't sure exactly how much longevity I want from my data. 2 years, 5 years, 20 years? Starting with needs-constraints rather than desires, I thought that storing my data should be easy and free (as in beer). That's why I'm opting for using Github in the DMP. But there are two worries with Github, one is that it limits files to 100mb, so perhaps it's not suitable for all possible data. The second concern is that Github is a company, like any other swashbuckler with venture-capitalism-driven bravado they could disappear easily. So then I thought that I might rely on some HTTP accessible servers at my University: no filesize limits, corporate independency, and tape backup storage. I am rather happy with those combinations, but if I wanted to invest a lot more effort for not a lot more benefit then I could nitpick at both of them being centrally managed regardless of profit motive. The only way to get around this would be to what? Create torrents of my data and seed them from personal servers. The idealist in me is tempted, but the time-scheduler sees the headache.&lt;/p&gt;
&lt;p&gt;If you haven't had to make a data management plan, I could understand that it seems rather abstruse and time consuming. However you should do it anyway because a) it forces you to think more closely about your data on an abstract level, and b) &lt;a href="http://dmptool.org/"&gt;http://dmptool.org/&lt;/a&gt; makes it 50% less scarier than it needs to be. Oh and in the interest of openness after all, my DMP is here: &lt;a href="https://notconfusing.com/images/uploads/2015/11/Open-PhD-data-management-plan.pdf"&gt;Open-PhD data management plan.pdf&lt;/a&gt;.&lt;/p&gt;</content><category term="hacking"></category></entry><entry><title>Wikipedia Indicator of Gender Inequality: Analysing Who We Write About</title><link href="https://notconfusing.com/wikimania2015.html" rel="alternate"></link><published>2015-07-17T20:27:00-07:00</published><updated>2015-07-17T20:27:00-07:00</updated><author><name>max</name></author><id>tag:notconfusing.com,2015-07-17:/wikimania2015.html</id><content type="html">&lt;p&gt;From my &lt;a href="https://wikimania2015.wikimedia.org/wiki/Submissions/Wikipedia_Gender_Inequality_Index:_Analysing_Who_We_Write_About"&gt;presentation at Wikimania 2015&lt;/a&gt;, this infographic is a quick overview of what to expect from WIGI. (Click image for big version, or &lt;a href="http://notconfusing.com/WIGI_Infographic_v05_20150718.svg"&gt;here for SVG version&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;a href="https://notconfusing.com/images/uploads/2015/07/infographic.png"&gt;&lt;img alt="WIGI Infographic" class="aligncenter size-full wp-image-3858" src="https://notconfusing.com/images/uploads/2015/07/infographic.png" style="width:960px !important"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The full presentation (click to enter presentation).&lt;/p&gt;
&lt;p&gt;&lt;a href="http://notconfusing.com/Who%20We%20Write%20About%20-%20Max%20Klein%20-%20%20Wikimania%202015.svg"&gt;&lt;img alt="presgrab" class="aligncenter size-large wp-image-3864" src="https://notconfusing.com/images/uploads/2015/07/presgrab.png" style="width:474px !important"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Our &lt;a href="https://meta.wikimedia.org/wiki/Grants:IdeaLab/WIGI:_Wikipedia_Gender_Index_Tools"&gt;IEG page&lt;/a&gt;, and on &lt;a href="https://github.com/notconfusing/WIGI"&gt;github&lt;/a&gt;.&lt;/p&gt;</content><category term="hacking"></category></entry><entry><title>Joining many DataFrames at once in Pandas: "n-ary Join"</title><link href="https://notconfusing.com/joining-many-dataframes-at-once-in-pandas-n-ary-join.html" rel="alternate"></link><published>2015-02-11T19:28:00-08:00</published><updated>2015-02-11T19:28:00-08:00</updated><author><name>max</name></author><id>tag:notconfusing.com,2015-02-11:/joining-many-dataframes-at-once-in-pandas-n-ary-join.html</id><summary type="html">&lt;p&gt;::: {.cell .border-box-sizing .text_cell .rendered}
::: {.prompt .input_prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}
Joining many DataFrames at once with Reduce
===========================================&lt;/p&gt;
&lt;p&gt;In my last project I wanted to compare many different Gender Inequality Indexes at once, including the one I had just come up with, called &lt;a href="http://notconfusing.com/preliminary-results-from-wigi-the-wikipedia-gender-inequality-index/" title="Preliminary Results From WIGI, The Wikipedia Gender Inequality Index"&gt;"WIGI"&lt;/a&gt;. The problem was that the rank and …&lt;/p&gt;</summary><content type="html">&lt;p&gt;::: {.cell .border-box-sizing .text_cell .rendered}
::: {.prompt .input_prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}
Joining many DataFrames at once with Reduce
===========================================&lt;/p&gt;
&lt;p&gt;In my last project I wanted to compare many different Gender Inequality Indexes at once, including the one I had just come up with, called &lt;a href="http://notconfusing.com/preliminary-results-from-wigi-the-wikipedia-gender-inequality-index/" title="Preliminary Results From WIGI, The Wikipedia Gender Inequality Index"&gt;"WIGI"&lt;/a&gt;. The problem was that the rank and score data for each index was in a separate DataFrame. I need to perform repeated SQL-style &lt;code&gt;join&lt;/code&gt;s. In this case I actually only had to join 5 dataframes, for 5 indices. But later, in helping my partner with her research, she came across the same problem needed to join more than 100. In my mind I saw that we wanted to accomplish this n-ary join. Mathematically I wanted this type of operation, which I couldn't find in &lt;code&gt;pandas&lt;/code&gt;. &lt;a href="https://notconfusing.com/images/uploads/2015/02/join.jpg"&gt;&lt;img alt="join" class="aligncenter wp-image-3729 size-medium" src="https://notconfusing.com/images/uploads/2015/02/join.jpg" style="width:300px !important"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The answer I enjoyed implementing, perhaps because I saw it as this type of repeated operation, is the &lt;code&gt;reduce&lt;/code&gt; of functional programming.&lt;/p&gt;
&lt;p&gt;Ok, say we have these two data sets:
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .rendered}
::: {.input}
::: {.prompt .input_prompt}
In [5]:
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.input_area}
::: {.highlight}
    wigi
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.output_wrapper}
::: {.output}
::: {.output_area}
::: {.prompt .output_prompt}
Out[5]:
:::&lt;/p&gt;
&lt;p&gt;::: {.output_html .rendered_html .output_subarea .output_pyout}
::: {style="max-height: 1000px; max-width: 1500px; overflow: auto;"}&lt;/p&gt;
&lt;table class="dataframe" border="1"&gt;

&lt;thead&gt;


&lt;tr style="text-align: right;"&gt;


&lt;th&gt;

:::
:::
:::
:::
:::
:::


&lt;/th&gt;


&lt;th&gt;

Rank

&lt;/th&gt;


&lt;th&gt;

Score

&lt;/th&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

Economy

&lt;/th&gt;


&lt;th&gt;


&lt;/th&gt;


&lt;th&gt;


&lt;/th&gt;


&lt;/tr&gt;


&lt;/thead&gt;


&lt;tbody&gt;


&lt;tr&gt;


&lt;th&gt;

Republic of China

&lt;/th&gt;


&lt;td&gt;

1

&lt;/td&gt;


&lt;td&gt;

0.356890

&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

Kingdom of Denmark

&lt;/th&gt;


&lt;td&gt;

2

&lt;/td&gt;


&lt;td&gt;

0.347826

&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

Sweden

&lt;/th&gt;


&lt;td&gt;

3

&lt;/td&gt;


&lt;td&gt;

0.345212

&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

South Korea

&lt;/th&gt;


&lt;td&gt;

4

&lt;/td&gt;


&lt;td&gt;

0.343662

&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

Hong Kong

&lt;/th&gt;


&lt;td&gt;

5

&lt;/td&gt;


&lt;td&gt;

0.342857

&lt;/td&gt;


&lt;/tr&gt;


&lt;/tbody&gt;


&lt;/table&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;::: {.cell .border-box-sizing .code_cell .rendered}
::: {.input}
::: {.prompt .input_prompt}
In [6]:
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.input_area}
::: {.highlight}
    world_economic_forum
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.output_wrapper}
::: {.output}
::: {.output_area}
::: {.prompt .output_prompt}
Out[6]:
:::&lt;/p&gt;
&lt;p&gt;::: {.output_html .rendered_html .output_subarea .output_pyout}
::: {style="max-height: 1000px; max-width: 1500px; overflow: auto;"}&lt;/p&gt;
&lt;table class="dataframe" border="1"&gt;

&lt;thead&gt;


&lt;tr style="text-align: right;"&gt;


&lt;th&gt;

:::
:::
:::
:::
:::
:::


&lt;/th&gt;


&lt;th&gt;

Rank

&lt;/th&gt;


&lt;th&gt;

Score

&lt;/th&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

Economy

&lt;/th&gt;


&lt;th&gt;


&lt;/th&gt;


&lt;th&gt;


&lt;/th&gt;


&lt;/tr&gt;


&lt;/thead&gt;


&lt;tbody&gt;


&lt;tr&gt;


&lt;th&gt;

Iceland

&lt;/th&gt;


&lt;td&gt;

1

&lt;/td&gt;


&lt;td&gt;

0.8594

&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

Finland

&lt;/th&gt;


&lt;td&gt;

2

&lt;/td&gt;


&lt;td&gt;

0.8453

&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

Norway

&lt;/th&gt;


&lt;td&gt;

3

&lt;/td&gt;


&lt;td&gt;

0.8374

&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

Sweden

&lt;/th&gt;


&lt;td&gt;

4

&lt;/td&gt;


&lt;td&gt;

0.8165

&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

Denmark

&lt;/th&gt;


&lt;td&gt;

5

&lt;/td&gt;


&lt;td&gt;

0.8025

&lt;/td&gt;


&lt;/tr&gt;


&lt;/tbody&gt;


&lt;/table&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;::: {.cell .border-box-sizing .text_cell .rendered}
::: {.prompt .input_prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}
We'd probably join them like this:
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .rendered}
::: {.input}
::: {.prompt .input_prompt}
In [7]:
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.input_area}
::: {.highlight}
    wigi.join(world_economic_forum, how='outer', lsuffix='_wigi', rsuffix='_wef')
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.output_wrapper}
::: {.output}
::: {.output_area}
::: {.prompt .output_prompt}
Out[7]:
:::&lt;/p&gt;
&lt;p&gt;::: {.output_html .rendered_html .output_subarea .output_pyout}
::: {style="max-height: 1000px; max-width: 1500px; overflow: auto;"}&lt;/p&gt;
&lt;table class="dataframe" border="1"&gt;

&lt;thead&gt;


&lt;tr style="text-align: right;"&gt;


&lt;th&gt;

:::
:::
:::
:::
:::
:::


&lt;/th&gt;


&lt;th&gt;

Rank\_wigi

&lt;/th&gt;


&lt;th&gt;

Score\_wigi

&lt;/th&gt;


&lt;th&gt;

Rank\_wef

&lt;/th&gt;


&lt;th&gt;

Score\_wef

&lt;/th&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

Economy

&lt;/th&gt;


&lt;th&gt;


&lt;/th&gt;


&lt;th&gt;


&lt;/th&gt;


&lt;th&gt;


&lt;/th&gt;


&lt;th&gt;


&lt;/th&gt;


&lt;/tr&gt;


&lt;/thead&gt;


&lt;tbody&gt;


&lt;tr&gt;


&lt;th&gt;

Denmark

&lt;/th&gt;


&lt;td&gt;

NaN

&lt;/td&gt;


&lt;td&gt;

NaN

&lt;/td&gt;


&lt;td&gt;

5

&lt;/td&gt;


&lt;td&gt;

0.8025

&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

Finland

&lt;/th&gt;


&lt;td&gt;

NaN

&lt;/td&gt;


&lt;td&gt;

NaN

&lt;/td&gt;


&lt;td&gt;

2

&lt;/td&gt;


&lt;td&gt;

0.8453

&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

Hong Kong

&lt;/th&gt;


&lt;td&gt;

5

&lt;/td&gt;


&lt;td&gt;

0.342857

&lt;/td&gt;


&lt;td&gt;

NaN

&lt;/td&gt;


&lt;td&gt;

NaN

&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

Iceland

&lt;/th&gt;


&lt;td&gt;

NaN

&lt;/td&gt;


&lt;td&gt;

NaN

&lt;/td&gt;


&lt;td&gt;

1

&lt;/td&gt;


&lt;td&gt;

0.8594

&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

Kingdom of Denmark

&lt;/th&gt;


&lt;td&gt;

2

&lt;/td&gt;


&lt;td&gt;

0.347826

&lt;/td&gt;


&lt;td&gt;

NaN

&lt;/td&gt;


&lt;td&gt;

NaN

&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

Norway

&lt;/th&gt;


&lt;td&gt;

NaN

&lt;/td&gt;


&lt;td&gt;

NaN

&lt;/td&gt;


&lt;td&gt;

3

&lt;/td&gt;


&lt;td&gt;

0.8374

&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

Republic of China

&lt;/th&gt;


&lt;td&gt;

1

&lt;/td&gt;


&lt;td&gt;

0.356890

&lt;/td&gt;


&lt;td&gt;

NaN

&lt;/td&gt;


&lt;td&gt;

NaN

&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

South Korea

&lt;/th&gt;


&lt;td&gt;

4

&lt;/td&gt;


&lt;td&gt;

0.343662

&lt;/td&gt;


&lt;td&gt;

NaN

&lt;/td&gt;


&lt;td&gt;

NaN

&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

Sweden

&lt;/th&gt;


&lt;td&gt;

3

&lt;/td&gt;


&lt;td&gt;

0.345212

&lt;/td&gt;


&lt;td&gt;

4

&lt;/td&gt;


&lt;td&gt;

0.8165

&lt;/td&gt;


&lt;/tr&gt;


&lt;/tbody&gt;


&lt;/table&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;::: {.cell .border-box-sizing .text_cell .rendered}
::: {.prompt .input_prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}
But we want to generalize. Notice here we also inject the name of the DataFrame into the column names to avoid "suffix-hell" as I would like to term it.
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .rendered}
::: {.input}
::: {.prompt .input_prompt}
In [1]:
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.input_area}
::: {.highlight}
    import pandas&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;make_df&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pandas&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;.&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;{}_{}&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;

&lt;span class="n"&gt;filenames&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;!&lt;/span&gt;&lt;span class="n"&gt;ls&lt;/span&gt;

&lt;span class="n"&gt;dfs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;make_df&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="n"&gt;filenames&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;:::
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .text_cell .rendered}
::: {.prompt .input_prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}
Now here's the reducer. I actually end up wanting an inner join in the end, but the type of join is not important to illustrate the fact.&lt;/p&gt;
&lt;p&gt;Here we join 5 DataFrames at once.
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .rendered}
::: {.input}
::: {.prompt .input_prompt}
In [2]:
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.input_area}
::: {.highlight}
    def join_dfs(ldf, rdf):
        return ldf.join(rdf, how='inner')&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;final_df = reduce(join_dfs, dfs) #that&amp;#39;s the magic&lt;/span&gt;
&lt;span class="err"&gt;final_df.head()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.output_wrapper}
::: {.output}
::: {.output_area}
::: {.prompt .output_prompt}
Out[2]:
:::&lt;/p&gt;
&lt;p&gt;::: {.output_html .rendered_html .output_subarea .output_pyout}
::: {style="max-height: 1000px; max-width: 1500px; overflow: auto;"}&lt;/p&gt;
&lt;table class="dataframe" border="1"&gt;

&lt;thead&gt;


&lt;tr style="text-align: right;"&gt;


&lt;th&gt;

:::
:::
:::
:::
:::
:::


&lt;/th&gt;


&lt;th&gt;

Score\_gdi

&lt;/th&gt;


&lt;th&gt;

Rank\_gdi

&lt;/th&gt;


&lt;th&gt;

Score\_gei

&lt;/th&gt;


&lt;th&gt;

Rank\_gei

&lt;/th&gt;


&lt;th&gt;

Rank\_sigi

&lt;/th&gt;


&lt;th&gt;

Score\_sigi

&lt;/th&gt;


&lt;th&gt;

Rank\_wdf

&lt;/th&gt;


&lt;th&gt;

Score\_wdf

&lt;/th&gt;


&lt;th&gt;

Rank\_wef

&lt;/th&gt;


&lt;th&gt;

Score\_wef

&lt;/th&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

Economy

&lt;/th&gt;


&lt;th&gt;


&lt;/th&gt;


&lt;th&gt;


&lt;/th&gt;


&lt;th&gt;


&lt;/th&gt;


&lt;th&gt;


&lt;/th&gt;


&lt;th&gt;


&lt;/th&gt;


&lt;th&gt;


&lt;/th&gt;


&lt;th&gt;


&lt;/th&gt;


&lt;th&gt;


&lt;/th&gt;


&lt;th&gt;


&lt;/th&gt;


&lt;th&gt;


&lt;/th&gt;


&lt;/tr&gt;


&lt;/thead&gt;


&lt;tbody&gt;


&lt;tr&gt;


&lt;th&gt;

Nicaragua

&lt;/th&gt;


&lt;td&gt;

0.912

&lt;/td&gt;


&lt;td&gt;

102

&lt;/td&gt;


&lt;td&gt;

74

&lt;/td&gt;


&lt;td&gt;

37

&lt;/td&gt;


&lt;td&gt;

53

&lt;/td&gt;


&lt;td&gt;

0.8405

&lt;/td&gt;


&lt;td&gt;

13

&lt;/td&gt;


&lt;td&gt;

0.272727

&lt;/td&gt;


&lt;td&gt;

6

&lt;/td&gt;


&lt;td&gt;

0.7894

&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

Rwanda

&lt;/th&gt;


&lt;td&gt;

0.950

&lt;/td&gt;


&lt;td&gt;

80

&lt;/td&gt;


&lt;td&gt;

77

&lt;/td&gt;


&lt;td&gt;

19

&lt;/td&gt;


&lt;td&gt;

43

&lt;/td&gt;


&lt;td&gt;

0.8661

&lt;/td&gt;


&lt;td&gt;

134

&lt;/td&gt;


&lt;td&gt;

0.096154

&lt;/td&gt;


&lt;td&gt;

7

&lt;/td&gt;


&lt;td&gt;

0.7854

&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

Philippines

&lt;/th&gt;


&lt;td&gt;

0.989

&lt;/td&gt;


&lt;td&gt;

17

&lt;/td&gt;


&lt;td&gt;

76

&lt;/td&gt;


&lt;td&gt;

26

&lt;/td&gt;


&lt;td&gt;

57

&lt;/td&gt;


&lt;td&gt;

0.8235

&lt;/td&gt;


&lt;td&gt;

6

&lt;/td&gt;


&lt;td&gt;

0.322785

&lt;/td&gt;


&lt;td&gt;

9

&lt;/td&gt;


&lt;td&gt;

0.7814

&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

Belgium

&lt;/th&gt;


&lt;td&gt;

0.977

&lt;/td&gt;


&lt;td&gt;

38

&lt;/td&gt;


&lt;td&gt;

79

&lt;/td&gt;


&lt;td&gt;

12

&lt;/td&gt;


&lt;td&gt;

1

&lt;/td&gt;


&lt;td&gt;

0.9984

&lt;/td&gt;


&lt;td&gt;

73

&lt;/td&gt;


&lt;td&gt;

0.163734

&lt;/td&gt;


&lt;td&gt;

10

&lt;/td&gt;


&lt;td&gt;

0.7809

&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

Latvia

&lt;/th&gt;


&lt;td&gt;

1.033

&lt;/td&gt;


&lt;td&gt;

52

&lt;/td&gt;


&lt;td&gt;

77

&lt;/td&gt;


&lt;td&gt;

19

&lt;/td&gt;


&lt;td&gt;

24

&lt;/td&gt;


&lt;td&gt;

0.9489

&lt;/td&gt;


&lt;td&gt;

82

&lt;/td&gt;


&lt;td&gt;

0.157623

&lt;/td&gt;


&lt;td&gt;

15

&lt;/td&gt;


&lt;td&gt;

0.7691

&lt;/td&gt;


&lt;/tr&gt;


&lt;/tbody&gt;


&lt;/table&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;::: {.cell .border-box-sizing .text_cell .rendered}
::: {.prompt .input_prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}
I really like the elegance of this solution. I admit there may be other ways to go about it with &lt;code&gt;pandas&lt;/code&gt; only, and I understand the &lt;code&gt;R&lt;/code&gt; mentality of "no for loops". Still this is precisely why I like &lt;code&gt;pandas&lt;/code&gt; in python - you still get the freedom to play as you wish if it makes more sense to you.
:::
:::
:::&lt;/p&gt;</content><category term="hacking"></category></entry><entry><title>Cyberwizard Institute: Retrospective</title><link href="https://notconfusing.com/cyberwizard-institute-retrospective.html" rel="alternate"></link><published>2015-02-02T07:10:00-08:00</published><updated>2015-02-02T07:10:00-08:00</updated><author><name>max</name></author><id>tag:notconfusing.com,2015-02-02:/cyberwizard-institute-retrospective.html</id><summary type="html">&lt;h2&gt;Cyber Wizard Institute&lt;/h2&gt;
&lt;p&gt;The &lt;a href="http://cyber.wizard.institute/"&gt;Cyberwizard Institute &lt;/a&gt; (CWI) was a free programming school based out of &lt;a href="http://sudoroom.org"&gt;Sudo Room&lt;/a&gt;, running for the month of January 2015. The proclamation that I saw on their website before I volunteered to teach there was:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href="https://notconfusing.com/images/uploads/2015/02/cwi.png"&gt;&lt;img alt="cwi" class="size-full wp-image-3704 alignleft" src="https://notconfusing.com/images/uploads/2015/02/cwi.png" style="width:232px !important"&gt;&lt;/a&gt;The idea is to be an anti-bootcamp. Anyone can participate. It's …&lt;/p&gt;&lt;/blockquote&gt;</summary><content type="html">&lt;h2&gt;Cyber Wizard Institute&lt;/h2&gt;
&lt;p&gt;The &lt;a href="http://cyber.wizard.institute/"&gt;Cyberwizard Institute &lt;/a&gt; (CWI) was a free programming school based out of &lt;a href="http://sudoroom.org"&gt;Sudo Room&lt;/a&gt;, running for the month of January 2015. The proclamation that I saw on their website before I volunteered to teach there was:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href="https://notconfusing.com/images/uploads/2015/02/cwi.png"&gt;&lt;img alt="cwi" class="size-full wp-image-3704 alignleft" src="https://notconfusing.com/images/uploads/2015/02/cwi.png" style="width:232px !important"&gt;&lt;/a&gt;The idea is to be an anti-bootcamp. Anyone can participate. It's free. We're going to try hard to have lecture notes, assignments, and lecture livestreams up online. It will be primarily self-directed, but with guidance from higher level wizards.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As a founding member of sudoroom since 2011, but suffering from a recent malaise in my hacktivism, this was the perfect project to reinvigorate my involvement&lt;em&gt;.&lt;/em&gt; What most appealed to me was the idea of an &lt;em&gt;anti-bootcamp,&lt;/em&gt; because I've wanted to make clear to world the distinction I care about between &lt;em&gt;start-up culture&lt;/em&gt; and &lt;em&gt;technology.&lt;/em&gt; I wanted to do something metaphorically akin to hijacking the stereo system at a \$4-coffee-wifi-shack and making a public service announcement that the computers are not just fancy TVs, but programmable instruments of self-empowerment, which, in addition, can be used for non-commercial purposes.&lt;/p&gt;
&lt;h2&gt;Meeting Every Day&lt;/h2&gt;
&lt;p&gt;Without any formal advertising, each sudoer leading CWI was pleasantly surprised when 27 wizardlings showed up on the first day (14 women and 13 men from my count).  When I remarked this to CWI's originator &lt;a href="https://twitter.com/marinakukso"&gt;\@marinakukso&lt;/a&gt;, she responded that "when you offer a free programming class, with no experience required - people want that".&lt;/p&gt;
&lt;p&gt;I recall some apprehension when we introduced ourselves, and there was the occasional naïve posturing  of people who claimed themselves as programmers with the phrase "I know HTML". But the need to impress quickly disappeared as we sat down to struggle with them in installing Linux on the laptops they'd brought.&lt;/p&gt;
&lt;p&gt;The next day I was nervous with anticipation to arrive at an empty room after all we had shown fresh minds was that computer programming was about inexplicable Ubuntu hurdles. Still, with only a slightly leaky attendance most wizards did come back for more. And we went right on with teaching them bash.&lt;/p&gt;
&lt;p&gt;We continued to meet for 5 hours daily with lectures and hackerspace-esque hands-on floating help from higher level wizards, which we dubbed &lt;em&gt;"social code".&lt;/em&gt; Our rhythm was found quickly, and only half way through the month CWI was feeling so magical, it received coverage in the &lt;a href="http://www.eastbayexpress.com/oakland/radically-sharing-temescal/Content?oid=4172033&amp;amp;showFullText=true"&gt;East Bay Express&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;"Many coding bootcamps in the Bay Area charge tens of thousands of dollars in fees, which can be seen as restricting access to what has become essential for finding a job in technology, let alone moving up in Silicon Valley's so-called "meritocracy." Kukso explained that Cyber Wizard Institute's mission is very much aligned with that of Sudo Room, which is to give everyday folks the opportunity to understand and create the technology in their lives. "For a lot people who consider themselves nontechnical," Kukso said, "a lot things relating to technology or coding seem mystical or secret, our perspective is ... everyone can learn these types of things.'&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Pedagogical Questions&lt;/h2&gt;
&lt;p&gt;Yet towards the end, I started to question the effectiveness and importance of CWI. From the beginning as facilitators we quipped that "anti-bootcamp" reallly meant "bootcamp". And the &lt;a href="http://cyber.wizard.institute/calendar.html"&gt;calendar&lt;/a&gt; began by reflecting that.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Day 1: Install Linux&lt;/li&gt;
&lt;li&gt;Day 2: Unix and Bash&lt;/li&gt;
&lt;li&gt;Day 3: vim&lt;/li&gt;
&lt;li&gt;Day 4: HTML&lt;/li&gt;
&lt;li&gt;Day 5: javascript&lt;/li&gt;
&lt;li&gt;Day 6: Networking&lt;/li&gt;
&lt;li&gt;Day 7: Node.js&lt;/li&gt;
&lt;li&gt;Day 8: Git&lt;/li&gt;
&lt;li&gt;&lt;a href="http://cyber.wizard.institute/calendar.html"&gt;etc...&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Which is exactly the way that &lt;a href="http://substack.net/"&gt;substack,&lt;/a&gt; Oakland's pre-eminent "unix philosopher," would have it. Yet, that was before the collaborative aspects took over and I began to try and think about how I would teach a less trained non-programmer version of myself what I know now. I mixed in:&lt;/p&gt;
&lt;p&gt;(click to view the recorded lectures)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Day 1: Install Linux (I counted 5 Ubuntu installs)&lt;/li&gt;
&lt;li&gt;Day 2: &lt;a href="https://www.youtube.com/watch?v=mMJp7X_ao8g"&gt;Turing Machines&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Day 3: &lt;a href="https://www.youtube.com/watch?v=ah9puba1aSQ"&gt;Emacs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Day 4: &lt;a href="https://www.youtube.com/watch?v=0R7dHozRNS4"&gt;Python&lt;/a&gt;, &lt;a href="http://nbviewer.ipython.org/github/cyberwizardinstitute/workshops/blob/master/python_intro_and_spell_checker.ipynb"&gt;(notes)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Day 5: Functional Programming&lt;/li&gt;
&lt;li&gt;Day 6: &lt;a href="https://github.com/cyberwizardinstitute/workshops/blob/master/data_analysis.md"&gt;Data Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Day 7: &lt;a href="https://www.youtube.com/watch?v=Esq5ms3rlo4"&gt;SQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Day 8: &lt;a href="http://nbviewer.ipython.org/github/cyberwizardinstitute/workshops/blob/master/Map%20Reduce.ipynb"&gt;Map Reduce&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Day 9: Algorithmic complexity&lt;/li&gt;
&lt;li&gt;Day 10: &lt;a href="https://www.youtube.com/watch?v=MH2ywQkmfjo"&gt;Set Theory&lt;/a&gt;, &lt;a href="https://www.youtube.com/watch?v=PAvpzjN_Tx0"&gt;(part 2)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Where substack was spreading his knowledge of artisinal web-buildery, I was attempting to proselytize a world of Mathematical elegance. At times I was worried this felt interfering and competitive to the wizards.&lt;/p&gt;
&lt;p&gt;However the final projects did come to life, instigate solely from the intrinsic motivation of the new-wizards. On the last day arduino hacks and personal-itch websites really had materialized. After speaking to those who made it all the way through the month, they spoke of a brighter perspective than my own: perhaps we inadvertently succeeding at being an &lt;em&gt;anti-&lt;/em&gt;bootcamp.&lt;/p&gt;
&lt;h2&gt;The Medium Was Always The Message&lt;/h2&gt;
&lt;p&gt;As another facilitator &lt;a href="https://twitter.com/johnnyscript"&gt;\@Johnnyscript,&lt;/a&gt; at the  ending &lt;em&gt;Cyberpunk Masquerade Wizard Initiation Ceremony,&lt;/em&gt; said we showed them what it coding is actually like - many differently opinionated hackers running around without too much top-down organization. We delivered the essence of the hackerspace more accessibly than just happening upon a room of silent geeks staring down. Our package, despite being a bit dishevelled, did form a solid curriculum, although it was not refined as something that you might pay \$17,000 for. Yet it also was not an altar for silicon-valley start-up-ism.&lt;/p&gt;
&lt;p&gt;Taken together, we find a point that I am surprised that I missed. Whereas  programming bootcamps are normally &lt;em&gt;Cathedrals, as Eric Raymond &lt;a href="https://en.wikipedia.org/wiki/The_Cathedral_and_the_Bazaar"&gt;might put it&lt;/a&gt;&lt;/em&gt;, we built a &lt;em&gt;Bazaar.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Notconfusingly yours,&lt;/p&gt;
&lt;p&gt;Your humble newb-druid.&lt;/p&gt;
&lt;h2&gt;Cyberwizard Institute II&lt;/h2&gt;
&lt;p&gt;"Will there be another Cyberwizard Institute?" many are asking. Likely, but it is as-yet unplanned because volunteer work is tiring. If you have the intitiative or want to hear about an inititiative, &lt;a href="https://github.com/cyberwizardinstitute/discussion/issues"&gt;join our discussion tracker on github&lt;/a&gt;.&lt;/p&gt;</content><category term="hacking"></category></entry><entry><title>Preliminary Results From WIGI, The Wikipedia Gender Inequality Index</title><link href="https://notconfusing.com/preliminary-results-from-wigi-the-wikipedia-gender-inequality-index.html" rel="alternate"></link><published>2015-01-08T06:41:00-08:00</published><updated>2015-01-08T06:41:00-08:00</updated><author><name>max</name></author><id>tag:notconfusing.com,2015-01-08:/preliminary-results-from-wigi-the-wikipedia-gender-inequality-index.html</id><summary type="html">&lt;p&gt;This is a preliminary list of results from a research project is being compiled into full paper on the subject.&lt;/p&gt;
&lt;p&gt;The full paper, in it's academic form &lt;a href="http://arxiv.org/abs/1502.03086"&gt;is now available on arxiv&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;WIGI is the &lt;strong&gt;Wi&lt;/strong&gt;kipedia &lt;strong&gt;G&lt;/strong&gt;ender &lt;strong&gt;I&lt;/strong&gt;nequality Index, a project whose purpose is to attempt …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This is a preliminary list of results from a research project is being compiled into full paper on the subject.&lt;/p&gt;
&lt;p&gt;The full paper, in it's academic form &lt;a href="http://arxiv.org/abs/1502.03086"&gt;is now available on arxiv&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;WIGI is the &lt;strong&gt;Wi&lt;/strong&gt;kipedia &lt;strong&gt;G&lt;/strong&gt;ender &lt;strong&gt;I&lt;/strong&gt;nequality Index, a project whose purpose is to attempt to gain insight into the gender gap through understanding which humans are represented in Wikipedia. &lt;a href="http://hanyang.academia.edu/PiotrKonieczny"&gt;Professor Piotr Konieczny&lt;/a&gt;, and myself thought that, whereas some gender gap research focuses on the editors of Wikipedia directly, we would view the content and metadata of articles as a proxy measure for those editing. Although the notion of analysing Wikipedia content seems quite old, I believe the advent of Wikidata allows us a new range of ambitious questions to be asked.&lt;/p&gt;
&lt;h2&gt;Methodology&lt;/h2&gt;
&lt;p&gt;We use &lt;a href="http://wikidata.org/"&gt;Wikidata&lt;/a&gt;, the new semantic database that feeds Wikipedia. By inspecting it's weekly data dumps, we are able to inspect all the semantic properties associated with every Wikipedia page in any language, all at once. In this case we focus on any article that is about a person, and their any data recorded for the properties &lt;em&gt;gender&lt;/em&gt;, &lt;em&gt;date of birth&lt;/em&gt;, &lt;em&gt;date of death&lt;/em&gt;, &lt;em&gt;place of birth&lt;/em&gt;, &lt;em&gt;citizenship&lt;/em&gt;, and &lt;em&gt;ethnic group&lt;/em&gt; (&lt;a href="https://www.wikidata.org/wiki/Q169906"&gt;example&lt;/a&gt;). We do this courtesy of an excellent tool known as the &lt;a href="https://www.mediawiki.org/wiki/Wikidata_Toolkit"&gt;Wikidata Toolkit&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We compare the found data to historical census data and the World Economic Forum's Gender Gap Index.&lt;/p&gt;
&lt;p&gt;For other computations we also supplement the original data with with aggregation maps to make &lt;em&gt;cultures&lt;/em&gt; from &lt;em&gt;place of birth&lt;/em&gt;, &lt;em&gt;citizenship&lt;/em&gt;, and &lt;em&gt;ethnic group&lt;/em&gt;, by using &lt;a href="https://www.mturk.com/mturk/welcome"&gt;Mechanical Turk&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This project has been conducted in an &lt;a href="https://en.wikipedia.org/wiki/Open_notebook_science"&gt;Open Notebook Science&lt;/a&gt; way, where we have been posting our results and receiving feedback as we work. You can chat with us &lt;a href="https://meta.wikimedia.org/wiki/Research_talk:Wikipedia_Gender_Inequality_Index"&gt;on-wiki&lt;/a&gt;, or &lt;a href="https://github.com/notconfusing/WIGI"&gt;on-github&lt;/a&gt; where all the code and data needed to reproduce this research is available.&lt;/p&gt;
&lt;p&gt;Let's begin:&lt;/p&gt;
&lt;h2&gt;Summary Statistics&lt;/h2&gt;
&lt;p&gt;As of October 14 2014 we inspected a total of 2,561,999 or about &lt;a href="http://www.wikidata.org/wiki/Special:WhatLinksHere/Q5"&gt;&lt;strong&gt;2.5 million "human"&lt;/strong&gt;&lt;/a&gt; items, that is any Wikidata item with the property "instance of: &lt;a href="http://www.wikidata.org/wiki/Q5"&gt;Q5 (human)"&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;On each of those items we look for the following additional properties and found  them no the following number of items.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;                                                                                      \% of total   Items with property&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;hr&gt;
&lt;p&gt;&lt;a href="http://www.wikidata.org/wiki/Property:P172"&gt;ethnic group&lt;/a&gt;                                     0.30                 7,772
  country*                                                                                     23.47               601,361
  &lt;a href="http://www.wikidata.org/wiki/Property:P19"&gt;place of birth&lt;/a&gt;                                   23.93               613,092
  &lt;a href="http://www.wikidata.org/wiki/Property:P570"&gt;date of death&lt;/a&gt;                                   28.79               737,522
  &lt;a href="http://www.wikidata.org/wiki/Property:P27"&gt;citizenship&lt;/a&gt;                                      41.44             1,061,634
  culture**                                                                                   45.20             1,158,086
  &lt;a href="http://www.wikidata.org/wiki/Property:P569"&gt;date of birth&lt;/a&gt;                                   57.92             1,484,003
  &lt;a href="https://www.wikidata.org/wiki/Property:P21"&gt;gender&lt;/a&gt;                                          89.40             2,290,433
  &lt;a href="https://www.wikidata.org/wiki/Wikidata:Glossary#Sitelinks"&gt;at least one site link&lt;/a&gt;           99.05             2,537,545
  a "Q" ID                                                                                     100.00             2,561,999&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;*country&lt;/strong&gt; is determined by seeing if the &lt;em&gt;place of birth&lt;/em&gt; is a country, or if it is a city, see if the city has a &lt;em&gt;country&lt;/em&gt; property&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;**culture&lt;/strong&gt; is determined by using translating &lt;em&gt;ethnic group&lt;/em&gt;, &lt;em&gt;place of birth&lt;/em&gt;, and &lt;em&gt;citizenship&lt;/em&gt; into 1 of 9 world cultures as per &lt;a href="https://en.wikipedia.org/wiki/Inglehart%E2%80%93Welzel_cultural_map_of_the_world"&gt;Inglehart-Welzel map of the world&lt;/a&gt; with Mechanical Turk. Then we take the consensus of the three aggregated variables. (Actually there were no disagreements between the three variables.) All aggregation maps are available for inspection &lt;a href="https://github.com/notconfusing/WIGI/tree/master/helpers/aggregation_maps"&gt;on github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Now the first derived and naive statistic of interest - the total gender breakdown. As we've seen above &lt;strong&gt;10.3% is of unknown&lt;/strong&gt; gender, otherwise we encounter &lt;strong&gt;75.7% male, 13.9% female, and \&amp;lt;.01% nonbinary&lt;/strong&gt; which is perhaps better described as 152 cases.&lt;/p&gt;
&lt;h2&gt;Sanity Checking With Historical Data&lt;/h2&gt;
&lt;p&gt;We want some sanity checking that the data from Wikidata reflects the world at large. To do this we compared our total population per year, calculated by date of birth, versus the world population.&lt;/p&gt;
&lt;p&gt;Comparing the &lt;strong&gt;Wikidata&lt;/strong&gt; data to &lt;a href="https://commons.wikimedia.org/wiki/File:Population_curve.svg"&gt;historical &lt;strong&gt;census data&lt;/strong&gt;&lt;/a&gt;  we find a high significant correlation in total population - Pearson correlation &lt;strong&gt;coefficent = .983&lt;/strong&gt; with  p\&amp;lt;0.01. This lends some credence to the notion that this dataset reflects the world at large. (By the way the historical data trends backwards to 10,000 BCE, but the earliest date of birth in Wikidata is about 4,000 BCE.)&lt;/p&gt;
&lt;h2&gt;Total Biographies Over Time&lt;/h2&gt;
&lt;p&gt;These graphs show the &lt;strong&gt;absolute volume&lt;/strong&gt; of items by &lt;strong&gt;date of birth&lt;/strong&gt; and &lt;strong&gt;death by&lt;/strong&gt; &lt;strong&gt;gender&lt;/strong&gt;, and over &lt;strong&gt;all time, and 1800 onwards&lt;/strong&gt;.&lt;a href="https://notconfusing.com/images/uploads/2015/01/dob_dod_totals_pretty.png"&gt;&lt;img alt="dob_dod_totals_pretty" class="size-full wp-image-3649 alignnone" src="https://notconfusing.com/images/uploads/2015/01/dob_dod_totals_pretty.png" style="width:816px !important"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This first visualization of the gender gap shows how Wikipedia's retroactive focus on history has been consistent in it's bias in representing females. It's also generally quite a smooth curve save for some noticable spikes around World War I and II.&lt;/p&gt;
&lt;p&gt;It's intriguing to contemplate how we might expect date of birth and death to be related. If they were equally well recorded - and barring extreme events like wars - the death curve would look like a right-shifted birth curve. However we see empirically that is not true. At all times the death curve remains absolutely smaller than birth, by a factor of about two-thirds. So we can see a bias in recording the date of birth more often than than date of death.&lt;/p&gt;
&lt;h2&gt;Gender Ratios Over Time&lt;/h2&gt;
&lt;p&gt;The indication of visual skew in gender prodded me to look at how the ratio of male female and nonbinary genders develop over time.&lt;/p&gt;
&lt;p&gt;Note: From here I aggregate the nonbinary genders into a single class not for philosophical reasons of them, but for the ease of visualising the more dimensions they represent. I consider it import to be descriptive about what is found in the data, and to not to lose any perspective because of personal assumptions about gender. If you think there are better ways to describe this data, I would be glad to here from you.&lt;/p&gt;
&lt;p&gt;We adjust our viewing window here to start at 1400CE here because the data is too sparse to provide meaningful visual data.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://notconfusing.com/images/uploads/2015/01/fem_nonbin_per_time_pretty.png"&gt;&lt;img alt="fem_nonbin_per_time_pretty" class="size-full wp-image-3650 alignnone" src="https://notconfusing.com/images/uploads/2015/01/fem_nonbin_per_time_pretty.png" style="width:751px !important"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Curiously since about 1800 to present, the female ratio of biographies is greater when using the date of birth measure than the date of death measure. What is the interperation? Somehow recording female date of birth is more prominent in a way that recording date of death isn't. Although both ratios are rising, somehow date of birth is outstripping date of death. It would be great to investigate how much this is owed to recording practices and how much it is owed to social phenomenon.&lt;/p&gt;
&lt;p&gt;Notice after about 1990, the spike is very large, and even crosses 50%. This is more statistical anomaly than anything else, since the number of humans with date of birth about 1990 is very small as you can see in the volumes plot. There are only 12,000 entries with date of birth in 1990 and only 199 biographies born in the year 2000. Even with discounting very recent trends of the last 20 years, which describe humans that are just entering adulthood or younger, the female ratio is rising exponentially. I was expecting to fit a logistics curve to the female percentage so that we could predict when we might reach parity, however that notion does not makes sense with what is being shown. Although there it may not necessarily indicated equity, fitting an exponential model to this percentage we can calculate when the female percentage would reach 50%. By our calculations it would be &lt;strong&gt;February 2034&lt;/strong&gt; when the exponential extrapolation would reach 50% female representation.  But of course predicting growth of percentages can be lead to nonsensical results (&lt;a href="http://xkcd.com/1007/"&gt;as humourously shown in this xkcd comic&lt;/a&gt;). I suspect we will see a logistics model, but simply haven't encountered the inflection point of slowing rate of growth yet.&lt;/p&gt;
&lt;h2&gt;Aggregating Cultures&lt;/h2&gt;
&lt;p&gt;First some caveats as to method which we use in the next section.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There is no good way to aggregate cultures perfectly. Aggregation in general assumes some loss of fidelity. The point in doing so is to gain a broader-stroke picture, and in this case simplify visualizations.&lt;/li&gt;
&lt;li&gt;The method we used for aggregation - starting from the &lt;a href="https://en.wikipedia.org/wiki/Inglehart%E2%80%93Welzel_cultural_map_of_the_world"&gt;Inglehart-Welzel map of the world&lt;/a&gt; (right), and then "mechanical turking" in the rest of the values - comes loaded with it's own cultural baggage and perspective.&lt;a href="https://commons.wikimedia.org/wiki/File%3AInglehart_Values_Map2.svg"&gt;&lt;img alt="" src="https://upload.wikimedia.org/wikipedia/commons/thumb/7/79/Inglehart_Values_Map2.svg/691px-Inglehart_Values_Map2.svg.png" style="width:691px !important"&gt;&lt;/a&gt; By DancingPhilosopher [CC BY-SA 3.0 (http://creativecommons.org/licenses/by-sa/3.0)], via Wikimedia Commons[/caption]&lt;/li&gt;
&lt;li&gt;Inglehart-Welzel map only really makes sense for modern geopolitical boundaries. For instance the notion of having a &lt;em&gt;Protestant&lt;/em&gt; and &lt;em&gt;Catholic&lt;/em&gt; world before Protestantism and Catholicism, does not make sense. We use those soft modern boundaries superimposed over the geographical region to determine historical values. So if you were born in ancient Greece, you are known as O&lt;em&gt;rthodox&lt;/em&gt; in this method.&lt;/li&gt;
&lt;li&gt;Some ethnicities were a mixture of two cultures, like "Thai-American", in those cases we took the modifier, so we'd use "Thai" -&gt; &lt;em&gt;South Asian.&lt;/em&gt; There are two ways to do this and both of them are not very good, we make a compromise to get a rough picture. &lt;a href="https://github.com/notconfusing/WIGI/tree/master/helpers/aggregation_maps"&gt;The full data is available for more munging if you would like to fine tune it&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;We aggregate the the 9 cultures from 3 similar but different Wikidata properties in &lt;em&gt;citizenship, ethnic group, and place of birth.&lt;/em&gt; Since each of those are different concepts, a conflict may arise - however in this research we did not find a case where different property aggregations gave different world cultures.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Gender Ratios By Culture&lt;/h2&gt;
&lt;p&gt;We make a cross-tabulation of gender by culture. A Chi-squared test show the observed distributions of gender by culture to be significantly. We now graph the female percentage of biographies by culture.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://notconfusing.com/images/uploads/2015/01/female_by_culture.png"&gt;&lt;img alt="female_by_culture" class="size-large wp-image-3653 alignnone" src="https://notconfusing.com/images/uploads/2015/01/female_by_culture.png" style="width:474px !important"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;More than anything, I think what astounds most is the &lt;strong&gt;large different in the difference in absolute number of biographies by culture&lt;/strong&gt;. The European and English-speaking world dominates by a large amount here. Although, it might be that European and English-Speaking biographies are simply more likely to be described in Wikidata at the moment, by some sort of quirk of the volunteer import process. Later we'll see how that affects German and Austrian items.&lt;/p&gt;
&lt;p&gt;If we do inspect the female percentages as-is, we find a &lt;strong&gt;very high showing for in the Confucian culture&lt;/strong&gt;. After talking to some Confucian-world Wikipedians on twitter (who I can't find now to credit) and fellow Wikipedia Researcher Hai-Yi Zhu from University of Minnesota, we produced the hypothesis that this is because the phenomenon of celebrity is larger in those cultures, and celebrity is more evenly gender-distributed. We will investigate the celebrity hypothesis in a bit. If you have another hypothesis, we welcome your input for testing.&lt;/p&gt;
&lt;p&gt;We provide the graph of &lt;strong&gt;nonbinary percentages of biographie&lt;/strong&gt;s by culture too. The cultures are ordered in the same way as the female graph for ease of comparison. Notice that the &lt;strong&gt;ordering is relatively similar to the female graph&lt;/strong&gt; - so on the surface, recording female biographies is linked to recording nonbinary genders too.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://notconfusing.com/images/uploads/2015/01/nonbinary_by_culture.png"&gt;&lt;img alt="nonbinary_by_culture" class="size-full wp-image-3652 alignnone" src="https://notconfusing.com/images/uploads/2015/01/nonbinary_by_culture.png" style="width:592px !important"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Gender Ratios Over Time&lt;/h2&gt;
&lt;p&gt;Lets mix all these variable now, by viewing the culture ratio trends over time. To note our sample size as we continue, only 951,101 or about &lt;strong&gt;35% of total records have all of &lt;em&gt;date of birth&lt;/em&gt;, &lt;em&gt;culture&lt;/em&gt;, and&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;gender&lt;/em&gt;&lt;/strong&gt; data.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://notconfusing.com/images/uploads/2015/01/culture_dob_combined.png"&gt;&lt;img alt="culture_dob_combined" class="wp-image-3655 size-full alignnone" src="https://notconfusing.com/images/uploads/2015/01/culture_dob_combined.png" style="width:1292px !important"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You can see that the recent past around &lt;strong&gt;1800 is a low point for female recognition in all cultures&lt;/strong&gt; and most of history in the past 3 millennia. Likewise visually it is evident that historical trends in different cultures have, while not reaching 50%, peaked at much higher percentages. In the modern historical graph, we can see &lt;strong&gt;a rise occurring for all cultures&lt;/strong&gt;, and super-linear growth even for the Confucian and South Asian countries. .The sky-rocketing ratios after 1990 are less significant as noted above.&lt;/p&gt;
&lt;h2&gt;Gender by Wikipedia Language&lt;/h2&gt;
&lt;p&gt;Now let us recall that there is one more dimension we have recorded, the &lt;strong&gt;&lt;em&gt;sitelink&lt;/em&gt; dimension&lt;/strong&gt;, which indicates whether or not for an item a Wikipedia language has an entry for it. To be clear, say for instance that Finnish Wikipedia has an article about a Japanese human; we would be commenting on Finnish Wikipedia. With this data we can analyse the female and nonbinary tendencies of a Language, not a nationality or culture.&lt;/p&gt;
&lt;p&gt;Here we have plots that show the &lt;strong&gt;relative frequencies of female articles per Wikipedia language, versus the size of the language&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://notconfusing.com/images/uploads/2015/01/Language_femper_Scatter1.png"&gt;&lt;img alt="Language_femper_Scatter" class="alignnone wp-image-3677 size-full" src="https://notconfusing.com/images/uploads/2015/01/Language_femper_Scatter1.png" style="width:1064px !important"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;And again for nonbinary humans.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://notconfusing.com/images/uploads/2015/01/Language_nonbin_Scatter1.png"&gt;&lt;img alt="Language_nonbin_Scatter" class="alignnone wp-image-3678 size-full" src="https://notconfusing.com/images/uploads/2015/01/Language_nonbin_Scatter1.png" style="width:1074px !important"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Notice in general there is &lt;strong&gt;no simple trend linking Wikipedia size to female representation&lt;/strong&gt;. The visual technique with which I investigate here is to look at for the points whose magnitude from the origin is greatest. Mostly I see relatively a  flat constant rate, with a few &lt;strong&gt;Wikipedias standing out a bit, like the Japanese, Chinese and Tagalog&lt;/strong&gt;. So again we are seeing some evidence for Confucian and South Asian cultures being less gender biased when following the sitelink method analysis.&lt;/p&gt;
&lt;h2&gt;Gender by Aggregated Wikipedia Language&lt;/h2&gt;
&lt;p&gt;To sure up the idea of cultural influence in the sitelinks analysis we aggregate the languages into the nine World Cultures as before. In this case, since there are only about 280 languages, I assigned all of the languages by hand, rather than resorting to Mechanical Turk.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://notconfusing.com/images/uploads/2015/01/language_aggregate_femper.png"&gt;&lt;img alt="language_aggregate_femper" class="size-full wp-image-3662 alignnone" src="https://notconfusing.com/images/uploads/2015/01/language_aggregate_femper.png" style="width:558px !important"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;To clarify, the technique used here is that every Wikidata item counts towards a culture if a sitelink exists in at least one language associated with that culture. So if an article has language links to English, Chinese, and Japanese wikipedia, that item counts only once towards each of the English-speaking and Confucian categories.&lt;/p&gt;
&lt;p&gt;Now we have a more coherent picture about which types of Wikipedias by language are focusing on female articles. And &lt;strong&gt;we do continue to see a high Confucian showing&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Let us test our &lt;strong&gt;&lt;em&gt;celebrity hypothesis&lt;/em&gt;&lt;em&gt;.&lt;/em&gt; For the Chinese, Japanese, Korean, Tagalog, Urdu, German and English Wikipedias, we retrieved the &lt;/strong&gt;page content of each Biography from 1930 until 1989** (recall that there are very few Biographies with date of birth 1990 and higher).&lt;/p&gt;
&lt;p&gt;We search for the English or foreign language words that are associated with &lt;em&gt;celebrity&lt;/em&gt;. The dictionary used is:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;jawiki&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;俳優&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;選手&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;歌手&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;ミュージシャン&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;モデル&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;アイドル&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;

&lt;span class="s1"&gt;&amp;#39;zhwiki&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;演員&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;運動員&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;歌手&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;音乐家&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;模特兒&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;偶像&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;

&lt;span class="s1"&gt;&amp;#39;kowiki&amp;#39;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;배우&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;선수&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;가수&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;음악가&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;모델&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;우상&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;

&lt;span class="s1"&gt;&amp;#39;tlwiki&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;artista&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;aktor&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;player&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;mang-aawit&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;musikero&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;modelo&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;idolo&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;

&lt;span class="s1"&gt;&amp;#39;urwiki&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;اردو&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;کھلاڑ&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;گلوکار&amp;#39;&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;موسیقار&amp;#39;&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;ماڈل&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;بت&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;

&lt;span class="s1"&gt;&amp;#39;dewiki&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;schauspieler&amp;#39;&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;spieler&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Musiker&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Sänger&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Modell&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Idol&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;

&lt;span class="s1"&gt;&amp;#39;enwiki&amp;#39;&lt;/span&gt; &lt;span class="p"&gt;:[&lt;/span&gt;&lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;actor&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;actress&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;player&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;singer&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;musician&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;model&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;idol&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="err"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If you can provide better translations than Google's software, let me know. We consider a celebrity to be a biography that contains one of the &lt;strong&gt;above words within the first 200 characters&lt;/strong&gt; of its Wikipedia entry.&lt;/p&gt;
&lt;p&gt;Then we make a heatmap comparing the language, the decade and, the gender, and celebrity percentage.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://notconfusing.com/images/uploads/2015/01/celebrity_heatmap.png"&gt;&lt;img alt="celebrity_heatmap" class="size-full wp-image-3664 alignnone" src="https://notconfusing.com/images/uploads/2015/01/celebrity_heatmap.png" style="width:862px !important"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Using visual inspection, at first glance we can see that the female matrix is darker in general that the other two matrices. So recorded &lt;strong&gt;females are more likely to be celebrities&lt;/strong&gt; among these languages.&lt;/p&gt;
&lt;p&gt;Likewise you can see that in general the heatmap transitions to being darker at the top than bottom, so we have shifted to being &lt;strong&gt;more celebrity conscious in most languages in recent years&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Lastly we see some vertical-striped features showing that for instance &lt;strong&gt;Tagalog is prone to being celebrity conscious across gender and time&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;To determine the significance of the effects we perform a logistic regression analysis in predicting the celebrity percentage variable. The coefficient matrix is printed below.&lt;/p&gt;
&lt;hr&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;          coef       std err   z        P\&amp;gt;\|z\|   \[95.0% Conf. Int.\]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;decade      0.0236     0.013     1.823    0.068      -0.002 0.049
  enwiki      0.0509     0.875     0.058    0.954      -1.664 1.766
  jawiki      0.7763     0.837     0.927    0.354      -0.865 2.418
  kowiki      1.3834     0.832     1.662    0.097      -0.248 3.015
  tlwiki      3.0009     0.945     3.176    0.001      1.149 4.853
  urwiki      0.8901     0.869     1.025    0.306      -0.813 2.593
  zhwiki      0.5383     0.846     0.637    0.524      -1.119 2.196
  female      1.3580     0.453     2.999    0.003      0.471 2.245
  intercept   -47.9056   25.368    -1.888   0.059      -97.626 1.815&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Depending on which arbitrary significance threshold you choose to use, we find different answers, but at least the&lt;strong&gt; &lt;em&gt;female, &lt;/em&gt;and &lt;em&gt;Tagalog&lt;/em&gt;, variables are significant with p\&amp;lt;0.05&lt;/strong&gt;. If we loosen the significance threshold slighly, &lt;em&gt;decade, &lt;/em&gt;and &lt;em&gt;Korean&lt;/em&gt; also become predictors. This lends a lot of credence to the notion that in the cases in which Women are recorded in Wikipedias, they have a strong tendency to be a celebrity.&lt;/p&gt;
&lt;h2&gt;Connections To The World Economic Forum Index&lt;/h2&gt;
&lt;p&gt;Indexes are useful, but they are more useful as a group of compatible and comparable indexes. We c&lt;strong&gt;ompared our place of birth and citizenship data as it related to gender, to the &lt;a href="" title="http://reports.weforum.org/global-gender-gap-report-2014/rankings/"&gt;World Economic Forum Gender Gap Index&lt;/a&gt;&lt;/strong&gt;. The World Economic Forum uses its own methodology to produce a scalar value on the interval (0,1) to rank the gender equality of a country. To match to that format, we take the Wikidata data in the form of female composition of biographies by country.&lt;/p&gt;
&lt;p&gt;We performed a calibration step to see which time window of data would produce our ranking of countries most closely being correlated with the World economic forum. If the Wikidata dataset is used with the time window only considering the biographies with date of birth between 1890 and 1990, the Spearman rank correlation is 0.31 with a p value of 0.03. That means that there is &lt;strong&gt;some founding for accepting the female composition of Wikidata items of humans associated with a country as an inequality index&lt;/strong&gt;, because is significantly correlated with other respected inequality indexes.&lt;/p&gt;
&lt;p&gt;Here is a sample of the two rankings side-by-side. We display the top 10 as per the World Economic Forum rank, and then the top 10 as per the Wikipedia Rank. You'll aslo see the associated WIGI rank, the raw scores for each, and the difference in the ranking.&lt;br&gt;
&lt;em&gt;World Economic Forum Top 10&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Country       WEF Rank   Wikipedia Rank   WEF Score   Wikipedia Score   Rank Difference&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Iceland       1          30               0.8594      0.1895            -29
  Finland       2          39               0.8453      0.1807            -37
  Norway        3          22               0.8374      0.2142            -19
  Sweden        4          1                0.8165      0.3452            3
  Denmark       5          20               0.8025      0.2149            -15
  Nicaragua     6          9                0.7894      0.2727            -3
  Rwanda        7          108              0.7854      0.0962            -101
  Ireland       8          64               0.7850      0.1586            -56
  Philippines   9          3                0.7814      0.3228            6
  Belgium       10         58               0.7809      0.1637            -48&lt;/p&gt;
&lt;p&gt;&lt;em&gt;WIGI Top 10&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Country                      WEF Rank   Wikipedia Rank   WEF Score   Wikipedia Score   Rank Difference&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Sweden                       4          1                0.8165      0.3452            3
  South Korea                  117        2                0.6403      0.3437            115
  Philippines                  9          3                0.7814      0.3228            6
  Bahrain                      124        4                0.6261      0.3171            120
  Mauritius                    106        5                0.6541      0.2941            101
  People's Republic of China   87         6                0.6830      0.2812            81
  Australia                    24         7                0.7409      0.2760            17
  Japan                        104        8                0.6584      0.2732            96
  Nicaragua                    6          9                0.7894      0.2727            -3
  Swaziland                    92         10               0.6772      0.2593            82&lt;/p&gt;
&lt;p&gt;We see how the rankings bear some similarity, but that the correlation is mild. Still we can take away that the notion of what the WEF is driving at with it's measure, and the number of female biographies that exist about humans in a country, as somewhat related idea.&lt;/p&gt;
&lt;h2&gt;Data Reliability&lt;/h2&gt;
&lt;p&gt;The question of how well Wikidata accurately reflects all Wikipedias, is important to determine before addressing the question of how well Wikipedias reflect the world at-large.&lt;/p&gt;
&lt;p&gt;During our research, we found a curious quirk in the way that nationality is recorded, and the story is instructive in showing that &lt;strong&gt;Wikidata still has a few artefacts of its bot-imported nature&lt;/strong&gt;. A more in-depth analysis, I previously blogged about is available in a post about the &lt;a href="http://notconfusing.com/wikidata-and-the-measure-of-nationality-the-germanic-shift/" title="Wikidata and the Measure of Nationality: The Germanic Shift"&gt;"Wikidata and the Measure of Nationality&lt;/a&gt;".&lt;/p&gt;
&lt;p&gt;In short, the idea centres around an early finding, that indicated that Protestant European humans seemed to disappear in the 1930s, when we were determining culture just using the "Place of Birth" property. It looked like this:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://notconfusing.com/images/uploads/2015/01/germanic_shift_motivation.png"&gt;&lt;img alt="germanic_shift_motivation" class="alignnone wp-image-3667 size-full" src="https://notconfusing.com/images/uploads/2015/01/germanic_shift_motivation.png" style="width:1158px !important"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is what lead us to investigate how nationalities were being classified on Wikidata. The next graphs show which humans have which classification method - by &lt;em&gt;place of birth, citizenship, &lt;/em&gt;or  &lt;em&gt;both&lt;/em&gt; - for nationality. For Germanic humans we saw a large shift:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://notconfusing.com/images/uploads/2015/01/germanic_germanic.png"&gt;&lt;img alt="germanic_germanic" class="size-large wp-image-3669 alignnone" src="https://notconfusing.com/images/uploads/2015/01/germanic_germanic.png" style="width:410px !important"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;And for all other populations we witness no such thing:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://notconfusing.com/images/uploads/2015/01/germanic_all.png"&gt;&lt;img alt="germanic_all" class="size-full wp-image-3668 alignnone" src="https://notconfusing.com/images/uploads/2015/01/germanic_all.png" style="width:425px !important"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;After publishing these finding, a Wikimedian wrote in to explain that the import of Germanic human data into Wikidata occured through a bot called "FischBot", and that the shift is likely only related to the way that that software operated. The moral being that we should still be vigilant in staying aware of the data quality in Wikidata.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;It is not my intention to draw any large scale conclusions at the moment. For that I will &lt;strong&gt;wait until the publication of the paper&lt;/strong&gt; for which this analysis is intended. Still I would be &lt;strong&gt;glad to hear any insights you might see&lt;/strong&gt; until then.&lt;/p&gt;
&lt;h3&gt;Update:&lt;/h3&gt;
&lt;p&gt;We finished the writing &lt;a href="https://notconfusing.com/images/uploads/2015/01/Gender-Analysis-of-Wikipedia-Biographies.pdf"&gt;the paper&lt;/a&gt;. An excerpt from the conclusion there:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Our research confirms that gender inequality is a phenomenon with a long history, but whose patterns can be analyzed and quantified on a larger scale than previously thought possible. Through the use of Inglehart-Welzel cultural clusters, we show that gender inequality can be analyzed with regards to world’s cultures. In the dimension studied (coverage of females and other genders in reference works) we show a steadily improving trend, through one with aspects that deserve careful follow up analysis (such as the surprisingly high ranking of the Confucian and South Asian clusters).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Tweet at me &lt;a href="https://twitter.com/notconfusing"&gt;\@notconfusing&lt;/a&gt;&lt;/strong&gt; .&lt;/p&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;p&gt;I programmed all of this research in using the IPython notebook, and it's &lt;strong&gt;all entirely open source&lt;/strong&gt; and hopefully reproducible from &lt;a href="https://github.com/notconfusing/WIGI#readme"&gt;https://github.com/notconfusing/WIGI&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I plan to start parsing and filtering Wikidata montly to provide updated data, which should be coming soon.&lt;/p&gt;</content><category term="hacking"></category></entry><entry><title>Häskell und Grepl: Data Hacking Wikimedia Projects Exampled With The Open Access Signalling Project</title><link href="https://notconfusing.com/haskell-und-grepl-data-hacking-wikimedia-projects-exampled-with-the-open-access-signalling-project.html" rel="alternate"></link><published>2014-07-19T12:51:00-07:00</published><updated>2014-07-19T12:51:00-07:00</updated><author><name>max</name></author><id>tag:notconfusing.com,2014-07-19:/haskell-und-grepl-data-hacking-wikimedia-projects-exampled-with-the-open-access-signalling-project.html</id><summary type="html">&lt;p&gt;In what could easily be a recurring annual trip,&lt;a href="http://existenceproof.net/"&gt;Matt Senate&lt;/a&gt;, and I came to Berlin this week to participate in &lt;a href="http://2014.okfestival.org/"&gt;Open Knowledge Festival&lt;/a&gt;. We spoke at the &lt;a href="http://csvconf.com/"&gt;csv,conf&lt;/a&gt; a fringe event in its first year, ostensibly about the comma separated values, but more so about unusual data hacking …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In what could easily be a recurring annual trip,&lt;a href="http://existenceproof.net/"&gt;Matt Senate&lt;/a&gt;, and I came to Berlin this week to participate in &lt;a href="http://2014.okfestival.org/"&gt;Open Knowledge Festival&lt;/a&gt;. We spoke at the &lt;a href="http://csvconf.com/"&gt;csv,conf&lt;/a&gt; a fringe event in its first year, ostensibly about the comma separated values, but more so about unusual data hacking. On behalf of &lt;a href="http://en.wikipedia.org/wiki/Wikipedia:Open_Access/Signalling_OAness"&gt;WikiProject Open Access - Signalling OA-ness&lt;/a&gt; team, we generalized our experience in data-munging with Wikimedia projects for the new user. We were asked to make the talk more story-oriented than technical; and since we were in Germany, we decided to use that famous narrative of &lt;a href="https://www.wikidata.org/wiki/Q92611#sitelinks-wikipedia"&gt;Häskell&lt;/a&gt; &lt;a href="https://www.wikidata.org/wiki/Q11829#sitelinks-wikipedia"&gt;and&lt;/a&gt; &lt;a href="http://www.wikidata.org/wiki/Q283302#sitelinks-wikipedia"&gt;Grepl&lt;/a&gt;. In broad strokes we go through: &lt;em&gt;how Wikimedia projects work, history of Wiki Data-Hacking, from “Ignore All Rules” to calcification, Wikidata told as Hänsel and Gretel, signalling OA-ness, how you could do it too.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;These are the full slides (although slide show does not seem to like &lt;a href="https://github.com/wpoa/OA-signalling/blob/master/csvconf/csvconf_data-hacking_with_wikimedia_projects.odp?raw=true"&gt;our Open Office document&lt;/a&gt; so much):&lt;br&gt;
&lt;iframe src="//www.slideshare.net/slideshow/embed_code/37151192" style="width:476px !important" frameborder="0" marginwidth="0" marginheight="0" scrolling="no"&gt;&lt;/iframe&gt;&lt;/p&gt;
&lt;p&gt;And a crowdsourced recording of the session:&lt;br&gt;
&lt;iframe src="//www.youtube.com/embed/w5T6RoEckAQ" style="width:560px !important" frameborder="0" allowfullscreen="allowfullscreen"&gt;&lt;/iframe&gt;&lt;/p&gt;
&lt;p&gt;We missed half of lunch with the queue of questions extending past our sessions, which was fabulous to see such interest. There is a particular affinity we found with the &lt;a href="http://contentmine.org/"&gt;Content Mine&lt;/a&gt; initiative, which wants to programmatically extract facts from papers. Since we are finding and uploading mine-able papers, you could imagine some sort of suggestion system which says to an editor "you cited [fact x] from this paper, do you also want to cite [extracted facts] in the Wikipedia article too?". Let's work to make that system a fact in itself.&lt;/p&gt;</content><category term="hacking"></category></entry><entry><title>Wiki-Class Set-up Guide and Exploration</title><link href="https://notconfusing.com/wiki-class-set-up-guide-and-exploration.html" rel="alternate"></link><published>2014-07-06T21:58:00-07:00</published><updated>2014-07-06T21:58:00-07:00</updated><author><name>max</name></author><id>tag:notconfusing.com,2014-07-06:/wiki-class-set-up-guide-and-exploration.html</id><summary type="html">&lt;p&gt;[&lt;a href="http://nbviewer.ipython.org/github/notconfusing/Wiki-Class/blob/master/Wiki-Class%20Set-up%20Guide%20and%20Exploration.ipynb"&gt;Best viewed with IPython Notebook Viewer&lt;/a&gt;]{style="font-size: 18pt;"}&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;::: {.text_cell_render .border-box-sizing .rendered_html}
Wiki-Class Set-up Guide and Exploration
=======================================&lt;/p&gt;
&lt;p&gt;&lt;a href="http://pythonhosted.org/wikiclass/"&gt;Wiki-Class&lt;/a&gt; is python package that can determine the &lt;em&gt;quality&lt;/em&gt; of a Wikipedia page, using machine learning. It is the open-sourcing of the &lt;a href="https://www.wikidata.org/wiki/Q245748#sitelinks-wikipedia"&gt;Random Forest&lt;/a&gt; algorithm used by &lt;a href="https://en.wikipedia.org/wiki/User:SuggestBot"&gt;SuggestBot&lt;/a&gt;. SuggestBot is an opt-in recommender …&lt;/p&gt;</summary><content type="html">&lt;p&gt;[&lt;a href="http://nbviewer.ipython.org/github/notconfusing/Wiki-Class/blob/master/Wiki-Class%20Set-up%20Guide%20and%20Exploration.ipynb"&gt;Best viewed with IPython Notebook Viewer&lt;/a&gt;]{style="font-size: 18pt;"}&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;::: {.text_cell_render .border-box-sizing .rendered_html}
Wiki-Class Set-up Guide and Exploration
=======================================&lt;/p&gt;
&lt;p&gt;&lt;a href="http://pythonhosted.org/wikiclass/"&gt;Wiki-Class&lt;/a&gt; is python package that can determine the &lt;em&gt;quality&lt;/em&gt; of a Wikipedia page, using machine learning. It is the open-sourcing of the &lt;a href="https://www.wikidata.org/wiki/Q245748#sitelinks-wikipedia"&gt;Random Forest&lt;/a&gt; algorithm used by &lt;a href="https://en.wikipedia.org/wiki/User:SuggestBot"&gt;SuggestBot&lt;/a&gt;. SuggestBot is an opt-in recommender to Wikipedia editors, offering pages that need work which look like pages they've worked on before. Similarly, with this package, you get a function that accepts a string of &lt;a href="https://en.wikipedia.org/wiki/Wikipedia:Wikitext"&gt;wikitext&lt;/a&gt;, and returns a &lt;a href="https://en.wikipedia.org/wiki/Wikipedia:Quality_scale#Grades"&gt;Wikipedia Class ('Stub', 'C-Class', 'Featured Article', etc.)&lt;/a&gt;. Wiki-class is currently in &lt;code&gt;alpha&lt;/code&gt; according to its packager and developer [[\@halfak]]{.citation}(https://twitter.com/halfak), and although I had to make a few patches to get some examples to work, it's ready to start classifying your wikitext.&lt;/p&gt;
&lt;h1&gt;Overview&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Setting it up on Ubuntu.&lt;/li&gt;
&lt;li&gt;Testing the batteries-included model.&lt;/li&gt;
&lt;li&gt;Using the output by introducing a closeness measure.&lt;/li&gt;
&lt;li&gt;Testing making our own model.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Setup&lt;/h2&gt;
&lt;p&gt;At first you may be frustrated to learn that Wiki-Class is Python 3 only. You'll not be able to mix it with pywikibot, which is Python 2.7 only, and that can also mean upgrading some of your other tools. However just try to recall these update gripes next time you encounter a UnicodeError in Python 2.x; and then be thankful to Halfak for making us give Python 3 a try. I outline getting the environment running in Ubuntu 14.04 here.&lt;/p&gt;
&lt;p&gt;Firstly, if you want to use the Ipython notebook with python3 you can do so with &lt;code&gt;apt-get&lt;/code&gt;. And while we're at it, for convenince we'll also install another version of pip for Python 3.
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .vbox}
::: {.input .hbox}
::: {.prompt .input_prompt}
In [95]:
:::&lt;/p&gt;
&lt;p&gt;::: {.input_area .box-flex1}
::: {.highlight}
    !sudo apt-get install ipython3-notebook python3-pip
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.vbox .output_wrapper}
::: {.output .vbox}
::: {.hbox .output_area}
::: {.prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.box-flex1 .output_subarea .output_stream .output_stdout}
    [sudo] password for notconfusing: 
:::
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.text_cell_render .border-box-sizing .rendered_html}
Some requirements of Wiki-class, including sklearn, and nltk, which are a pain with Python 3 since they haven't been properly packaged for it yet. So these you'll have to get from source:
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .vbox}
::: {.input .hbox}
::: {.prompt .input_prompt}
In [1]:
:::&lt;/p&gt;
&lt;p&gt;::: {.input_area .box-flex1}
::: {.highlight}
    !pip3 install git+https://github.com/scikit-learn/scikit-learn.git
    !pip3 install git+https://github.com/nltk/nltk/#
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.text_cell_render .border-box-sizing .rendered_html}
Making some random pages for a test dataset&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;We'll need to get some Wikitext, with associated classifications, to start testing. I elected to make a random datasetin pywikibot, which as already stated is Python 2.7 only, and thus needs to be in a separate notebook, you can &lt;a href="http://nbviewer.ipython.org/github/notconfusing/Wiki-Class/blob/master/Data%20Download%20Random%20Pages%20with%20Class%20python2.ipynb"&gt;view it on the nbviewer still&lt;/a&gt;. Its output is a file &lt;code&gt;test_class_data.json&lt;/code&gt; &lt;a href="https://github.com/notconfusing/Wiki-Class/blob/master/test_class_data.json.bz2"&gt;(github link of the bzip)&lt;/a&gt; which is just a dictionary associating qualities and page-texts.&lt;/p&gt;
&lt;p&gt;Warning, this dataset has some examples that can cause a &lt;code&gt;ZeroDivisonError&lt;/code&gt; because some of these pages have 0 non-mark-up text. I wrote &lt;a href="https://github.com/halfak/Wiki-Class/pull/5"&gt;this patch&lt;/a&gt; which fixes this issue.
:::&lt;/p&gt;
&lt;p&gt;::: {.text_cell_render .border-box-sizing .rendered_html}
Testing the Pre-built Model
===========================
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .vbox}
::: {.input .hbox}
::: {.prompt .input_prompt}
In [3]:
:::&lt;/p&gt;
&lt;p&gt;::: {.input_area .box-flex1}
::: {.highlight}
    import json
    import pandas as pd
    from wikiclass.models import RFTextModel
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.vbox .output_wrapper}
::: {.output .vbox}
::: {.hbox .output_area}
::: {.prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.box-flex1 .output_subarea .output_stream .output_stderr}
    /usr/local/lib/python3.4/dist-packages/pandas/io/excel.py:626: UserWarning: Installed openpyxl is not supported at this time. Use &amp;gt;=1.6.1 and &amp;lt;2.0.0.
      .format(openpyxl_compat.start_ver, openpyxl_compat.stop_ver))
:::
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.text_cell_render .border-box-sizing .rendered_html}
Each model is stored in a &lt;code&gt;.model&lt;/code&gt; file. A default one is included in the github repo.
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .vbox}
::: {.input .hbox}
::: {.prompt .input_prompt}
In []:
:::&lt;/p&gt;
&lt;p&gt;::: {.input_area .box-flex1}
::: {.highlight}
    !wget https://github.com/halfak/Wiki-Class/blob/master/models/enwiki.rf_text.model?raw=true
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .vbox}
::: {.input .hbox}
::: {.prompt .input_prompt}
In [35]:
:::&lt;/p&gt;
&lt;p&gt;::: {.input_area .box-flex1}
::: {.highlight}
    !mv enwiki.rf_text.model\?raw\=true enwiki.rf_text.model
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.text_cell_render .border-box-sizing .rendered_html}
Now we load the model.
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .vbox}
::: {.input .hbox}
::: {.prompt .input_prompt}
In [4]:
:::&lt;/p&gt;
&lt;p&gt;::: {.input_area .box-flex1}
::: {.highlight}
    model = RFTextModel.from_file(open("enwiki.rf_text.model",'rb'))
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .vbox}
::: {.input .hbox}
::: {.prompt .input_prompt}
In [5]:
:::&lt;/p&gt;
&lt;p&gt;::: {.input_area .box-flex1}
::: {.highlight}
    classed_items = json.load(open('test_class_data.json','r'))
    print(sum([len(l) for l in classed_items.values()]))
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.vbox .output_wrapper}
::: {.output .vbox}
::: {.hbox .output_area}
::: {.prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.box-flex1 .output_subarea .output_stream .output_stdout}
    38959
:::
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.text_cell_render .border-box-sizing .rendered_html}
The Wiki-Class-provided model only deals with &lt;em&gt;'Stub', 'Start', 'B', 'C', 'Good Article'&lt;/em&gt;, and &lt;em&gt;'Featured Article'&lt;/em&gt; classifications. It does not include not &lt;em&gt;'List', 'Featured List',&lt;/em&gt; or &lt;em&gt;'Disambig'&lt;/em&gt; class pages. So we have to sort out the standard classes out of our 38,000 test articles.
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .vbox}
::: {.input .hbox}
::: {.prompt .input_prompt}
In [6]:
:::&lt;/p&gt;
&lt;p&gt;::: {.input_area .box-flex1}
::: {.highlight}
    standards = {actual: text for actual, text in classed_items.items() if actual in ['Stub', 'Start', 'C', 'B', 'GA', 'FA'] }
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .vbox}
::: {.input .hbox}
::: {.prompt .input_prompt}
In [5]:
:::&lt;/p&gt;
&lt;p&gt;::: {.input_area .box-flex1}
::: {.highlight}
    print(sum([len(l) for l in standards.values()]))
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.vbox .output_wrapper}
::: {.output .vbox}
::: {.hbox .output_area}
::: {.prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.box-flex1 .output_subarea .output_stream .output_stdout}
    36873
:::
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.text_cell_render .border-box-sizing .rendered_html}
Now we iterate over our 36,000 standard-class pages, and put their Wiki-Class assessments into a DataFrame.
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .vbox}
::: {.input .hbox}
::: {.prompt .input_prompt}
In [6]:
:::&lt;/p&gt;
&lt;p&gt;::: {.input_area .box-flex1}
::: {.highlight}
    accuracy_df = pd.DataFrame(index=classed_items.keys(), columns=['actual','correct', 'model_prob', 'actual_prob'])
    for actual, text_list in standards.items():
        #see if actual is even here, otherwise no fair comparison
            for text in text_list:
                try:
                    assessment, probabilities = model.classify(text)
                except ZeroDivisionError:
                    continue
                    #print(actual, text)
                accuracy_df = accuracy_df.append({'actual': actual,
                                                  'correct':int(assessment == actual),
                                                  'model_prob': probabilities[assessment],
                                                  'actual_prob': probabilities[actual]}, ignore_index=True)
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.text_cell_render .border-box-sizing .rendered_html}
What you see here is that the output of an assessment is really two things. The &lt;em&gt;'assessment'&lt;/em&gt; which is simply the &lt;em&gt;'class'&lt;/em&gt; which the algorithm predicts best, but secondly a &lt;em&gt;dictionary of probablities&lt;/em&gt; of how likely the text is to belong to each class.&lt;/p&gt;
&lt;p&gt;In our DataFrame we record four data. The &lt;em&gt;'actual'&lt;/em&gt; class as Wikipedia classes it; whether the actual class matches the model prediction. The probabilty (read: "confidence") of the model prediction. And lastly the probability of the actual class. Note in the "correct" case &lt;code&gt;model_prob&lt;/code&gt; and &lt;code&gt;actual_prob&lt;/code&gt; are the same.
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .vbox}
::: {.input .hbox}
::: {.prompt .input_prompt}
In [7]:
:::&lt;/p&gt;
&lt;p&gt;::: {.input_area .box-flex1}
::: {.highlight}
    df  = accuracy_df.dropna(how='all')
    df.head()
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.vbox .output_wrapper}
::: {.output .vbox}
::: {.hbox .output_area}
::: {.prompt .output_prompt}
Out[7]:
:::&lt;/p&gt;
&lt;p&gt;::: {.box-flex1 .output_subarea .output_pyout}
::: {.output_html .rendered_html}
::: {style="max-height: 1000px; max-width: 1500px; overflow: auto;"}
       actual   correct   model_prob   actual_prob&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;18   Start    0         0.4           0.0
  19   Start    1         0.8           0.8
  20   Start    0         0.4           0.0
  21   Start    0         1.0           0.0
  22   Start    1         0.7           0.7
:::
:::
:::
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.text_cell_render .border-box-sizing .rendered_html}
If we look at the &lt;code&gt;correct&lt;/code&gt; mean averages we should hopefully see something above 1/6th, which would be the performance of just guessing. Which we do.
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .vbox}
::: {.input .hbox}
::: {.prompt .input_prompt}
In [8]:
:::&lt;/p&gt;
&lt;p&gt;::: {.input_area .box-flex1}
::: {.highlight}
    groups = df.groupby(by='actual')
    groups['correct'].mean()
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.vbox .output_wrapper}
::: {.output .vbox}
::: {.hbox .output_area}
::: {.prompt .output_prompt}
Out[8]:
:::&lt;/p&gt;
&lt;p&gt;::: {.box-flex1 .output_subarea .output_pyout}
    actual
    B         0.247391
    C         0.278138
    FA        0.854167
    GA        0.444444
    Start     0.387334
    Stub      0.698394
    Name: correct, dtype: float64
:::
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.text_cell_render .border-box-sizing .rendered_html}
See how "close" predications are if they are not correct.
=========================================================
:::&lt;/p&gt;
&lt;p&gt;::: {.text_cell_render .border-box-sizing .rendered_html}
Now we hack on the output. The Random Forest is really just binning text into difference classes, it doesn't know that some of the classes are closer to each other than others. Therefore we define a distance metric on the Standard Wiki classes. I call this order the &lt;em&gt;"Classic Order"&lt;/em&gt; To get an intuition, consider this example. If an article is a &lt;em&gt;Good Aritcle&lt;/em&gt; and the model prediction is also &lt;em&gt;Good Article&lt;/em&gt; then it is off by 0; if the model prediction is &lt;em&gt;Featured Article&lt;/em&gt; it is off off by 1; if the model prediction is &lt;em&gt;Start&lt;/em&gt; then it was off by 3.
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .vbox}
::: {.input .hbox}
::: {.prompt .input_prompt}
In [7]:
:::&lt;/p&gt;
&lt;p&gt;::: {.input_area .box-flex1}
::: {.highlight}
    classic_order = ['Stub', 'Start', 'C', 'B', 'GA', 'FA']
    enum_classic = enumerate(classic_order)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;for enum, classic in dict(enum_classic).items():&lt;/span&gt;
&lt;span class="err"&gt;    print(enum, classic)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.vbox .output_wrapper}
::: {.output .vbox}
::: {.hbox .output_area}
::: {.prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.box-flex1 .output_subarea .output_stream .output_stdout}
    0 Stub
    1 Start
    2 C
    3 B
    4 GA
    5 FA
:::
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.text_cell_render .border-box-sizing .rendered_html}
Now we are going to iterate over the same dataset as above, but instead of recording "correctness", we record the closesness in a DataFrame.
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .vbox}
::: {.input .hbox}
::: {.prompt .input_prompt}
In [8]:
:::&lt;/p&gt;
&lt;p&gt;::: {.input_area .box-flex1}
::: {.highlight}
    classic_order = ['Stub', 'Start', 'C', 'B', 'GA', 'FA']
    classic_dict = dict(zip(classic_order, range(len(classic_order))))&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;off_by_df&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;classed_items&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=[&lt;/span&gt;&lt;span class="n"&gt;&amp;#39;actual&amp;#39;,&amp;#39;off_by&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;classic&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ow"&gt;in&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nl"&gt;classic_order&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nc"&gt;text&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ow"&gt;in&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;standards&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;classic&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;            &lt;/span&gt;&lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;                &lt;/span&gt;&lt;span class="n"&gt;assessment&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;probabilities&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;classify&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nc"&gt;text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;            &lt;/span&gt;&lt;span class="ow"&gt;except&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nl"&gt;ZeroDivisionError&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;                &lt;/span&gt;&lt;span class="k"&gt;continue&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;                &lt;/span&gt;&lt;span class="n"&gt;#print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;actual&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nc"&gt;text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;            &lt;/span&gt;&lt;span class="n"&gt;off_by_df&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;off_by_df&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;actual&amp;#39;&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;classic&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;                                              &lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;off_by&amp;#39;&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="nf"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;classic_dict&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;assessment&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;classic_dict&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;classic&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="err"&gt;}&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;ignore_index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.text_cell_render .border-box-sizing .rendered_html}
So it should look something like this as a table
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .vbox}
::: {.input .hbox}
::: {.prompt .input_prompt}
In [9]:
:::&lt;/p&gt;
&lt;p&gt;::: {.input_area .box-flex1}
::: {.highlight}
    off_by  = off_by_df.dropna(how='all')
    off_by.head()
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.vbox .output_wrapper}
::: {.output .vbox}
::: {.hbox .output_area}
::: {.prompt .output_prompt}
Out[9]:
:::&lt;/p&gt;
&lt;p&gt;::: {.box-flex1 .output_subarea .output_pyout}
::: {.output_html .rendered_html}
::: {style="max-height: 1000px; max-width: 1500px; overflow: auto;"}
       actual   off_by&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;18   Stub     2
  19   Stub     1
  20   Stub     0
  21   Stub     0
  22   Stub     0
:::
:::
:::
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.text_cell_render .border-box-sizing .rendered_html}
And as a chart.
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .vbox}
::: {.input .hbox}
::: {.prompt .input_prompt}
In [10]:
:::&lt;/p&gt;
&lt;p&gt;::: {.input_area .box-flex1}
::: {.highlight}
    %pylab inline
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.vbox .output_wrapper}
::: {.output .vbox}
::: {.hbox .output_area}
::: {.prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.box-flex1 .output_subarea .output_stream .output_stdout}
    Populating the interactive namespace from numpy and matplotlib
:::
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.text_cell_render .border-box-sizing .rendered_html}
We can see that the middle classes are less easy to predict where as the ends are easier. This would corroborate our expectations. Since the the quality sprectrum bleed past these rather arbitrary cut-off points,ore of the quality specturm would lie in these intervals, and so its easier to bin them.
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .vbox}
::: {.input .hbox}
::: {.prompt .input_prompt}
In [11]:
:::&lt;/p&gt;
&lt;p&gt;::: {.input_area .box-flex1}
::: {.highlight}
    ax = off_by.groupby(by='actual',sort=False).mean().plot(title='Prediction Closeness by Quality Class', kind='bar', legend=False)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;ax.set_ylabel(&amp;#39;&amp;#39;&amp;#39;Prediction Closeness (lower is more accurate)&amp;#39;&amp;#39;&amp;#39;)&lt;/span&gt;
&lt;span class="err"&gt;ax.set_xlabel(&amp;#39;&amp;#39;&amp;#39;Quality Class&amp;#39;&amp;#39;&amp;#39;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.vbox .output_wrapper}
::: {.output .vbox}
::: {.hbox .output_area}
::: {.prompt .output_prompt}
Out[11]:
:::&lt;/p&gt;
&lt;p&gt;::: {.box-flex1 .output_subarea .output_pyout}
    &lt;matplotlib.text.Text at 0x7fc089810550&gt;
:::
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.text_cell_render .border-box-sizing .rendered_html}
Making a model
==============
:::&lt;/p&gt;
&lt;p&gt;::: {.text_cell_render .border-box-sizing .rendered_html}
Now we test the model-making feature. We will use our dataset of 'standards' from above, using a random 80% for training and 20% for testing.
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .vbox}
::: {.input .hbox}
::: {.prompt .input_prompt}
In [27]:
:::&lt;/p&gt;
&lt;p&gt;::: {.input_area .box-flex1}
::: {.highlight}
    from wikiclass.models import RFTextModel
    from wikiclass import assessments
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.text_cell_render .border-box-sizing .rendered_html}
Divvyig up our data into two lists.
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .vbox}
::: {.input .hbox}
::: {.prompt .input_prompt}
In [28]:
:::&lt;/p&gt;
&lt;p&gt;::: {.input_area .box-flex1}
::: {.highlight}
    import random&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;train_set&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;list&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;test_set&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;list&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;actual&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;text_list&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="n"&gt;standards&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nb"&gt;text&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="n"&gt;text_list&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;test_set&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;actual&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;train_set&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;actual&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_set&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_set&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.vbox .output_wrapper}
::: {.output .vbox}
::: {.hbox .output_area}
::: {.prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.box-flex1 .output_subarea .output_stream .output_stdout}
    0.2510772571506124
:::
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.text_cell_render .border-box-sizing .rendered_html}
And the next step is quite simple, we just click a button supplying our train_set list, and test by supplying our test_set list. Also the package conveniently supplies a saving function for us to store our model for later use.
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .vbox}
::: {.input .hbox}
::: {.prompt .input_prompt}
In [29]:
:::&lt;/p&gt;
&lt;p&gt;::: {.input_area .box-flex1}
::: {.highlight}
    # Train a model
    model = RFTextModel.train(
        train_set,
        assessments=assessments.WP10
    )&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;Run&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt; &lt;span class="k"&gt;set&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;print&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;results&lt;/span&gt;
&lt;span class="n"&gt;results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_set&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;results&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="k"&gt;Write&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="k"&gt;to&lt;/span&gt; &lt;span class="n"&gt;disk&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;reuse&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_file&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;36K_random_enwiki.rf_text.model&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;wb&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.vbox .output_wrapper}
::: {.output .vbox}
::: {.hbox .output_area}
::: {.prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.box-flex1 .output_subarea .output_stream .output_stdout}
    pred assessment    B    C  FA  GA  Start  Stub
    real assessment                             &lt;br&gt;
    B                130   29   1   5    105    40
    C                 34  112   0   2    151    33
    FA                 7    3   4   0      1     0
    GA                 8    8   0  11      9     1
    Start             80   87   0   2   1420   525
    Stub              40   32   0   0    547  3973
:::
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.text_cell_render .border-box-sizing .rendered_html}
Now to look at accuracy, we norm the DataFrame row-wise.
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .vbox}
::: {.input .hbox}
::: {.prompt .input_prompt}
In [30]:
:::&lt;/p&gt;
&lt;p&gt;::: {.input_area .box-flex1}
::: {.highlight}
    norm_results = results.apply(lambda col: col / col.sum(), axis=1)
    norm_results
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.vbox .output_wrapper}
::: {.output .vbox}
::: {.hbox .output_area}
::: {.prompt .output_prompt}
Out[30]:
:::&lt;/p&gt;
&lt;p&gt;::: {.box-flex1 .output_subarea .output_pyout}
::: {.output_html .rendered_html}
::: {style="max-height: 1000px; max-width: 1500px; overflow: auto;"}&lt;/p&gt;
&lt;table class="dataframe" border="1"&gt;

&lt;thead&gt;


&lt;tr style="text-align: right;"&gt;


&lt;th&gt;

pred assessment
:::
:::
:::
:::
:::
:::
:::


&lt;/th&gt;


&lt;th&gt;

B


&lt;/th&gt;


&lt;th&gt;

C


&lt;/th&gt;


&lt;th&gt;

FA


&lt;/th&gt;


&lt;th&gt;

GA


&lt;/th&gt;


&lt;th&gt;

Start


&lt;/th&gt;


&lt;th&gt;

Stub


&lt;/th&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

real assessment


&lt;/th&gt;


&lt;th&gt;


&lt;/th&gt;


&lt;th&gt;


&lt;/th&gt;


&lt;th&gt;


&lt;/th&gt;


&lt;th&gt;


&lt;/th&gt;


&lt;th&gt;


&lt;/th&gt;


&lt;th&gt;


&lt;/th&gt;


&lt;/tr&gt;


&lt;/thead&gt;


&lt;tbody&gt;


&lt;tr&gt;


&lt;th&gt;

B


&lt;/th&gt;


&lt;td&gt;

0.419355


&lt;/td&gt;


&lt;td&gt;

0.093548


&lt;/td&gt;


&lt;td&gt;

0.003226


&lt;/td&gt;


&lt;td&gt;

0.016129


&lt;/td&gt;


&lt;td&gt;

0.338710


&lt;/td&gt;


&lt;td&gt;

0.129032


&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

C


&lt;/th&gt;


&lt;td&gt;

0.102410


&lt;/td&gt;


&lt;td&gt;

0.337349


&lt;/td&gt;


&lt;td&gt;

0.000000


&lt;/td&gt;


&lt;td&gt;

0.006024


&lt;/td&gt;


&lt;td&gt;

0.454819


&lt;/td&gt;


&lt;td&gt;

0.099398


&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

FA


&lt;/th&gt;


&lt;td&gt;

0.466667


&lt;/td&gt;


&lt;td&gt;

0.200000


&lt;/td&gt;


&lt;td&gt;

0.266667


&lt;/td&gt;


&lt;td&gt;

0.000000


&lt;/td&gt;


&lt;td&gt;

0.066667


&lt;/td&gt;


&lt;td&gt;

0.000000


&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

GA


&lt;/th&gt;


&lt;td&gt;

0.216216


&lt;/td&gt;


&lt;td&gt;

0.216216


&lt;/td&gt;


&lt;td&gt;

0.000000


&lt;/td&gt;


&lt;td&gt;

0.297297


&lt;/td&gt;


&lt;td&gt;

0.243243


&lt;/td&gt;


&lt;td&gt;

0.027027


&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

Start


&lt;/th&gt;


&lt;td&gt;

0.037843


&lt;/td&gt;


&lt;td&gt;

0.041154


&lt;/td&gt;


&lt;td&gt;

0.000000


&lt;/td&gt;


&lt;td&gt;

0.000946


&lt;/td&gt;


&lt;td&gt;

0.671712


&lt;/td&gt;


&lt;td&gt;

0.248344


&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

Stub


&lt;/th&gt;


&lt;td&gt;

0.008711


&lt;/td&gt;


&lt;td&gt;

0.006969


&lt;/td&gt;


&lt;td&gt;

0.000000


&lt;/td&gt;


&lt;td&gt;

0.000000


&lt;/td&gt;


&lt;td&gt;

0.119120


&lt;/td&gt;


&lt;td&gt;

0.865200


&lt;/td&gt;


&lt;/tr&gt;


&lt;/tbody&gt;


&lt;/table&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;::: {.text_cell_render .border-box-sizing .rendered_html}
And finally we can view the peformance by class, which intriguingly seems to be better than what we got with the batteries-included model.
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .vbox}
::: {.input .hbox}
::: {.prompt .input_prompt}
In [35]:
:::&lt;/p&gt;
&lt;p&gt;::: {.input_area .box-flex1}
::: {.highlight}
    for c in classic_order:
        print(c, norm_results.loc[c][c])
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.vbox .output_wrapper}
::: {.output .vbox}
::: {.hbox .output_area}
::: {.prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.box-flex1 .output_subarea .output_stream .output_stdout}
    Stub 0.865200348432
    Start 0.671712393567
    C 0.33734939759
    B 0.41935483871
    GA 0.297297297297
    FA 0.266666666667
:::
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.text_cell_render .border-box-sizing .rendered_html}
We can see that, having a large number of stubs to train on really gives us a high precision in classifying them.&lt;/p&gt;
&lt;p&gt;So there you have it - a brief playing around with Wiki-Class, an easy way to get rough quality estimates out of your data. If you extend any more examples of using this class, I'd be intrigued to see and collaborate on them.&lt;/p&gt;
&lt;p&gt;‽[[\@notconusing]]{.citation}(https://twitter.com/notconfusing)
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .vbox}
::: {.input .hbox}
::: {.prompt .input_prompt}
In []:
:::&lt;/p&gt;
&lt;p&gt;::: {.input_area .box-flex1}
:::
:::
:::&lt;/p&gt;</content><category term="hacking"></category></entry><entry><title>Method of Reflections: Explained and Exampled in Python</title><link href="https://notconfusing.com/method-of-reflections-explained-and-exampled-in-python.html" rel="alternate"></link><published>2014-06-09T06:53:00-07:00</published><updated>2014-06-09T06:53:00-07:00</updated><author><name>max</name></author><id>tag:notconfusing.com,2014-06-09:/method-of-reflections-explained-and-exampled-in-python.html</id><summary type="html">&lt;p&gt;[The introduction of post is mirrored here, but the full tutorial is &lt;a href="http://nbviewer.ipython.org/github/notconfusing/wiki_econ_capability/blob/master/Method%20of%20Reflections%20Explained%20and%20Exampled.ipynb"&gt;on IPython Notebook Viewer&lt;/a&gt;.]{style="font-size: 18pt;"}&lt;/p&gt;
&lt;p&gt;::: {#notebook .border-box-sizing tabindex="-1"}
::: {#notebook-container .container}
::: {.cell .border-box-sizing .text_cell .rendered}
::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}
Method of Reflections Explained and Exampled in Python
======================================================&lt;/p&gt;
&lt;p&gt;&lt;a href="https://notconfusing.com/images/uploads/2014/06/example_evolution.png"&gt;&lt;img alt="See how the Method of Reflections evolves as a recursive process." class="size-medium wp-image-683" src="https://notconfusing.com/images/uploads/2014/06/example_evolution.png" style="width:295px !important"&gt;&lt;/a&gt; See how the Method of Reflections evolves as a …&lt;/p&gt;</summary><content type="html">&lt;p&gt;[The introduction of post is mirrored here, but the full tutorial is &lt;a href="http://nbviewer.ipython.org/github/notconfusing/wiki_econ_capability/blob/master/Method%20of%20Reflections%20Explained%20and%20Exampled.ipynb"&gt;on IPython Notebook Viewer&lt;/a&gt;.]{style="font-size: 18pt;"}&lt;/p&gt;
&lt;p&gt;::: {#notebook .border-box-sizing tabindex="-1"}
::: {#notebook-container .container}
::: {.cell .border-box-sizing .text_cell .rendered}
::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}
Method of Reflections Explained and Exampled in Python
======================================================&lt;/p&gt;
&lt;p&gt;&lt;a href="https://notconfusing.com/images/uploads/2014/06/example_evolution.png"&gt;&lt;img alt="See how the Method of Reflections evolves as a recursive process." class="size-medium wp-image-683" src="https://notconfusing.com/images/uploads/2014/06/example_evolution.png" style="width:295px !important"&gt;&lt;/a&gt; See how the Method of Reflections evolves as a recursive process.&lt;/p&gt;
&lt;p&gt;The Method of Reflection (MOR) is a algorithm first coming out of macroeconomics, that ranks nodes in a bi-partite network. This notebook should hopefully help you implement the &lt;em&gt;method of reflection&lt;/em&gt; in python. To be precise, it is the modified algorithm that is proposed by Caldarelli et al., which solves some problems with the original Hidalgo-Hausmann (HH) algorithm &lt;a href="http://chidalgo.com/Papers/HidalgoHausmann_PNAS_2009_PaperAndSM.pdf"&gt;doi:10.1073/pnas.0900943106&lt;/a&gt;. The main problem with (HH) is that all values converge to a single fixed point after sufficiently many iterations. The Caldarelli version solves this by adding a new term to the recursive equation - what they call a &lt;em&gt;biased random walker&lt;/em&gt; (function &lt;em&gt;G&lt;/em&gt;). &lt;a href="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0047278"&gt;doi: 10.1371/journal.pone.0047278&lt;/a&gt; . I hadn't seen any open-source implementations of this algorithm, so I thought I'd share my naïve approach.
:::
:::
:::&lt;/p&gt;
&lt;p&gt;Read on at &lt;a href="http://nbviewer.ipython.org/github/notconfusing/wiki_econ_capability/blob/master/Method%20of%20Reflections%20Explained%20and%20Exampled.ipynb"&gt;http://nbviewer.ipython.org/github/notconfusing/wiki_econ_capability/blob/master/Method%20of%20Reflections%20Explained%20and%20Exampled.ipynb&lt;/a&gt;
:::
:::&lt;/p&gt;</content><category term="hacking"></category></entry><entry><title>Profiles of Inspiring Wikimedians I Met at Wikiconference USA 2014</title><link href="https://notconfusing.com/profiles-of-inspiring-wikimedians-i-met-at-wikiconference-usa-2014.html" rel="alternate"></link><published>2014-06-06T16:55:00-07:00</published><updated>2014-06-06T16:55:00-07:00</updated><author><name>max</name></author><id>tag:notconfusing.com,2014-06-06:/profiles-of-inspiring-wikimedians-i-met-at-wikiconference-usa-2014.html</id><summary type="html">&lt;p&gt;&lt;a href="http://wikiconferenceusa.org/"&gt;Wikiconference USA 2014&lt;/a&gt;, in New York, just finished, and more than usual this conference instilled in me a lot of motivating social energy. Yes, I did present there, twice, on "&lt;a href="http://wikiconferenceusa.org/wiki/Submissions:Answering_Big_Questions_With_Wikidata"&gt;Answering Big Questions With Wikidata&lt;/a&gt;", and "&lt;a href="http://wikiconferenceusa.org/wiki/Submissions:Signalling_Open_Access_References"&gt;Signalling Open Access References&lt;/a&gt;," but more so than usual I &lt;em&gt;enjoyed&lt;/em&gt; attending other presentations …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="http://wikiconferenceusa.org/"&gt;Wikiconference USA 2014&lt;/a&gt;, in New York, just finished, and more than usual this conference instilled in me a lot of motivating social energy. Yes, I did present there, twice, on "&lt;a href="http://wikiconferenceusa.org/wiki/Submissions:Answering_Big_Questions_With_Wikidata"&gt;Answering Big Questions With Wikidata&lt;/a&gt;", and "&lt;a href="http://wikiconferenceusa.org/wiki/Submissions:Signalling_Open_Access_References"&gt;Signalling Open Access References&lt;/a&gt;," but more so than usual I &lt;em&gt;enjoyed&lt;/em&gt; attending other presentations. On reflecting why that was, I came to realize it was the earnest authentic effort of other Wikimedians, that shone so brightly. These are some of the more inspiring characters from the conference, but by no means a complete list.&lt;/p&gt;
&lt;h2&gt;Sumana H[arihareswara]{.gD}&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.harihareswara.net/"&gt;http://www.harihareswara.net/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://twitter.com/brainwane"&gt;\@brainwane&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[![Sumana: Attribution https://commons.wikimedia.org/wiki/Category: hacking&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Sumana gave the opening keynote wherein she talked about implicit versus explicit exclusion. To introduce the subject she told of her positive experience at &lt;a href="https://www.hackerschool.com/"&gt;Hacker School&lt;/a&gt; which does actively exclude some people (there's an application process), but as a result makes a more intentionally inviting space. That is because only inviting, inclusive individuals are selected for Hacker School. As she related this to Wikipedia, the shortcomings of our &lt;em&gt;emphasis on liberty&lt;/em&gt; highlighted, perhaps it doesn't ensure a safe learning space. A key quote that summed this was "in the Wikimedia community, since we don't exclude anyone explicitly, we exclude others implicitly [sic]." Strong free speech defence is not muting some overbearing voices. A full &lt;a href="http://wikiconferenceusa.org/wiki/Sumana_Harihareswara_keynote"&gt;transcript of the talk is available.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I particularly became aware of &lt;a href="https://www.hackerschool.com/manual#sub-sec-social-rules"&gt;Hacker School's "no well-actually's"&lt;/a&gt; rule that Sumana presented. Many times during the conference, when someone was doing something in Python that had a technical side-note I wanted to slot into the conversation, but being newly aware of how this is disruptive, I simply allowed the real learning to continue. Sumana very much practiced this too, as I witnessed when someone came up to her to her to talk of her discomfort about someone who was wearing Google Glass at the conference. She without pause jumped to help, offering to go with the privacy advocate to find a conference organizer without any judgement on the camera-controversy itself.&lt;/p&gt;
&lt;p&gt;Sumana also gave an impromptu Gender Diversity training, which came from &lt;a href="https://adainitiative.org/what-we-do/workshops-and-training/"&gt;Ada initiative&lt;/a&gt;. Actually this was offered twice and I attended both sessions (and it was my third time since I'd watched it online). Sumana's rapid-fire style resonates with my personality and preferred learning style very well.  This allows me to really synch-up with the lesson and download the content with high-mental-bandwidth. In general Sumana is an over-clocked, but liquid-cooled processor, which is brilliant if you are too, and have fibre-optic connection.&lt;/p&gt;
&lt;p&gt;In Zürich we were going to work on a python project together because we were both talking about wanting to pair-program python on twitter. Then we realized we didn't want to work on the same project. I was really impressed with her straightforward, unpretentious communication when she said "It doesn't seem like want to work on the same project - so perhaps another time." The combination of directness and openness is liberating. Often we see one without the other, but having both is a fantastic combination.&lt;/p&gt;
&lt;p&gt;And she is the mentor of:&lt;/p&gt;
&lt;h2&gt;Frances Hocutt&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://franceshocutt.com/"&gt;http://franceshocutt.com/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://notconfusing.com/images/uploads/2014/06/frances.png"&gt;&lt;img alt="Frances also has quite inspiring hair. Attribution: https://twitter.com/franceshocutt" class="size-full wp-image-650" src="https://notconfusing.com/images/uploads/2014/06/frances.png" style="width:297px !important"&gt;&lt;/a&gt; Frances also has quite inspiring hair. &lt;a href="https://twitter.com/franceshocutt"&gt;Attribution&lt;/a&gt;[/caption]&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Frances gave a walkthrough and workshop on the MediaWiki API. I actually interact with the API a lot, but never directly only through pywikibot, so I was much enlightened by this lesson. In fact Frances explained with great care and deliberateness, from step zero, what is an API, all the way to the specifics of the MediaWiki API and how to use a client library. Frances' teaching style is methodical. The pace is never frantic, taking time to get every word right, never needing to allow herself extra time with "er" or "um". Learning from Frances is like having an immaculate syllogism patiently unfurl in front of you.&lt;/p&gt;
&lt;p&gt;She also did the brave thing of giving a live demo of &lt;a href="https://pypi.python.org/pypi/mwclient/0.6.5"&gt;mwclient&lt;/a&gt;, starting from pip installation. Which was great to learn because I am only familiar with its not-quite-competitor &lt;a href="https://www.mediawiki.org/wiki/Manual:Pywikibot"&gt;pywikibot&lt;/a&gt;. So she both didn't assume any technical knowledge, but didn't leave experienced programmers bored, which is a hard balance to strike. This is &lt;a href="http://franceshocutt.com/2014/06/02/wikiconference-usa-2014-rundown/"&gt;her blog&lt;/a&gt; about her presentation including links to her slides.&lt;/p&gt;
&lt;p&gt;Frances also taught the same Gender Diversity Training aimed at cis-men, which I attended. It was in this reprise that I most caught the proverbial advice -  "to follow your discomfort."&lt;/p&gt;
&lt;p&gt;Finally I'd like to credit her Chemistry knowledge and quick wit. In my previous &lt;a href="http://notconfusing.com/sex-ratios-in-wikidata-part-iii/" title="Sex Ratios in Wikidata Part III"&gt;blog post about sex ratios&lt;/a&gt;, I mentioned I'd found an occurrence of "sodium" for a sex. Frances quickly Sherlock Holmes'd that this was likely because someone had tried to enter &lt;em&gt;not applicable&lt;/em&gt; - "na", and probably received the auto-suggest chemical element.&lt;/p&gt;
&lt;h2&gt;Joelle fleurantin&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://notconfusing.com/images/uploads/2014/06/joel.png"&gt;&lt;img alt="Joelle reflecting. Attribution: http://fleurantin.cc/" class="wp-image-649 size-medium" src="https://notconfusing.com/images/uploads/2014/06/joel.png" style="width:300px !important"&gt;&lt;/a&gt; Joelle reflecting. &lt;a href="%20http://fleurantin.cc/"&gt;Attribution&lt;/a&gt;[/caption]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://twitter.com/joasqueeniebee"&gt;\@joasqueenibee&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://fleurantin.cc/"&gt;http://fleurantin.cc/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At the conference Joelle gave a lightning talk that I enjoyed about her involvement in improving the Mozilla wiki, at which she has an internship. One day, she said, she became curious about the wiki's usage statistics, but could not find anything more than minimal information that was contained in maintenance reports. So she has started building some scripts to analyse and visualize the Mozilla wiki's usage.&lt;/p&gt;
&lt;p&gt;Later in her lightning talk she also discussed her own autodidactic learning techniques, where she told of being a big recorded-conference-video watcher. Joelle has a particular penchant for linux.com, she shared. Therefore her being part of Gnome's Outreach Program for Women should come as no surprise. As part of her Gnome involvement, Joelle fuzzes 0MQ, a stress-test debugging techinque as she patiently explained to me.&lt;/p&gt;
&lt;p&gt;Over a beer in Brooklyn later on, she was coaxed to shed some immodesty and recap the tech-art piece that she'd made. It is a interactive installation where one wears headphones listening to a monologue of a woman talking about her inner thoughts and as you approach a video screen a proximity sensor tracks you. The closer you get to the screen, the video changes to reveal increasingly more intimate footage.&lt;/p&gt;
&lt;h2&gt;Megan Wacha&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://notconfusing.com/images/uploads/2014/06/megan.jpeg"&gt;&lt;img alt="Megan on the Left. Attribution https://twitter.com/Museocat/status/472387217298825216/photo/1" class="wp-image-647 size-medium" src="https://notconfusing.com/images/uploads/2014/06/megan.jpeg" style="width:223px !important"&gt;&lt;/a&gt; Megan on the Left. &lt;a href="https://twitter.com/Museocat/status/472387217298825216/photo/1"&gt;Attribution&lt;/a&gt;[/caption]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://twitter.com/mew687"&gt;\@mew687&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.meganwacha.info/"&gt;http://www.meganwacha.info/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Megan is the Research and Instruction Librarian for the Performing Arts at Barnard College of Columbia University. Her presentation at the conference was about the multiplicities of roles for Librarians in Wikipedia. Regretfully because of scheduling I couldn't attend it. She however attended mine and &lt;a href="https://twitter.com/wrought"&gt;Wrought's&lt;/a&gt; &lt;a href="http://wikiconferenceusa.org/wiki/Submissions:Signalling_Open_Access_References"&gt;Signalling Open Access&lt;/a&gt; talk, and amazed from the Q and A. In a debate about whether it is it overcomplicating to import Open Access articles to Wikisource, as there may be corrections or retractions published, she noticed the more general problem. This was the first time I heard someone say "I'm going to bring this up with MLA." Her reasonable position is that "we should really be citing the used-source and not the original publication." I didn't even know you could take issue with MLA.&lt;/p&gt;
&lt;p&gt;During a lightning talk about there not being enough video in Wikipedia, a list of high profile articles without videos were cited, "Racing, Soccer, Dance." On the word dance, with a large hacker-confidence she leaned over to me and said "we're going to fix that." What an assertion,  and I believe it because of her other on-wiki work. Do you know the &lt;a href="https://en.wikipedia.org/wiki/Ntozake_Shange"&gt;Ntozake Shange&lt;/a&gt; article? Well its existence is owed to the inclusion of particularly hard to find sources - which is her speciality.&lt;/p&gt;
&lt;p&gt;Two last special mentions, that I didn't get enough time to know well, but want to hat-tip.&lt;/p&gt;
&lt;h2&gt;Dorothy Howard&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://twitter.com/DorothyR_Howard"&gt;\@DorothyR_Howard&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Is currently working at the Metropolitan Library Council, as Wikipedian in residence.&lt;br&gt;
Endearingly to me, she promotes the Wikipedia &lt;a href="https://en.wikipedia.org/wiki/Wikipedia:Authority_control"&gt;Authority Control project&lt;/a&gt;, which is easy to enjoy since it aggrandizes the work with &lt;a href="http://hangingtogether.org/?p=2306"&gt;VIAFbot.&lt;/a&gt; But this is also part of a holistic effort of hers to be a sort of techno-evangelist for a lot of wiki-library projects, and anyone that is in that space knows its presence is hotting up.&lt;/p&gt;
&lt;h2&gt;Jennifer Baek&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://twitter.com/jenbaek"&gt;\@jenbaek&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Has been involved in SFC for a long time, Wrought told me that he remembers meeting her in 2008 in Berkeley. Apparently since then she has not let up. She was the main conference organizer, and fire-putter-outer. When I was accidentally double-booked (to speak in two places at once) she coolly helped make sane the logistics. Thank you for making the conference happen.&lt;/p&gt;</content><category term="hacking"></category></entry><entry><title>Sex Ratios in Wikidata Part III</title><link href="https://notconfusing.com/sex-ratios-in-wikidata-part-iii.html" rel="alternate"></link><published>2014-05-21T11:00:00-07:00</published><updated>2014-05-21T11:00:00-07:00</updated><author><name>max</name></author><id>tag:notconfusing.com,2014-05-21:/sex-ratios-in-wikidata-part-iii.html</id><summary type="html">&lt;p&gt;::: {#notebook .border-box-sizing tabindex="-1"}
::: {#notebook-container .container}
::: {.cell .border-box-sizing .text_cell .rendered}
::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}
[For a better reading experience read &lt;a href="http://nbviewer.ipython.org/github/notconfusing/wikidataSex/blob/master/ratio_analysis/Sex%20Ratios%20By%20Language%20May%202014.ipynb"&gt;this post in the IPython Notebook Viewer&lt;/a&gt;.]{style="font-size: 18pt;"}&lt;/p&gt;
&lt;h2 id="Sex-Ratios-and-Wikidata-Part-III"&gt;Introduction&lt;a class="anchor-link" href="#Introduction"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .text_cell .rendered}
::: {.prompt .input_prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}
I recently got back from the &lt;a href="http://www.mediawiki.org/wiki/Z%C3%BCrich_Hackathon_2014"&gt;Mediawiki Hackathon in …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;::: {#notebook .border-box-sizing tabindex="-1"}
::: {#notebook-container .container}
::: {.cell .border-box-sizing .text_cell .rendered}
::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}
[For a better reading experience read &lt;a href="http://nbviewer.ipython.org/github/notconfusing/wikidataSex/blob/master/ratio_analysis/Sex%20Ratios%20By%20Language%20May%202014.ipynb"&gt;this post in the IPython Notebook Viewer&lt;/a&gt;.]{style="font-size: 18pt;"}&lt;/p&gt;
&lt;h2 id="Sex-Ratios-and-Wikidata-Part-III"&gt;Introduction&lt;a class="anchor-link" href="#Introduction"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .text_cell .rendered}
::: {.prompt .input_prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}
I recently got back from the &lt;a href="http://www.mediawiki.org/wiki/Z%C3%BCrich_Hackathon_2014"&gt;Mediawiki Hackathon in Zürich&lt;/a&gt;, where I was once again energized and inspired by the Wikidata Dev Team and Community. In chatting, they reminded me of analyses I ran in March 2013, &lt;a href="http://hangingtogether.org/?p=2877"&gt;Sex Ratios and Wikidata Parts I&lt;/a&gt; and &lt;a href="http://hangingtogether.org/?p=2986"&gt;II&lt;/a&gt;, about the state of a controversial Wikidata Property - &lt;a href="https://www.wikidata.org/wiki/Property:P21"&gt;P21&lt;/a&gt; a.k.a. "sex or gender". They suggested it was about time to reinvestigate.&lt;/p&gt;
&lt;p&gt;Since Part I and II a lot has happened: the property has been renamed (from "sex" to "sex or gender"), it's database constraints have been changed (from 3 accepted values to &lt;a href="https://www.wikidata.org/wiki/Property_talk:P21"&gt;13&lt;/a&gt;), and of course Wikidata has continued to proliferate (now about 400 million triples).&lt;/p&gt;
&lt;p&gt;Therefore a few questions are begged:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;What are the currently used values of 'sex or gender', and their ratios in each language?&lt;/li&gt;
&lt;li&gt;How does May 2014 data compare to a year ago?&lt;/li&gt;
&lt;li&gt;What are the most represented neither 'male' nor 'female' 'sex or gender's?&lt;ol&gt;
&lt;li&gt;And which languages use them most?&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Per used sex value, what are the average number of accompanying properties?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;That last question is not like the others, but comes from exploring the new &lt;a href="https://www.mediawiki.org/wiki/Wikidata_Toolkit"&gt;Wikidata Toolkit&lt;/a&gt;, a library for parsing Wikidata dumps. Trying to stretch the imagination of what can be done with Wikidata data is a new hobby of mine, and I am giving a talk about it at Wikiconference USA, titled &lt;a href="https://meta.wikimedia.org/wiki/WikiConference_USA#Schedule_of_sessions"&gt;"Answering Big Questions with Wikidata"&lt;/a&gt;. For now the Wikidata Toolkit is at version 0.1.0, which is still not entirely feature-complete, but works perfectly to extract complete, daily-fresh data. For my own convenience I subset the data in &lt;a href="https://github.com/notconfusing/wikidataSex/tree/master/ratio_analysis"&gt;java and then json export (github link)&lt;/a&gt;, allowing me to munge it in Python with the "Pandas" library, which is exactly what you see here.&lt;/p&gt;
&lt;p&gt;The biggest change in my opinion is the broadening - (but still not broad enough in my opinion - of the "accepted values" of the property. Let's see what wikidatians are using these days. Below are the English titles of the QIDs and how many different wikidata-linked-wikis are connected to an item utilising that value.
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .rendered}
::: {.input}
::: {.prompt .input_prompt}
In [1]:
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.input_area}
::: {.highlight}
    {english_label(qid): language_count for qid, language_count in used_sexes_count.iteritems()}
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.output_wrapper}
::: {.output}
::: {.output_area}
::: {.prompt .output_prompt}
Out[1]:
:::&lt;/p&gt;
&lt;p&gt;::: {.output_text .output_subarea .output_pyout}
    {u'Female': 89,
     u'female': 367,
     u'female animal': 55,
     u'genderqueer': 23,
     u'intersex': 51,
     u'kathoey': 10,
     u'male': 395,
     u'male animal': 66,
     u'man': 3,
     u'sodium': 1,
     u'transgender female': 63,
     u'transgender male': 24}
:::
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .text_cell .rendered}
::: {.prompt .input_prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}
So without delving into the &lt;em&gt;validity&lt;/em&gt; of the classifications used, which I'll adress later, we see 12 classifications in active duty. Compare this to the begrudging trinary we had a year ago, or &lt;a href="https://duckduckgo.com/?q=facebook+50+gender+options&amp;amp;t=canonical"&gt;Facebook's announcement to use about 50 classifcations&lt;/a&gt;. We can also see that there are two heavily used classifications - by the metric of number of wikis using them - called 'male' (395 wikis) and 'female' (367 wikis). Why the difference in the number of wikis? We must clarify what we mean by use. We are talking about a Wikipedia, or a Wikisource, or a Wikivoyage instance that has a article that is linked to a Wikidata item which has a P21 "sex or gender" property. So that means that there are 28 or more Wikimedia wikis which have an article which Wikidata claims is about a 'male', but have no articles about 'female's. But there are also a lot of tiny wikis out there, which might explain this discrepancy.&lt;/p&gt;
&lt;p&gt;Let's restrict our data set only to those Wikis which have 1,000 or more articles that are linked to a Wikidata item with a P21 property. There are 42 such wikis as of May 2014. Now we plot, the ratios or composition of the values of this property in each of those 42 wikis.
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .rendered}
::: {.input}
::: {.prompt .input_prompt}
In [2]:
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.input_area}
::: {.highlight}
    show_by_lang_plot()&lt;/p&gt;
&lt;p&gt;::: {.prompt .input_prompt}
Out[2]:
:::
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.output_wrapper}
&lt;a href="https://notconfusing.com/images/uploads/2014/05/Sex-Ratios-Wikidata-May-2014.png"&gt;&lt;img alt="Sex Ratios Wikidata May 2014" class="alignnone size-full wp-image-570" src="https://notconfusing.com/images/uploads/2014/05/Sex-Ratios-Wikidata-May-2014.png" style="width:770px !important"&gt;&lt;/a&gt;
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .text_cell .rendered}
::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}
As is visually evident, only the 'male' and 'female' categories are large enough to appear in the plot (later on we investigate this numerically). Therefore the chart is ordered by the 'female' percentage which ranges from 8.83% - Slovenian Wikipedia, to 19.97% - Serbian Wikipedia. English Wikipedia, the largest Wikipedia by article count comes in at 14.21% which is in the lowest quarter. Wikimedia commons, the only non-Wikipedia represented here performs &lt;em&gt;relatively&lt;/em&gt; well at 18.86%.&lt;/p&gt;
&lt;p&gt;These percentages are still systematically low, and tell a story that we've long known about representational bias. But what about the momentum? What difference are our efforts at uncovering and addressing systemic bias producing?
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .text_cell .rendered}
::: {.prompt .input_prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}
Comparing May 2013 to March 2014&lt;a class="anchor-link" href="#Comparing-May-2013-to-March-2014"&gt;¶&lt;/a&gt; {#Comparing-May-2013-to-March-2014}&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .text_cell .rendered}
::: {.prompt .input_prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}
These next tables inspect how a language's composition changed in the previous year. We consider all languages that had at least 1,000 P21 associated properties in &lt;em&gt;both&lt;/em&gt; years. I'll disclaim that the differences come from the a Wikipedia's content changing, but also from Wikidata becoming more connected to those different wikis. It's not possible at the moment to disentangle these two causes. Another complicating factor, that we will investigate later on, is the growth of the neither-male-nor-female entries which could account for this drop - but (spoiler) they don't.&lt;/p&gt;
&lt;p&gt;In each table there is the percentage female from May 2013, from March 2014, and the &lt;em&gt;'change%'&lt;/em&gt; that this represents, "year-over-year" (even though its about 14 months).&lt;/p&gt;
&lt;p&gt;We sort by &lt;em&gt;'change%'&lt;/em&gt;, and first look at the largest losses in 'female' percentage.
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .text_cell .rendered}
::: {.prompt .input_prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}&lt;/p&gt;
&lt;h3 id="Top-Losers"&gt;Top Losers&lt;a class="anchor-link" href="#Top-Losers"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .rendered}
::: {.input}
::: {.prompt .input_prompt}
In [17]:
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.input_area}
::: {.highlight}
    diffdf.sort(columns='change%', ascending=True)[['female_may2013','female_march2014','change%']].head(10)
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.output_wrapper}
::: {.output}
::: {.output_area}
::: {.prompt .output_prompt}
Out[17]:
:::&lt;/p&gt;
&lt;p&gt;::: {.output_html .rendered_html .output_subarea .output_pyout}
::: {style="max-height: 1000px; max-width: 1500px; overflow: auto;"}&lt;/p&gt;
&lt;table class="dataframe" border="1"&gt;

&lt;thead&gt;


&lt;tr style="text-align: right;"&gt;


&lt;th&gt;

:::
:::
:::
:::
:::
:::
:::
:::


&lt;/th&gt;


&lt;th&gt;

female\_may2013


&lt;/th&gt;


&lt;th&gt;

female\_march2014


&lt;/th&gt;


&lt;th&gt;

change%


&lt;/th&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

lang


&lt;/th&gt;


&lt;th&gt;


&lt;/th&gt;


&lt;th&gt;


&lt;/th&gt;


&lt;th&gt;


&lt;/th&gt;


&lt;/tr&gt;


&lt;/thead&gt;


&lt;tbody&gt;


&lt;tr&gt;


&lt;th&gt;

enwiki


&lt;/th&gt;


&lt;td&gt;

0.1845


&lt;/td&gt;


&lt;td&gt;

0.142132


&lt;/td&gt;


&lt;td&gt;

-22.96


&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

gawiki


&lt;/th&gt;


&lt;td&gt;

0.1456


&lt;/td&gt;


&lt;td&gt;

0.118133


&lt;/td&gt;


&lt;td&gt;

-18.86


&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

afwiki


&lt;/th&gt;


&lt;td&gt;

0.1406


&lt;/td&gt;


&lt;td&gt;

0.115850


&lt;/td&gt;


&lt;td&gt;

-17.60


&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

cswiki


&lt;/th&gt;


&lt;td&gt;

0.1705


&lt;/td&gt;


&lt;td&gt;

0.141063


&lt;/td&gt;


&lt;td&gt;

-17.27


&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

frwiki


&lt;/th&gt;


&lt;td&gt;

0.1658


&lt;/td&gt;


&lt;td&gt;

0.141045


&lt;/td&gt;


&lt;td&gt;

-14.93


&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

zhwiki


&lt;/th&gt;


&lt;td&gt;

0.2062


&lt;/td&gt;


&lt;td&gt;

0.178885


&lt;/td&gt;


&lt;td&gt;

-13.25


&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

itwiki


&lt;/th&gt;


&lt;td&gt;

0.1667


&lt;/td&gt;


&lt;td&gt;

0.144760


&lt;/td&gt;


&lt;td&gt;

-13.16


&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

hywiki


&lt;/th&gt;


&lt;td&gt;

0.1633


&lt;/td&gt;


&lt;td&gt;

0.141859


&lt;/td&gt;


&lt;td&gt;

-13.13


&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

ruwiki


&lt;/th&gt;


&lt;td&gt;

0.1627


&lt;/td&gt;


&lt;td&gt;

0.142226


&lt;/td&gt;


&lt;td&gt;

-12.58


&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

htwiki


&lt;/th&gt;


&lt;td&gt;

0.0531


&lt;/td&gt;


&lt;td&gt;

0.047382


&lt;/td&gt;


&lt;td&gt;

-10.77


&lt;/td&gt;


&lt;/tr&gt;


&lt;/tbody&gt;


&lt;/table&gt;

&lt;p&gt;10 rows × 3 columns&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;::: {.cell .border-box-sizing .text_cell .rendered}
::: {.prompt .input_prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}
English Wikipedia fell the most in the percentage of it's P21 properties being marked 'female'. The reprsentation of articles marked 'female', compared to all others, dropped by about 4% in absolute terms, which is -23% year-on-year.&lt;/p&gt;
&lt;p&gt;Also of note, the Hatian Wikipedia, the previous worst at 5.3% slid to retain the dubious title at 4.7%.&lt;/p&gt;
&lt;p&gt;What about the other end of the chart?
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .text_cell .rendered}
::: {.prompt .input_prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}&lt;/p&gt;
&lt;h3 id="Top-Winners"&gt;Top Winners&lt;a class="anchor-link" href="#Top-Winners"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .rendered}
::: {.input}
::: {.prompt .input_prompt}
In [18]:
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.input_area}
::: {.highlight}
    diffdf.sort(columns='change%', ascending=False)[['female_may2013','female_march2014','change%']].head(10)
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.output_wrapper}
::: {.output}
::: {.output_area}
::: {.prompt .output_prompt}
Out[18]:
:::&lt;/p&gt;
&lt;p&gt;::: {.output_html .rendered_html .output_subarea .output_pyout}
::: {style="max-height: 1000px; max-width: 1500px; overflow: auto;"}&lt;/p&gt;
&lt;table class="dataframe" border="1"&gt;

&lt;thead&gt;


&lt;tr style="text-align: right;"&gt;


&lt;th&gt;

:::
:::
:::
:::
:::
:::


&lt;/th&gt;


&lt;th&gt;

female\_may2013


&lt;/th&gt;


&lt;th&gt;

female\_march2014


&lt;/th&gt;


&lt;th&gt;

change%


&lt;/th&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

lang


&lt;/th&gt;


&lt;th&gt;


&lt;/th&gt;


&lt;th&gt;


&lt;/th&gt;


&lt;th&gt;


&lt;/th&gt;


&lt;/tr&gt;


&lt;/thead&gt;


&lt;tbody&gt;


&lt;tr&gt;


&lt;th&gt;

urwiki


&lt;/th&gt;


&lt;td&gt;

0.1319


&lt;/td&gt;


&lt;td&gt;

0.486671


&lt;/td&gt;


&lt;td&gt;

268.97


&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

ocwiki


&lt;/th&gt;


&lt;td&gt;

0.1261


&lt;/td&gt;


&lt;td&gt;

0.159599


&lt;/td&gt;


&lt;td&gt;

26.57


&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

mlwiki


&lt;/th&gt;


&lt;td&gt;

0.1636


&lt;/td&gt;


&lt;td&gt;

0.202758


&lt;/td&gt;


&lt;td&gt;

23.93


&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

bnwiki


&lt;/th&gt;


&lt;td&gt;

0.1313


&lt;/td&gt;


&lt;td&gt;

0.161183


&lt;/td&gt;


&lt;td&gt;

22.76


&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

mznwiki


&lt;/th&gt;


&lt;td&gt;

0.1041


&lt;/td&gt;


&lt;td&gt;

0.125305


&lt;/td&gt;


&lt;td&gt;

20.37


&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

arzwiki


&lt;/th&gt;


&lt;td&gt;

0.2392


&lt;/td&gt;


&lt;td&gt;

0.287158


&lt;/td&gt;


&lt;td&gt;

20.05


&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

ltwiki


&lt;/th&gt;


&lt;td&gt;

0.1190


&lt;/td&gt;


&lt;td&gt;

0.142340


&lt;/td&gt;


&lt;td&gt;

19.61


&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

arwiki


&lt;/th&gt;


&lt;td&gt;

0.1293


&lt;/td&gt;


&lt;td&gt;

0.153516


&lt;/td&gt;


&lt;td&gt;

18.73


&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

warwiki


&lt;/th&gt;


&lt;td&gt;

0.1003


&lt;/td&gt;


&lt;td&gt;

0.116598


&lt;/td&gt;


&lt;td&gt;

16.25


&lt;/td&gt;


&lt;/tr&gt;


&lt;tr&gt;


&lt;th&gt;

tlwiki


&lt;/th&gt;


&lt;td&gt;

0.2943


&lt;/td&gt;


&lt;td&gt;

0.340477


&lt;/td&gt;


&lt;td&gt;

15.69


&lt;/td&gt;


&lt;/tr&gt;


&lt;/tbody&gt;


&lt;/table&gt;

&lt;p&gt;10 rows × 3 columns&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;::: {.cell .border-box-sizing .text_cell .rendered}
::: {.prompt .input_prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}
Urdu Wikipedia gained a massive 268% year-on-year increase in their 'female' ratio. Now 49% of their P21-tagged articles have label 'female'. Does anyone closer to this community know if there were any tagging efforts?&lt;/p&gt;
&lt;p&gt;Tagalog Wiki, previous best, continued to increase to 34% of their P21-tagged articles having label 'female'.&lt;/p&gt;
&lt;p&gt;There has been a lot of movement in sex-ratios of different languages. As stated earlier this is also due in some part to the maturing of wikidata clusters. Next year we will be able to see if there is a deceleration in these ratios moving.&lt;/p&gt;
&lt;p&gt;We now move on to invesitage the second confounding factor, the increase in accepted "sex or gender" values.
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .text_cell .rendered}
::: {.prompt .input_prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}&lt;/p&gt;
&lt;h3 id="Non-'male'-or-'female'-values."&gt;Non 'male' or 'female' values.&lt;a class="anchor-link" href="#Non-'male'-or-'female'-values."&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .text_cell .rendered}
::: {.prompt .input_prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}
As we saw earlier, there are now 12 properties that are being used to describe P21. In May 2013 the only non-male-female term was intersex, and P21 said that you &lt;strong&gt;should&lt;/strong&gt; be one of male, female, or intersex. I was quite angered by this prescriptiveness, but with help from online discussions, and with thanks to the &lt;a href="https://lists.wikimedia.org/mailman/listinfo/gendergap"&gt;gendergap mailing list&lt;/a&gt; some of those policies have changed. This now is the "sex or gender" property, rather than just "sex" which I consider a mixed result - quite literally. I am pleased that there is one instance of this value that is "sodium", because I support this prorpety allowing any value. To be clear, what an "accepted" value means, is that periodically a check is run on the database, and non-accepted values are compiled into a list for user-attention. So robot won't fight you if you use an unaccepted value, but the fodder is there for a human combatant.&lt;/p&gt;
&lt;p&gt;Two more of the new values mention "animal" because in &lt;a href="https://www.wikidata.org/wiki/Property_talk:P21#Values_not_making_sense"&gt;Czech and Finnish, there are seperate words to describe sexes of non-human animals&lt;/a&gt;. And of course Wikipedia has articles on famous animals too.&lt;/p&gt;
&lt;p&gt;Some others like 'Female' (a 1933 film), and 'man' are probably due to human tagging errors.&lt;/p&gt;
&lt;p&gt;Below are the wiki's which have the highest number of non 'male' or 'female' values as represented by the new column at the end "non_MF%". As you can see none of them exceed two-tenths of 1%. So the upper analysis of year-on-year-change could at most be influenced by error range of ±0.2%.
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .rendered}
::: {.input}
::: {.prompt .input_prompt}
In [130]:
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.input_area}
::: {.highlight}
    top_non_MF.sort('non_MF%', ascending=False)
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.output_wrapper}
::: {.output}
::: {.output_area}
::: {.prompt .output_prompt}
Out[130]:
:::&lt;/p&gt;
&lt;p&gt;::: {.output_html .rendered_html .output_subarea .output_pyout}
::: {style="max-height: 1000px; max-width: 1500px; overflow: auto;"}
                       wiki               non_MF%&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;male animal          yiwiki             0.181159
  transgender female   urwiki             0.092994
  Female               mgwiki             0.080321
  genderqueer          zh_min_nanwiki   0.066445
  intersex             ckbwiki            0.058754
  transgender male     arzwiki            0.042105
  female animal        hywiki             0.038812
  kathoey              thwiki             0.012922
  man                  jawiki             0.001523
  sodium               eswiki             0.000990&lt;/p&gt;
&lt;p&gt;10 rows × 2 columns
:::
:::
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .text_cell .rendered}
::: {.prompt .input_prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}
Intriguingly, Urdu which tops the charts in year-on-year 'female' increase also the leader in higest ratio of "transgender female" at nearly one-tenth of one percent, or about 1 in a 1000. This lends credence to the idea that some Urdu Wikipedians have been busy.
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .text_cell .rendered}
::: {.prompt .input_prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}&lt;/p&gt;
&lt;h3 id="Accompanying-Data-Richness"&gt;Accompanying Data Richness&lt;a class="anchor-link" href="#Accompanying-Data-Richness"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .text_cell .rendered}
::: {.prompt .input_prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}
Markus Krötzsch, instigator of the Wikidata Toolkit, on which this research rests, talks about the complexity of Wikidata data. Convincingly he discusses why full unstructured access to the data is important - creative queries. Both star and tree shaped queries should be possible and at the users discretion.&lt;/p&gt;
&lt;p&gt;One less trivial query I wanted to cook up was - "on items with a P21 value what are the properties by per item, by P21 value?" Framed in English, are wikidata items about males data-richer?
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .rendered}
::: {.input}
::: {.prompt .input_prompt}
In [44]:
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.input_area}
::: {.highlight}
    sex_props_df[sex_props_df['item_count'] &amp;gt; 5]
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.output_wrapper}
::: {.output}
::: {.output_area}
::: {.prompt .output_prompt}
Out[44]:
:::&lt;/p&gt;
&lt;p&gt;::: {.output_html .rendered_html .output_subarea .output_pyout}
::: {style="max-height: 1000px; max-width: 1500px; overflow: auto;"}
                       item_count   total_props   props_per_item&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;transgender female   41            398            9.707317
  intersex             8             88             11.000000
  female animal        6             44             7.333333
  male animal          55            385            7.000000
  genderqueer          8             63             7.875000
  female               122288        738962         6.042801
  male                 768646        4816357        6.266028&lt;/p&gt;
&lt;p&gt;7 rows × 3 columns
:::
:::
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .text_cell .rendered}
::: {.prompt .input_prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}
The above chart displays the properties per item, for all P21 values that occur 5 or more times. The results are telling, on avereage items with the 'male' property have 6.27 properties, and those with 'female' 6.04. It's also worth mentioning that 'transgender female' averages 9.71 properties per item.
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .text_cell .rendered}
::: {.prompt .input_prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}&lt;/p&gt;
&lt;h3 id="Conclusions"&gt;Conclusions&lt;a class="anchor-link" href="#Conclusions"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .text_cell .rendered}
::: {.prompt .input_prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}
Without becoming too political, the biases that exist in Wikipedia's representation of the world are systemic, and appreciable. We can see from our year-on-year calculations that there is movement in this dataset - albeit not always for the better. The representation of non-male-female items I suspect is lower than what a sample from the wolrd would indicate, but I don't have any statistical reference, and would welcome suggestions on datasets with which to compare. Lastly we showed that not only in representation, but also in attention given to each item, underepresnted 'sex or genders' are less semantically-described.&lt;/p&gt;
&lt;p&gt;Questions and criticism greatly recieved,&lt;/p&gt;
&lt;p&gt;&lt;a href="https://twitter.com/notconfusing"&gt;‽notconfusing&lt;/a&gt;
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .text_cell .rendered}
::: {.prompt .input_prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}
Start of Supporting Code&lt;a class="anchor-link" href="#Start-of-Supporting-Code"&gt;¶&lt;/a&gt; {#Start-of-Supporting-Code}&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .rendered}
::: {.input}
::: {.prompt .input_prompt}
In [1]:
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.input_area}
::: {.highlight}
    import json
    from collections import defaultdict
    import pandas as pd
    import pywikibot
    import decimal
    NOPLACES = decimal.Decimal(10) &lt;strong&gt; 0
    TWOPLACES = decimal.Decimal(10) &lt;/strong&gt; -2
    %pylab inline
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.output_wrapper}
::: {.output}
::: {.output_area}
::: {.prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.output_subarea .output_stream .output_stderr .output_text}
    VERBOSE:pywiki:Starting 1 threads...
:::
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .rendered}
::: {.input}
::: {.prompt .input_prompt}
In [20]:
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.input_area}
::: {.highlight}
    norm_sex[sexdf['total']&amp;gt;1000].sort(columns='non_MF', ascending=False).head(10)
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.output_wrapper}
::: {.output}
::: {.output_area}
::: {.prompt .output_prompt}
Out[20]:
:::&lt;/p&gt;
&lt;p&gt;::: {.output_html .rendered_html .output_subarea .output_pyout}
::: {style="max-height: 1000px; max-width: 1500px; overflow: auto;"}
                     female animal   intersex   kathoey    Female     transgender female   male animal   male       female     transgender male   genderqueer   man   sodium   non_MF&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;zh_min_nanwiki   0.000000        0.000000   0.000000   0.000664   0.000000             0.000664      0.787375   0.210631   0.000000           0.000664      0     0        0.001993
  yiwiki             0.000000        0.000000   0.000000   0.000000   0.000000             0.001812      0.897645   0.100543   0.000000           0.000000      0     0        0.001812
  cywiki             0.000371        0.000000   0.000000   0.000186   0.000371             0.000000      0.820375   0.178326   0.000186           0.000186      0     0        0.001299
  ckbwiki            0.000000        0.000588   0.000000   0.000000   0.000588             0.000000      0.893067   0.105758   0.000000           0.000000      0     0        0.001175
  thwiki             0.000000        0.000000   0.000129   0.000129   0.000388             0.000388      0.788345   0.210492   0.000129           0.000000      0     0        0.001163
  mswiki             0.000000        0.000000   0.000000   0.000223   0.000223             0.000446      0.802679   0.196205   0.000223           0.000000      0     0        0.001116
  ruwikiquote        0.000000        0.000000   0.000000   0.000552   0.000000             0.000000      0.909492   0.089404   0.000000           0.000552      0     0        0.001104
  mlwiki             0.000270        0.000270   0.000000   0.000270   0.000270             0.000000      0.796161   0.202758   0.000000           0.000000      0     0        0.001081
  eowiki             0.000125        0.000187   0.000062   0.000062   0.000374             0.000249      0.851300   0.147640   0.000000           0.000000      0     0        0.001060
  kowiki             0.000075        0.000075   0.000038   0.000038   0.000491             0.000113      0.801608   0.197373   0.000075           0.000113      0     0        0.001019&lt;/p&gt;
&lt;p&gt;10 rows × 13 columns
:::
:::
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .rendered}
::: {.input}
::: {.prompt .input_prompt}
In [3]:
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.input_area}
::: {.highlight}
    jsonfile = open('lang_sex.json','r')
    bigdict = json.load(jsonfile)
    lang_sex = defaultdict(dict)
    for keystring, count in bigdict.iteritems():
        lang, sex = keystring.split('--')
        lang_sex[lang][sex] = count&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;used_sexes&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;defaultdict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;list&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;lang&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;sex_dict&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ow"&gt;in&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;lang_sex&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iteritems&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;sex&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ow"&gt;in&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;sex_dict&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iterkeys&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;used_sexes&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;sex&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lang&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="n"&gt;used_sexes_count&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="nl"&gt;sex&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lang_list&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;sex&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;lang_list&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ow"&gt;in&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;used_sexes&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iteritems&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="err"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;:::
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .rendered}
::: {.input}
::: {.prompt .input_prompt}
In [6]:
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.input_area}
::: {.highlight}
    sexdf = pd.DataFrame.from_dict(lang_sex, orient='index')
    sexdf = sexdf.fillna(value=0)
    #sexdf.plot(kind='bar', stacked=True, figsize=(10,10))
    Norm_sex is not "normal" sex, but rather the Sex-data normed into percentages.
    norm_sex = sexdf.apply(lambda col: col / float(col.sum()), axis=1)
:::
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .rendered}
::: {.input}
::: {.prompt .input_prompt}
In [8]:
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.input_area}
::: {.highlight}
    #Tranforming QIDs into English labels.
    enwp = pywikibot.Site('en','wikipedia')
    wikidata = enwp.data_repository()&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;english_label&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;qid&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;page&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pywikibot&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ItemPage&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;wikidata&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;qid&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;page&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;get&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="k"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;labels&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;en&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;sex_qs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="n"&gt;norm_sex&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;sex_labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;english_label&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sex_q&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;sex_q&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="n"&gt;sex_qs&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;norm_sex&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sex_labels&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.output_wrapper}
::: {.output}
::: {.output_area}
::: {.prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.output_subarea .output_stream .output_stderr .output_text}
    VERBOSE:pywiki:Found 1 wikidata:wikidata processes running, including this one.
:::
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .rendered}
::: {.input}
::: {.prompt .input_prompt}
In [9]:
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.input_area}
::: {.highlight}
    #norm_sex.index = [label.replace('wiki','') for label in norm_sex.index]
    #comparing by total between two different dataframes requires 
    #that norm_sex has not had any rows modified since it was created from sexdf
    sexdf['total'] = sexdf.sum(axis=1)
    fs1000 = norm_sex[sexdf['total']&amp;gt;10000].sort('female', ascending=True)
:::
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .rendered}
::: {.input}
::: {.prompt .input_prompt}
In [11]:
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.input_area}
::: {.highlight}
    def show_by_lang_plot():
        fsplot = fs1000.plot(kind='bar', stacked=True, legend=True, figsize=(13,8), alpha=0.9, ylim=(0,1),
                             title= '''Comoposition of Wikidata Prorerty:P21 "Sex or Gender" by Language 
        (Languages with over 1,000 associated P21)''',
                             colormap='Set1')&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;yticks&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;decimal&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nb"&gt;Decimal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;quantize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;NOPLACES&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;%&amp;#39;&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;

    &lt;span class="n"&gt;ticklocs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;langs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xticks&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;langstrs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;decimal&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nb"&gt;Decimal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;norm_sex&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;lang&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_text&lt;/span&gt;&lt;span class="p"&gt;()][&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;female&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;quantize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;TWOPLACES&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;%  &amp;#39;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;lang&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_text&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;lang&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="n"&gt;langs&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xticks&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ticklocs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;langstrs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Language-Wiki percentage &amp;quot;female&amp;quot;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;:::
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .text_cell .rendered}
::: {.prompt .input_prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}
For your edification, the full data, and not just the 'female' percentages.
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .rendered}
::: {.input}
::: {.prompt .input_prompt}
In [15]:
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.input_area}
::: {.highlight}
    fs1000
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.output_wrapper}
::: {.output}
::: {.output_area}
::: {.prompt .output_prompt}
Out[15]:
:::&lt;/p&gt;
&lt;p&gt;::: {.output_html .rendered_html .output_subarea .output_pyout}
::: {style="max-height: 1000px; max-width: 1500px; overflow: auto;"}
                female animal   intersex   kathoey    Female     transgender female   male animal   male       female     transgender male   genderqueer   man        sodium&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;slwiki        0.000074        0.000000   0.000000   0.000074   0.000074             0.000074      0.911398   0.088307   0.000000           0.000000      0.000000   0.00000
  lawiki        0.000060        0.000060   0.000000   0.000060   0.000180             0.000120      0.889302   0.110219   0.000000           0.000000      0.000000   0.00000
  bewiki        0.000000        0.000000   0.000000   0.000099   0.000000             0.000099      0.876528   0.123273   0.000000           0.000000      0.000000   0.00000
  cawiki        0.000026        0.000000   0.000000   0.000026   0.000103             0.000129      0.870905   0.128786   0.000026           0.000000      0.000000   0.00000
  elwiki        0.000000        0.000000   0.000000   0.000068   0.000068             0.000068      0.869061   0.130734   0.000000           0.000000      0.000000   0.00000
  euwiki        0.000080        0.000000   0.000000   0.000080   0.000080             0.000239      0.865686   0.133837   0.000000           0.000000      0.000000   0.00000
  skwiki        0.000078        0.000000   0.000000   0.000078   0.000078             0.000078      0.864363   0.135245   0.000078           0.000000      0.000000   0.00000
  frwiki        0.000020        0.000025   0.000000   0.000005   0.000107             0.000097      0.858680   0.141045   0.000005           0.000015      0.000000   0.00000
  cswiki        0.000048        0.000000   0.000000   0.000024   0.000096             0.000096      0.858648   0.141063   0.000024           0.000000      0.000000   0.00000
  enwiki        0.000009        0.000012   0.000002   0.000002   0.000069             0.000052      0.857699   0.142132   0.000007           0.000014      0.000002   0.00000
  ruwiki        0.000038        0.000019   0.000010   0.000010   0.000077             0.000077      0.857515   0.142226   0.000019           0.000010      0.000000   0.00000
  dawiki        0.000036        0.000073   0.000000   0.000036   0.000109             0.000073      0.856768   0.142904   0.000000           0.000000      0.000000   0.00000
  ukwiki        0.000062        0.000031   0.000000   0.000031   0.000062             0.000031      0.855573   0.144178   0.000031           0.000000      0.000000   0.00000
  dewiki        0.000013        0.000006   0.000003   0.000003   0.000034             0.000047      0.855591   0.144277   0.000013           0.000013      0.000000   0.00000
  itwiki        0.000006        0.000028   0.000000   0.000006   0.000090             0.000107      0.854986   0.144760   0.000011           0.000006      0.000000   0.00000
  eowiki        0.000125        0.000187   0.000062   0.000062   0.000374             0.000249      0.851300   0.147640   0.000000           0.000000      0.000000   0.00000
  glwiki        0.000082        0.000082   0.000000   0.000082   0.000329             0.000164      0.851602   0.147658   0.000000           0.000000      0.000000   0.00000
  etwiki        0.000079        0.000079   0.000000   0.000079   0.000079             0.000237      0.846676   0.152771   0.000000           0.000000      0.000000   0.00000
  arwiki        0.000040        0.000040   0.000000   0.000040   0.000079             0.000079      0.846166   0.153516   0.000000           0.000040      0.000000   0.00000
  idwiki        0.000208        0.000052   0.000000   0.000052   0.000104             0.000156      0.843220   0.156156   0.000052           0.000000      0.000000   0.00000
  hrwiki        0.000078        0.000078   0.000000   0.000078   0.000078             0.000156      0.842670   0.156863   0.000000           0.000000      0.000000   0.00000
  eswiki        0.000030        0.000040   0.000000   0.000010   0.000109             0.000079      0.841094   0.158589   0.000020           0.000020      0.000000   0.00001
  ptwiki        0.000043        0.000043   0.000000   0.000014   0.000199             0.000114      0.840760   0.158785   0.000014           0.000028      0.000000   0.00000
  bgwiki        0.000091        0.000045   0.000000   0.000045   0.000091             0.000181      0.839242   0.160305   0.000000           0.000000      0.000000   0.00000
  huwiki        0.000120        0.000040   0.000000   0.000040   0.000200             0.000200      0.839014   0.160386   0.000000           0.000000      0.000000   0.00000
  plwiki        0.000031        0.000021   0.000000   0.000010   0.000063             0.000094      0.839206   0.160575   0.000000           0.000000      0.000000   0.00000
  nlwiki        0.000041        0.000027   0.000000   0.000014   0.000082             0.000123      0.838302   0.161343   0.000014           0.000041      0.000014   0.00000
  hewiki        0.000085        0.000042   0.000000   0.000042   0.000297             0.000127      0.836914   0.162449   0.000000           0.000042      0.000000   0.00000
  trwiki        0.000076        0.000038   0.000000   0.000038   0.000114             0.000191      0.833810   0.165656   0.000038           0.000038      0.000000   0.00000
  fiwiki        0.000060        0.000060   0.000020   0.000020   0.000099             0.000079      0.824523   0.175100   0.000040           0.000000      0.000000   0.00000
  jawiki        0.000030        0.000030   0.000015   0.000015   0.000167             0.000107      0.823550   0.176039   0.000000           0.000030      0.000015   0.00000
  zhwiki        0.000087        0.000029   0.000029   0.000029   0.000261             0.000145      0.820476   0.178885   0.000029           0.000029      0.000000   0.00000
  nowiki        0.000020        0.000020   0.000000   0.000020   0.000082             0.000082      0.819593   0.180183   0.000000           0.000000      0.000000   0.00000
  shwiki        0.000073        0.000073   0.000000   0.000073   0.000367             0.000000      0.817768   0.181571   0.000000           0.000073      0.000000   0.00000
  fawiki        0.000066        0.000033   0.000033   0.000033   0.000332             0.000033      0.816748   0.182721   0.000000           0.000000      0.000000   0.00000
  rowiki        0.000096        0.000048   0.000000   0.000048   0.000048             0.000096      0.816821   0.182844   0.000000           0.000000      0.000000   0.00000
  simplewiki    0.000051        0.000051   0.000000   0.000051   0.000306             0.000102      0.815151   0.184084   0.000051           0.000153      0.000000   0.00000
  viwiki        0.000093        0.000093   0.000000   0.000093   0.000093             0.000186      0.813103   0.186247   0.000000           0.000093      0.000000   0.00000
  svwiki        0.000042        0.000042   0.000000   0.000014   0.000111             0.000056      0.811433   0.188275   0.000014           0.000014      0.000000   0.00000
  commonswiki   0.000045        0.000000   0.000000   0.000045   0.000089             0.000268      0.810849   0.188614   0.000045           0.000045      0.000000   0.00000
  kowiki        0.000075        0.000075   0.000038   0.000038   0.000491             0.000113      0.801608   0.197373   0.000075           0.000113      0.000000   0.00000
  srwiki        0.000074        0.000074   0.000000   0.000074   0.000074             0.000223      0.799718   0.199688   0.000000           0.000074      0.000000   0.00000&lt;/p&gt;
&lt;p&gt;42 rows × 12 columns
:::
:::
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .rendered}
::: {.input}
::: {.prompt .input_prompt}
In [16]:
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.input_area}
::: {.highlight}
    maydf = pd.read_table('may2013.csv',sep=',', index_col=0)
    maydf['female'] = maydf['perc'] / 100.0
    diffdf = maydf.join(other=norm_sex,how='inner',lsuffix='_may2013', rsuffix='_march2014')
    diffdf['change%'] = (diffdf['female_march2014'] - diffdf['female_may2013']) / diffdf['female_may2013']
    diffdf['change%'] = diffdf['change%'].apply(lambda x: decimal.Decimal(x * 100).quantize(TWOPLACES) )
:::
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .rendered}
::: {.input}
::: {.prompt .input_prompt}
In [19]:
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.input_area}
::: {.highlight}
    non_MF_cols = [col for col in norm_sex.columns if col not in ['male','female']]
    norm_sex['non_MF'] = norm_sex[non_MF_cols].sum(axis=1)
:::
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .rendered}
::: {.input}
::: {.prompt .input_prompt}
In [128]:
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.input_area}
::: {.highlight}
    top_non_MF_dict = dict()
    for s in non_MF_cols:
        t = norm_sex[sexdf['total']&amp;gt;1000].sort(columns=s, ascending=False)[s].head(1)
        top_non_MF_dict[s] = {'wiki':t.index[0],'non_MF%':t[0]*100}
    top_non_MF = pd.DataFrame.from_dict(data=top_non_MF_dict, orient='index')
:::
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .rendered}
::: {.input}
::: {.prompt .input_prompt}
In [43]:
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.input_area}
::: {.highlight}
    jsonfile = open('sex_propcount.json','r')
    sex_props_json = json.load(jsonfile)
    sex_props = defaultdict(dict)
    for keystring, count in sex_props_json.iteritems():
        sex, prop = keystring.split('_')
        sex_props[sex][prop] = count&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;sex_props_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sex_props&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;orient&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;index&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;sex_qs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="n"&gt;sex_props_df&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;sex_labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;english_label&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sex_q&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;sex_q&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="n"&gt;sex_qs&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;sex_props_df&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;item_count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;total_props&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;sex_props_df&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sex_labels&lt;/span&gt;

&lt;span class="n"&gt;sex_props_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;props_per_item&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sex_props_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;total_props&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;sex_props_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;item_count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.output_wrapper}
::: {.output}
::: {.output_area}
::: {.prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.output_subarea .output_stream .output_stderr .output_text}
    VERBOSE:pywiki:Found 1 wikidata:wikidata processes running, including this one.
:::
:::
:::
:::
:::&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;</content><category term="hacking"></category></entry><entry><title>The Listiness of Wikipedia</title><link href="https://notconfusing.com/the-listiness-of-wikipedia.html" rel="alternate"></link><published>2014-04-22T03:58:00-07:00</published><updated>2014-04-22T03:58:00-07:00</updated><author><name>max</name></author><id>tag:notconfusing.com,2014-04-22:/the-listiness-of-wikipedia.html</id><summary type="html">&lt;p&gt;::: {#notebook .border-box-sizing tabindex="-1"}
::: {#notebook-container .container}
::: {.cell .border-box-sizing .text_cell .rendered}
::: {.prompt .input_prompt}
View this report with the &lt;a href="http://nbviewer.ipython.org/github/notconfusing/listiness/blob/master/Listiness.ipynb"&gt;Ipython Notebook Viewer&lt;/a&gt; (where it looks best).
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}
The Listiness of Wikipedia&lt;a class="anchor-link" href="#The-Listiness-of-Wikipedia"&gt;¶&lt;/a&gt; {#The-Listiness-of-Wikipedia}&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .text_cell .rendered}
::: {.prompt .input_prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}
Although it was only an aside, an …&lt;/p&gt;</summary><content type="html">&lt;p&gt;::: {#notebook .border-box-sizing tabindex="-1"}
::: {#notebook-container .container}
::: {.cell .border-box-sizing .text_cell .rendered}
::: {.prompt .input_prompt}
View this report with the &lt;a href="http://nbviewer.ipython.org/github/notconfusing/listiness/blob/master/Listiness.ipynb"&gt;Ipython Notebook Viewer&lt;/a&gt; (where it looks best).
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}
The Listiness of Wikipedia&lt;a class="anchor-link" href="#The-Listiness-of-Wikipedia"&gt;¶&lt;/a&gt; {#The-Listiness-of-Wikipedia}&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .text_cell .rendered}
::: {.prompt .input_prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}
Although it was only an aside, an answer of "What is a Reference work?" caught my attention at UC Berkeley iSchool's &lt;a href="http://www.ischool.berkeley.edu/newsandevents/events/ias/20140321"&gt;March 21st Friday Afternoon Seminar by Michael Buckland&lt;/a&gt;. One possible answer suggested was: works that are over 80% list.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://notconfusing.com/images/uploads/2014/04/marciabatesfig1.png"&gt;&lt;img alt="marciabatesfig1" class="wp-image-486 size-medium" src="https://notconfusing.com/images/uploads/2014/04/marciabatesfig1.png" style="width:300px !important"&gt;&lt;/a&gt; Bates' classification of References works search patterns.&lt;/p&gt;
&lt;p&gt;That definition, although seeming a bit short, was actually serious suggestion published by Marcia Bates in 1984. [Bates, Marcia J. "What Is a Reference Book: A Theoretical and Empirical Analysis." RQ 26 (Fall 1986): 37-57] This is an elegant solution in my opinion as a way to define reference works because although heuristic, it's entirely quantitative. Still necessary though, is a definition of list. According to Bates every book is a certain percentage list. Consider a classical monograph, it probably has a table of contents, or index - which is a list structure.&lt;/p&gt;
&lt;p&gt;At this point in reading, I realised that it would be simple identify what parts of Wikipedia articles are list. And so, we could determine the percentage list - or the "listiness" of - each Wikipedia article.
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .text_cell .rendered}
::: {.prompt .input_prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}&lt;/p&gt;
&lt;h3 id="Method"&gt;Method&lt;a class="anchor-link" href="#Method"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .text_cell .rendered}
::: {.prompt .input_prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}
Analysing a May 2014 copy of English Wikipedia, we look at the listiness of all articles in the main namespace. To do this I used the &lt;code&gt;xml_dump&lt;/code&gt; library from the excellent &lt;a href="https://pypi.python.org/pypi/mediawiki-utilities/0.2.1"&gt;mediawiki-utilities&lt;/a&gt; by [&lt;a href="https://twitter.com/halfak"&gt;\@halfak&lt;/a&gt;]{.citation}.&lt;/p&gt;
&lt;p&gt;In &lt;a href="https://www.wikidata.org/wiki/Q826308"&gt;wikitext&lt;/a&gt; lists are identified, by the prepending of a line with the characters __*__ (unordered list) and &lt;strong&gt;#&lt;/strong&gt; (numbered list). Additionally there are Infoboxes, which use | (pipe character) and tables whose rows begin with |- (pipe dash). What percentage of lines begin with any of these characters therefore determine the share of list of an article, or the "listiness" as I am now coining it.&lt;/p&gt;
&lt;p&gt;We do not allow redirect pages - pages with a starting '#' character and are one line long. Those pages which are not redirects, we term 'canonical pages'. We do not allow "talk" pages either.&lt;/p&gt;
&lt;p&gt;So for instance, we can look at the statistics of each of these different line-starting characters. Below are the mean number of these line-startings per page.
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .text_cell .rendered}
::: {.prompt .input_prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}&lt;/p&gt;
&lt;h3 id="Results"&gt;Results&lt;a class="anchor-link" href="#Results"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .rendered}
::: {.input}
::: {.prompt .input_prompt}
In [3]:
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.input_area}
::: {.highlight}
    canon_description = canonical.describe()
    canon_description.loc[['mean','std']]
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.output_wrapper}
::: {.output}
::: {.output_area}
::: {.prompt .output_prompt}
Out[3]:
:::&lt;/p&gt;
&lt;p&gt;::: {.output_html .rendered_html .output_subarea .output_pyout}
::: {style="max-height: 1000px; max-width: 1500px; overflow: auto;"}
         *          #         |-         |           total&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;mean   6.660130    0.457347   3.392566    29.755385    2284.055053
  std    33.667805   7.173435   24.983288   124.254805   4120.072758&lt;/p&gt;
&lt;p&gt;2 rows × 5 columns
:::
:::
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .text_cell .rendered}
::: {.prompt .input_prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}
We read this as the mean average number of undorded list items per page is 6.6, the number of orderer list items per page is 0.45 etc. These number seem reasonable, and commensurate with casual browsing experience. However we also see that the average number of lines per article is 2284, which seems very high. But as you can see the standard deviation for total lines is very large too. That means there are some extremely long articles out there, especailly when considering an article I wrote at recently is only 24 lines long! Although, to be sure, all lines are not created equally.&lt;/p&gt;
&lt;p&gt;Now we are in a perfect place to start looking at listiness distributions. Let's visualise a histogram of the listinesses of all the pages. This first histogram just considers the classical ordered and unordered lists.
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .rendered}
::: {.input}
::: {.prompt .input_prompt}
In [5]:
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.input_area}
::: {.highlight}
    show_list_plot()
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.output_wrapper}
&lt;a href="https://notconfusing.com/images/uploads/2014/04/Listiness1.png"&gt;&lt;img alt="Listiness1" class="alignnone size-full wp-image-565" src="https://notconfusing.com/images/uploads/2014/04/Listiness1.png" style="width:531px !important"&gt;&lt;/a&gt;
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .text_cell .rendered}
::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}
Quite clearly we can see a power law distribution. Lets recall that 80% listiness is Bates' threshhold for considering a book a reference book. In our case lets see how many articles are at least 80% listy.
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .rendered}
::: {.input}
::: {.prompt .input_prompt}
In [6]:
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.input_area}
::: {.highlight}
    canonical[canonical['*and#per'] &amp;gt;= 0.8].shape[0] / float(canonical.shape[0])
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.output_wrapper}
::: {.output}
::: {.output_area}
::: {.prompt .output_prompt}
Out[6]:
:::&lt;/p&gt;
&lt;p&gt;::: {.output_text .output_subarea .output_pyout}
    0.0032001019048792404
:::
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .text_cell .rendered}
::: {.prompt .input_prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}
It's 0.32%, or about three-tenths of one percent of articles are 80% or more listy when considering ordered and unordered lists.&lt;/p&gt;
&lt;p&gt;Now lets allow for charts and infoboxes in our analysis. To eliminate confusion, pages like &lt;a href="http://en.wikipedia.org/wiki/List_of_feminists"&gt;List of Feminists&lt;/a&gt; is actually mainly a table! In our terminology it might be called "Table of Feminists", althought the "See Also" section at the end is a proper list.&lt;/p&gt;
&lt;p&gt;Now we allow table cells and infoboxes that exist to come into our listiness calculations. This produces a new picture.
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .rendered}
::: {.input}
::: {.prompt .input_prompt}
In [8]:
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.input_area}
::: {.highlight}
    show_list_and_table_plot()
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.output_wrapper}
&lt;a href="https://notconfusing.com/images/uploads/2014/04/Listiness2.png"&gt;&lt;img alt="Listiness2" class="alignnone size-full wp-image-566" src="https://notconfusing.com/images/uploads/2014/04/Listiness2.png" style="width:531px !important"&gt;&lt;/a&gt;
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .text_cell .rendered}
::: {.prompt .input_prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}
Right away a noticeable change has occurred in the distribution. One can make out a Gaussian distribution centred at about 50% listiness. Also the scale has changed, with about half of those articles which were previously 0% listy, becoming listy. Just visually we see that percentage of lines that are contributed to by tables and infoboxes, are appreciable at a large level. In fact, if we find our percentage of articles that exceed the 80% listiness threshold under the table and infobox definition, now we have:
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .rendered}
::: {.input}
::: {.prompt .input_prompt}
In [9]:
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.input_area}
::: {.highlight}
    canonical[canonical['allper'] &amp;gt;= 0.8].shape[0] / float(canonical.shape[0])
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.output_wrapper}
::: {.output}
::: {.output_area}
::: {.prompt .output_prompt}
Out[9]:
:::&lt;/p&gt;
&lt;p&gt;::: {.output_text .output_subarea .output_pyout}
    0.03052258792303092
:::
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .text_cell .rendered}
::: {.prompt .input_prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}
... about 3% of all articles. This represents an order of magnitude increase.&lt;/p&gt;
&lt;p&gt;Now we have such a figure, so what? By itself I'm not sure if we can draw any conclusions about Wikipedia being very listy. It would be instructive to compare this to other encyclopedias or libraries. You could measure whatever of Google Books is OCR'd, if you had access to it. If anyone knows of any comparable statistics please get in touch.
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .text_cell .rendered}
::: {.prompt .input_prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}&lt;/p&gt;
&lt;h3 id="More-curiosities"&gt;More curiosities&lt;a class="anchor-link" href="#More-curiosities"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .text_cell .rendered}
::: {.prompt .input_prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}
To understand more about listy Wikipedia articles there were a few more avenues of inquiry to take. A first stop was to investigate if the list and table types correlated with each other.
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .rendered}
::: {.input}
::: {.prompt .input_prompt}
In [2]:
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.input_area}
::: {.highlight}
    canonical.corr()
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.output_wrapper}
::: {.output}
::: {.output_area}
::: {.prompt .output_prompt}
Out[2]:
:::&lt;/p&gt;
&lt;p&gt;::: {.output_html .rendered_html .output_subarea .output_pyout}
::: {style="max-height: 1000px; max-width: 1500px; overflow: auto;"}
          *          #          |-         |          total&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;*      1.000000    0.016075    0.049516    0.045971    -0.093864
  #      0.016075    1.000000    0.019311    0.023440    -0.031146
  |-     0.049516    0.019311    1.000000    0.764544    -0.045862
  |      0.045971    0.023440    0.764544    1.000000    -0.093092
  total   -0.093864   -0.031146   -0.045862   -0.093092   1.000000&lt;/p&gt;
&lt;p&gt;5 rows × 5 columns
:::
:::
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .text_cell .rendered}
::: {.prompt .input_prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}
None of these correlations are very strong, except for one. The correlation between the number of lines starting with &lt;strong&gt;|-&lt;/strong&gt; and &lt;strong&gt;|&lt;/strong&gt; is 0.76 over English Wikipedia. To those who know more about Wikitext, this would be obvious because table rows start with &lt;strong&gt;|-&lt;/strong&gt;, and the delimiter for each column in that row begins with &lt;strong&gt;|&lt;/strong&gt;. However what makes this difficult to disentangle, is that the number of columns may vary, so a 4-column table would have 4 times as many &lt;strong&gt;|&lt;/strong&gt; as &lt;strong&gt;|-&lt;/strong&gt;. And of course Templates, use &lt;strong&gt;|&lt;/strong&gt; character to separate parameters and are not related to tables whatsoever.&lt;/p&gt;
&lt;p&gt;With our correlation matrix being rather uninformative, I wanted to see which where the most highly occurring words in the titles of our listiest pages. The left column is the total number of times the word occurs in &lt;em&gt;all&lt;/em&gt; page titles. The &lt;em&gt;listiest&lt;/em&gt; column is the number of times that word occurs in the titles articles who are 80% listy by the "list and table" definition. Lastly the &lt;em&gt;ratio&lt;/em&gt; column is a division of &lt;em&gt;listiest/all&lt;/em&gt;. Compare displayed ratio to the ratio of 0.03 which is the composition of total listy articles versus all articles. The table is sorted by the &lt;em&gt;listiest&lt;/em&gt; column.
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .rendered}
::: {.input}
::: {.prompt .input_prompt}
In [74]:
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.input_area}
::: {.highlight}
    lexemes_combined[lexemes_combined["listiest"] &amp;gt; 150].sort(columns="listiest", ascending=False).head(20)
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.output_wrapper}
::: {.output}
::: {.output_area}
::: {.prompt .output_prompt}
Out[74]:
:::&lt;/p&gt;
&lt;p&gt;::: {.output_html .rendered_html .output_subarea .output_pyout}
::: {style="max-height: 1000px; max-width: 1500px; overflow: auto;"}
                         all      listiest   ratio (listiest/all)&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;of                     640540   60163      0.093925
  list                   92978    34307      0.368980
  in                     488559   22937      0.046948
  the                    355866   17071      0.047970
  –                      31977    10836      0.338869
  for                    415190   9120       0.021966
  mens                   23931    5837       0.243910
  singles                7986     5391       0.675056
  wikipediaarticles      295972   5358       0.018103
  season                 41053    5323       0.129662
  and                    154046   5133       0.033321
  championships          17045    5121       0.300440
  at                     47756    5066       0.106081
  world                  34675    4981       0.143648
  district               51854    4780       0.092182
  by                     105340   4673       0.044361
  team                   36328    4256       0.117155
  county                 104867   4255       0.040575
  football               53007    3853       0.072689
  wikipediawikiproject   183711   3665       0.019950&lt;/p&gt;
&lt;p&gt;20 rows × 3 columns
:::
:::
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .text_cell .rendered}
::: {.prompt .input_prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}
As one might expect, the word "list" is the second most occurring word in titles of the listiest articles. These articles represent 36% of all the articles that have list in their title, which is significantly higher than what we'd expect on with no other information - 3%. Even "in" and "by" some occur slightly more than expected. This should make sense if you've ever browsed a Wikipedia article with a title something like &lt;a href="https://en.wikipedia.org/wiki/List_of_lists_of_lists"&gt;"List of [x] by [y]" or "List of [x] in [z]"&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The remainder of this Top 20 just goes to show how sport fans have chronicled results with great zeal. And the inexplicability of their zeal is an analogue to my curiosity for investigating this topic.&lt;/p&gt;
&lt;p&gt;Contact me with more ideas of question on twitter [[\@notconfusing]]{.citation}(https://twitter.com/notconfusing)‽‽‽&lt;/p&gt;
&lt;p&gt;Find all the code for this analysis at https://github.com/notconfusing/listiness .
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .text_cell .rendered}
::: {.prompt .input_prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}
Start of Supporting Code&lt;a class="anchor-link" href="#Start-of-Supporting-Code"&gt;¶&lt;/a&gt; {#Start-of-Supporting-Code}&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .rendered}
::: {.input}
::: {.prompt .input_prompt}
In [4]:
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.input_area}
::: {.highlight}
    import pandas as pd
    import re
    import decimal
    import string&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nf"&gt;%pylab&lt;/span&gt; &lt;span class="kr"&gt;inline&lt;/span&gt;
&lt;span class="n"&gt;linestarts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_table&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;linestarts&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;txt&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="cp"&gt;#somehow the txtfile has an extra column&lt;/span&gt;
&lt;span class="n"&gt;del&lt;/span&gt; &lt;span class="n"&gt;linestarts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="nl"&gt;Unnamed&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="cp"&gt;#The redirect condition is that there is only one line and it began with a &amp;quot;#&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;redir_cond&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;linestarts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;total&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;linestarts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="sc"&gt;&amp;#39;#&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;canon_cond&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;not&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;redir_cond&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;redirs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;linestarts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;redir_cond&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;canonical&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  &lt;span class="n"&gt;linestarts&lt;/span&gt;&lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;canon_cond&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.output_wrapper}
::: {.output}
::: {.output_area}
::: {.prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.output_subarea .output_stream .output_stdout .output_text}
    Populating the interactive namespace from numpy and matplotlib
:::
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .rendered}
::: {.input}
::: {.prompt .input_prompt}
In [6]:
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.input_area}
::: {.highlight}
    canonical['&lt;em&gt;and#per'] = ( canonical['&lt;/em&gt;'] + canonical['#'] ) / canonical['total'] 
    canonical['allper'] = ( canonical['*'] + canonical['#'] + canonical['|'] ) / canonical['total']&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;show_list_plot&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;histplot&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;canonical&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;*and#per&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;].&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;axes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bins&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;blue&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;NOPLACES&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;decimal&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nb"&gt;Decimal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xticks&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;decimal&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nb"&gt;Decimal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;quantize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;NOPLACES&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Listiness - ordered and unordered lists only&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Article frequency&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Frequency of Article List-Percentages of English Wikipedia&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;show_list_and_table_plot&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;histplot&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;canonical&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;allper&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;].&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;axes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bins&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;green&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;NOPLACES&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;decimal&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nb"&gt;Decimal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xticks&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;decimal&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nb"&gt;Decimal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;quantize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;NOPLACES&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Listiness - ordered and undordered lists, tables, and infoboxes&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Article frequency&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Frequency of Article List-Percentages of English Wikipedia&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;:::
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .rendered}
::: {.input}
::: {.prompt .input_prompt}
In [67]:
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.input_area}
::: {.highlight}
    from collections import defaultdict
    lexeme_freq_all = defaultdict(int)
    lexeme_freq_list_only = defaultdict(int)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;exclude&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;string&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;punctuation&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;exclude&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;u&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;–&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;strip_punct&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;ls&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;lower&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;ls&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ch&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;ch&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ow"&gt;in&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;ls&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;ch&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ow"&gt;in&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;exclude&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;ls&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ow"&gt;in&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;canonical&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;&amp;#39;page+title&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nf"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;lexeme&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ow"&gt;in&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;cleaned&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;strip_punct&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lexeme&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;#print&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;lexeme&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;cleaned&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;lexeme_freq_all&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;cleaned&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ow"&gt;in&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;canonical&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;canonical[&amp;#39;allper&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.8&lt;/span&gt;&lt;span class="err"&gt;]&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;&amp;#39;page+title&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nf"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;lexeme&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ow"&gt;in&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;cleaned&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;strip_punct&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lexeme&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;lexeme_freq_list_only&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;cleaned&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="n"&gt;lexemes_all&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;lexeme_freq_all&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;orient&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;index&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;lexemes_list_only&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;lexeme_freq_list_only&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;orient&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;index&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;lexemes_combined&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;lexemes_all&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lexemes_list_only&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;how&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;inner&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;lsuffix&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;_all&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;rsuffix&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;_listiest&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;lexemes_combined&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;u&amp;#39;all&amp;#39;, u&amp;#39;listiest&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;lexemes_combined&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;&amp;#39;ratio (listiest/all)&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;lexemes_combined&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;&amp;#39;listiest&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;lexemes_combined&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;&amp;#39;all&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;:::
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .code_cell .rendered}
::: {.input}
::: {.prompt .input_prompt}
In [70]:
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.input_area}
::: {.highlight}
    lexemes_combined[lexemes_combined["listiest"] &amp;gt; 150].sort(columns="ratio (listiest/all)", ascending=False).head(16)
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.output_wrapper}
::: {.output}
::: {.output_area}
::: {.prompt .output_prompt}
Out[70]:
:::&lt;/p&gt;
&lt;p&gt;::: {.output_html .rendered_html .output_subarea .output_pyout}
::: {style="max-height: 1000px; max-width: 1500px; overflow: auto;"}
                all    listiest   ratio (listiest/all)&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;pri           728    664        0.912088
  vrh           235    210        0.893617
  divisional    410    325        0.792683
  vas           365    279        0.764384
  gornji        229    167        0.729258
  filmography   526    371        0.705323
  secretariat   463    325        0.701944
  numberone     2394   1675       0.699666
  singles       7986   5391       0.675056
  stakes        1149   727        0.632724
  handicap      342    213        0.622807
  billboard     740    454        0.613514
  fia           369    207        0.560976
  iaaf          718    354        0.493036
  grade         895    421        0.470391
  listings      2959   1376       0.465022&lt;/p&gt;
&lt;p&gt;16 rows × 3 columns
:::
:::
:::
:::
:::
:::&lt;/p&gt;
&lt;p&gt;::: {.cell .border-box-sizing .text_cell .rendered}
::: {.prompt .input_prompt}
:::&lt;/p&gt;
&lt;p&gt;::: {.inner_cell}
::: {.text_cell_render .border-box-sizing .rendered_html}
Research questions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;What percentage of articles are above 80% list.&lt;ol&gt;
&lt;li&gt;how does that answer change if we allow tables and infoboxes as well?&lt;/li&gt;
&lt;li&gt;What do the different distributions mean?&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;What are some top occuring word in the listiest articles?&lt;ol&gt;
&lt;li&gt;How does inclusion of infoboxes and tables affect this as well?&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Are there any strong correllations between the different list types?
:::
:::
:::
:::
:::&lt;/li&gt;
&lt;/ol&gt;</content><category term="hacking"></category></entry><entry><title>Sudo Room Virtual Tour</title><link href="https://notconfusing.com/sudo-room-virtual-tour.html" rel="alternate"></link><published>2013-10-30T23:29:00-07:00</published><updated>2013-10-30T23:29:00-07:00</updated><author><name>max</name></author><id>tag:notconfusing.com,2013-10-30:/sudo-room-virtual-tour.html</id><summary type="html">&lt;p&gt;Come with me now on a journey through sudo room. Originally I designed this as a self-guided tour, tour guide but now I'll walkthrough and explain 14 details of the mystical land that sudo has become since I help set it up with Friends close to two years ago. Contains …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Come with me now on a journey through sudo room. Originally I designed this as a self-guided tour, tour guide but now I'll walkthrough and explain 14 details of the mystical land that sudo has become since I help set it up with Friends close to two years ago. Contains some swearing due to excitement.&lt;br&gt;
&lt;iframe src="//www.youtube.com/embed/9Xkf_LK1yM4" height="315" width="560" allowfullscreen frameborder="0"&gt;&lt;/iframe&gt;&lt;/p&gt;</content><category term="hacking"></category></entry><entry><title>10 Questions about #VIAF, #Wikidata and the #World</title><link href="https://notconfusing.com/10-questions-about-viaf-wikidata-and-the-world.html" rel="alternate"></link><published>2013-09-06T22:36:00-07:00</published><updated>2013-09-06T22:36:00-07:00</updated><author><name>max</name></author><id>tag:notconfusing.com,2013-09-06:/10-questions-about-viaf-wikidata-and-the-world.html</id><summary type="html">&lt;p&gt;&lt;a href="https://apis.google.com/u/0/102002715844393834165"&gt;Gerard Meijssen&lt;/a&gt; blogs a lot about Wikidata, and has asked me to answer 10 questions about VIAF, which my bots made &lt;a href="https://www.wikidata.org/wiki/Wikidata:Database_reports/Popular_properties"&gt;12th most used Wikidata property&lt;/a&gt; (as of September 2013). Here is the a snippet of the interview, visit Gerard's blog to read in full &lt;a href="http://ultimategerardm.blogspot.nl/2013/09/10-questions-about-viaf-wikidata-and.html"&gt;http://ultimategerardm.blogspot.nl/2013 …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://apis.google.com/u/0/102002715844393834165"&gt;Gerard Meijssen&lt;/a&gt; blogs a lot about Wikidata, and has asked me to answer 10 questions about VIAF, which my bots made &lt;a href="https://www.wikidata.org/wiki/Wikidata:Database_reports/Popular_properties"&gt;12th most used Wikidata property&lt;/a&gt; (as of September 2013). Here is the a snippet of the interview, visit Gerard's blog to read in full &lt;a href="http://ultimategerardm.blogspot.nl/2013/09/10-questions-about-viaf-wikidata-and.html"&gt;http://ultimategerardm.blogspot.nl/2013/09/10-questions-about-viaf-wikidata-and.html.&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;div&gt;

&lt;p&gt;Please describe what VIAF is and why it is relevant&lt;/p&gt;
&lt;/div&gt;

&lt;blockquote&gt;
&lt;p&gt;We've all joked about what it would be like if people had numbers instead of names. The funny thing is though it would be much more convenient to organize our information about people if we did have numbers as well as names. National Libraries have already done this behind the scenes to make the reader's life easier. It's just that each National Library did it independently, so VIAF (the Virtual International Authority File) matches the National Libraries to each other.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div&gt;

&lt;p&gt;In Wikidata links to VIAF and other repositories are mentioned why is this done.&lt;/p&gt;
&lt;/div&gt;

&lt;blockquote&gt;
&lt;p&gt;Wikidata, however great and revolutionary it will be, is not the first big online database. What's cooler than any one big online database, is connecting their records up with "same-as" and other relation, so we can use the databases together. Putting VIAF IDs in Wikidata allows operations between the two databases.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://notconfusing.com/images/uploads/2013/09/SexStats.png"&gt;&lt;img alt="SexStats" class="alignnone size-large wp-image-313" src="https://notconfusing.com/images/uploads/2013/09/SexStats.png" style="width:1024px !important"&gt;&lt;/a&gt;&lt;/p&gt;</content><category term="hacking"></category></entry><entry><title>Wikidata Babel Multi-Language Display Easter Egg</title><link href="https://notconfusing.com/wikidata-babel-multi-language-display-easter-egg.html" rel="alternate"></link><published>2013-08-14T08:50:00-07:00</published><updated>2013-08-14T08:50:00-07:00</updated><author><name>max</name></author><id>tag:notconfusing.com,2013-08-14:/wikidata-babel-multi-language-display-easter-egg.html</id><summary type="html">&lt;p&gt;At Wikimania 2013 Hong Kong during a Wikidata presentation, there was a small nod to a hidden but useful feature of Wikidata - multi-language display. Wikidata items are language-agnostic in concept, they are identified by their Q-ID which is displayed in the URL. Depending on which language your interface is set …&lt;/p&gt;</summary><content type="html">&lt;p&gt;At Wikimania 2013 Hong Kong during a Wikidata presentation, there was a small nod to a hidden but useful feature of Wikidata - multi-language display. Wikidata items are language-agnostic in concept, they are identified by their Q-ID which is displayed in the URL. Depending on which language your interface is set to independent &lt;em&gt;labels,&lt;/em&gt; &lt;em&gt;descriptions, and aliases&lt;/em&gt; will be displayed in that language. With this feature you can enable the &lt;em&gt;labels,&lt;/em&gt; &lt;em&gt;descriptions, and aliases&lt;/em&gt; of many languages to appear at once.&lt;/p&gt;
&lt;p&gt;This is a boon, because it allows you to better peer into the structural translation the names of over 14,000,000 Wikidata concepts into 270 languages. Needless to say such a corpus will be really useful for a variety of multilingual research. In fact I have already populated over 14,000 Wikidata labels and aliases, by bot, using data from the Library of Congress, as I explain later on. 14,000 labels and aliases are really a drop in the bucket, which is my this small-work-unit task is so necessary to activate.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://notconfusing.com/images/uploads/2013/08/screen1.png"&gt;&lt;img alt="Default view of Wikidata with Language set to English." class="size-large wp-image-289" src="https://notconfusing.com/images/uploads/2013/08/screen1.png" style="width:1024px !important"&gt;&lt;/a&gt; Default view of Wikidata with Language set to English.&lt;/p&gt;
&lt;p&gt;Douglas Adams is a favourite target for demonstrations on Wikidata because his Q-ID is easy to remember if you're geeky enough. If you click on the language selector in the top row, you could change your language to see Wikidata, and item labels, descriptions and aliases displayed in another language.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://notconfusing.com/images/uploads/2013/08/screen4.png"&gt;&lt;img alt="The language selector active." class="size-large wp-image-294" src="https://notconfusing.com/images/uploads/2013/08/screen4.png" style="width:1024px !important"&gt;&lt;/a&gt; The language selector active.&lt;/p&gt;
&lt;p&gt;Now, in order to enable the multiple languages at the same time, we make a customization to our user page. Go to your user page which is also in the top row of links and edit the page. In addition to any text there we add at line of code which will have the form:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="cp"&gt;{{&lt;/span&gt;&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="nv"&gt;babel&lt;/span&gt;&lt;span class="o"&gt;:(&lt;/span&gt;&lt;span class="nv"&gt;language&lt;/span&gt; &lt;span class="nv"&gt;code&lt;/span&gt;&lt;span class="o"&gt;)-(&lt;/span&gt;&lt;span class="nv"&gt;level&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="err"&gt;|&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;language&lt;/span&gt; &lt;span class="nv"&gt;code&lt;/span&gt;&lt;span class="o"&gt;)-(&lt;/span&gt;&lt;span class="nv"&gt;level&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="err"&gt;|...&lt;/span&gt;&lt;span class="cp"&gt;}}&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Language codes are short strings representing languages, you can use any &lt;a href="http://www.wikidata.org/wiki/Q917906#sitelinks-wikipedia"&gt;ISO 639-1 code&lt;/a&gt;. Level is a rating from 0-5 or &lt;em&gt;N&lt;/em&gt;, with 0 being not able to speak, 5 being a professional translator, and &lt;em&gt;N&lt;/em&gt; being native. The level setting will change the graphical display on your user page but has no effect on your item-view, it's simply controlled by which languages are present in this babel box.&lt;/p&gt;
&lt;p&gt;In my case for instance I have:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="cp"&gt;{{&lt;/span&gt;&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="nv"&gt;babel&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="nv"&gt;en-N&lt;/span&gt;&lt;span class="o"&gt;|&lt;/span&gt;&lt;span class="nf"&gt;he&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="o"&gt;|&lt;/span&gt;&lt;span class="nf"&gt;es&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="o"&gt;|&lt;/span&gt;&lt;span class="nf"&gt;jbo&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="cp"&gt;}}&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="Adding the call to babel in your user page." class="size-large wp-image-290" src="https://notconfusing.com/images/uploads/2013/08/screen2.png" style="width:1024px !important"&gt; Adding the call to babel in your user page.&lt;/p&gt;
&lt;p&gt;After we save our user page, we can go back to the Douglas Adams item. And see that there is a new page section displayed &lt;em&gt;In other languages&lt;/em&gt;. For each language in your babel box, you will have the chance to edit the label and description.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://notconfusing.com/images/uploads/2013/08/screen3.png"&gt;&lt;img alt="The new view with Babel active. Able to edit in Spanish, Hebrew, and Lojban as well." class="size-large wp-image-291" src="https://notconfusing.com/images/uploads/2013/08/screen3.png" style="width:1024px !important"&gt;&lt;/a&gt; The new view with Babel active. Able to edit in Spanish, Hebrew, and Lojban as well.&lt;/p&gt;
&lt;p&gt;If you speak multiple languages this is useful because even if you cannot read the description - or one does not exist - in a particular language you may be able to read it in another language that you also know. Plus, if you feel so inclined you can quickly add to the label completeness goal with minimal clicks.&lt;/p&gt;
&lt;p&gt;You can probably tell by now that I'm obsessed with label completeness. It's important for understanding what we don't understand. Even though there are 4 million articles in English Wikipedia, there are 10 million more Wikidata items. With label completeness we can understand which articles exist in other languages but not in our own. Knowing what you don't know is the first hint to really expanding your knowledge into non-obvious areas.&lt;/p&gt;
&lt;p&gt;I tried to automate solving part of this problem. As I connect over 400,000 Wikidata items with their &lt;a href="http://viaf.org/"&gt;VIAF&lt;/a&gt; IDs, and VIAF has information about alternate names in different languages, there was some promise. For each &lt;a href="https://www.wikidata.org/wiki/Special:WhatLinksHere/Property:P214"&gt;item with an associated VIAF ID&lt;/a&gt; I found the potential alternative name. Then I used python to determine the probable language of string of text. If the alternate name was in a language that didn't already have a Wikidata label or alias, I checked Levenshtein distance of the label and alias versus the alternate name. If it was less that 65% similar - a threshold I determined by training a Bayesian filter - then I would write that label or alias. Here are the results: new AKA means that an alias was added, new label the same, had AKA means that the alias was available but greater than 65% similar, and likewise a label could be too similar.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://notconfusing.com/images/uploads/2013/08/alias_results.png"&gt;&lt;img alt="The results of VIAFbot, task 4, adding aliases from Library of Congress" class="size-full wp-image-292" src="https://notconfusing.com/images/uploads/2013/08/alias_results.png" style="width:780px !important"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;dl class="wp-caption alignnone" id="attachment_292" style="width: 790px;"&gt;

&lt;dd class="wp-caption-dd"&gt;

The results of VIAFbot, task 4, adding aliases from Library of Congress


&lt;/dd&gt;


&lt;/dl&gt;

&lt;p&gt;Remember also to see my post about finding the &lt;a href="http://notconfusing.com/a-word-for-every-lanaguage-in-every-language/" title="A word FOR every Lanaguage IN every Language"&gt;label in every language for each language&lt;/a&gt; . Soon with babel, we might have label completion.&lt;/p&gt;</content><category term="hacking"></category></entry><entry><title>Referential</title><link href="https://notconfusing.com/referrential.html" rel="alternate"></link><published>2013-07-05T21:25:00-07:00</published><updated>2013-07-05T21:25:00-07:00</updated><author><name>max</name></author><id>tag:notconfusing.com,2013-07-05:/referrential.html</id><summary type="html">&lt;p&gt;You might be perusing through the latest issue of &lt;a href="http://www.worldcat.org/search?q=n2%3A0144-2384&amp;amp;qt=advanced&amp;amp;dblist=638"&gt;Refer Journal&lt;/a&gt; and come across my latest article &lt;em&gt;Wikipedia in the Library&lt;/em&gt;. Andrew Gray of the British Library and I focus on the need and opportunity of bringing Library data in Wikipedia. Form the introduction,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Wikipedia has traditionally been a divisive …&lt;/p&gt;&lt;/blockquote&gt;</summary><content type="html">&lt;p&gt;You might be perusing through the latest issue of &lt;a href="http://www.worldcat.org/search?q=n2%3A0144-2384&amp;amp;qt=advanced&amp;amp;dblist=638"&gt;Refer Journal&lt;/a&gt; and come across my latest article &lt;em&gt;Wikipedia in the Library&lt;/em&gt;. Andrew Gray of the British Library and I focus on the need and opportunity of bringing Library data in Wikipedia. Form the introduction,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Wikipedia has traditionally been a divisive topic among librarians and academics. Its goal is undeniably positive and almost utopian – access to all of human knowledge, in every language, offered freely to the world. In practice, however, it can typify “the problem of the internet” - a morass of disorganised information, of dubious accuracy and reliability, offered up without &lt;strong&gt;&lt;em&gt;authority or control&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Adding identifiers such as Authority Control has been my main focus for the last year. It's not the most obvious backbone support for a Wikipedia article, but for those who want to move past just &lt;a href="www.wikidata.org/wiki/Q1777902"&gt;satisficing&lt;/a&gt; themselves it's key. When you get drawn into the debate around the reliability of Wikipedia you are tempted to choose a side in the warring factions. But of course the answer is to use both for what they're worth. To link both we need identifiers, as we write,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The presence of the identifiers allows external services to link into Wikipedia, allows Wikipedia articles to reliably link out to library-hosted records, and helps curate and control the organisation of the articles themselves.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Read more in you're library. REFER / JOURNAL OF THE ISG / Vol. 29, No 2 / Summer 2013 / ISSN 0144-2384&lt;/p&gt;
&lt;p&gt;&lt;a href="https://notconfusing.com/images/uploads/2013/07/isglogo.jpg"&gt;&lt;img alt="isglogo" class="alignnone size-medium wp-image-278" src="https://notconfusing.com/images/uploads/2013/07/isglogo.jpg" style="width:300px !important"&gt;&lt;/a&gt;&lt;/p&gt;</content><category term="hacking"></category></entry><entry><title>The Most Unique Wikipedias According To Wikidata</title><link href="https://notconfusing.com/the-most-unique-wikipedias-according-to-wikidata.html" rel="alternate"></link><published>2013-06-12T20:14:00-07:00</published><updated>2013-06-12T20:14:00-07:00</updated><author><name>max</name></author><id>tag:notconfusing.com,2013-06-12:/the-most-unique-wikipedias-according-to-wikidata.html</id><summary type="html">&lt;p&gt;If you read Wikipedia in a more than one language you'll have noticed the sidebar sometimes ready to link you to the topic of the current article in one or more other languages. If you've been following the trends you'll know that Wikidata is now in charge of keeping these …&lt;/p&gt;</summary><content type="html">&lt;p&gt;If you read Wikipedia in a more than one language you'll have noticed the sidebar sometimes ready to link you to the topic of the current article in one or more other languages. If you've been following the trends you'll know that Wikidata is now in charge of keeping these language links in order. (To understand more about how Wikidata works watch my &lt;a href="http://www.youtube.com/watch?v=cMMzfG9LITc&amp;amp;feature=youtu.be"&gt;youtube tutorial&lt;/a&gt; starting at 5:15) One upshot of that is that we can easily count these links and understand more about the Wikipedia projects - like how "unique" different Wikipedias are. I &lt;strong&gt;define&lt;/strong&gt; a &lt;strong&gt;unique Wikidata Item&lt;/strong&gt; &lt;strong&gt; of a &lt;/strong&gt;language X** to be a Wikidata Item that has only one language link, and the language link is in language X. I also find the total number of items that have occurrences of language X in their language links. Below I graph then total items against unique items for every Wikipedia language, first on a linear scale and then on logarithmic scale.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://notconfusing.com/images/uploads/2013/06/LangLinks_linear.png"&gt;&lt;img alt="" class="wp-image-258" src="https://notconfusing.com/images/uploads/2013/06/LangLinks_linear.png" style="width:512px !important" title="LangLinks_linear"&gt;&lt;/a&gt; Wikidata languages comparing unique versus total items. Linear scale. [Click to expand] &lt;/p&gt;
&lt;p&gt;&lt;a href="https://notconfusing.com/images/uploads/2013/06/LangLinks_log.png"&gt;&lt;img alt="" class="wp-image-259" src="https://notconfusing.com/images/uploads/2013/06/LangLinks_log.png" style="width:512px !important" title="LangLinks_log"&gt;&lt;/a&gt; Wikidata languages comparing unique versus total items. Logarithmic scale. [Click to expand]We can see how far and away English is in both absolute uniques and total items, that's unsurprising. What's curious though is that you can see from the linear plot, the a curve fitting the data appears to be roughly parabolic or exponential. That would indicate that the more total items a language has, the greater the chance it that those items are unique. This might seem obvious, but it doesn't neccesarily have to be. It could be that the English Items were half covered by German, and the other half French for instance (but it isn't).&lt;br&gt;
If you look at the Logarithmic plot, you'll still see the same line of best fit in blue, which represents the expected level of unique items for a Wikipedia at a given total size. If a Wikipedia lies above the line, it's more unique than expected, and if it lies below the line its less unique than expected. For instance at the high end of the total axis German and Chinese show higher than expected uniqueness, and Dutch and Polish are slightly under. It's fun if you have the time to draw an imaginary vertical line in the middle of the plot and see at a fixed total size the varying uniquenesses of different Wikipedias.&lt;br&gt;
By now you might be asking to look at these Wikipedias as ratio of [Unique Items] / [Total Items]. Below it a plot doing just that. They are ordered and coloured on how large their total size is a percentage of English's total size. Then they're split into two categories, wikis with more than 100,000 items and those with less.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://notconfusing.com/images/uploads/2013/06/Uniquenesses.png"&gt;&lt;img alt="" class="wp-image-261" src="https://notconfusing.com/images/uploads/2013/06/Uniquenesses.png" style="width:512px !important" title="Uniquenesses"&gt;&lt;/a&gt; Uniqueness percentage of Wikipedias ordered by total size. [Click to expand]A good way to read this chart is to look at the tallest bars compared how far left they are. If two bars are of equal heights the one on the left is coming from a smaller total Wikipedia, and is therefore more impressive at having that uniqueness ratio. Of the 100,000 or more category, English sets the standard at 49% unique. However Arabic Wikipedia is a high performer in its class because its 35.77% uniqueness comes in the middle of the pack.&lt;/p&gt;
&lt;p&gt;UPDATE: By request I replotted the above uniqunesses. but with the X axis ordered by &lt;a href="http://stats.wikimedia.org/EN/Sitemap.htm"&gt;usage defined by hourly page views&lt;/a&gt;. It seems like there is a higher correlation this way too. (Credit for this intuition goes to Italian Wikipedian "User:Nemo_bis")&lt;/p&gt;
&lt;p&gt;&lt;a href="https://notconfusing.com/images/uploads/2013/06/Uniqunesses-by-Usage.png"&gt;&lt;img alt="" class="wp-image-269" src="https://notconfusing.com/images/uploads/2013/06/Uniqunesses-by-Usage.png" style="width:512px !important" title="Uniqunesses by Usage"&gt;&lt;/a&gt; Uniqueness percentage of Wikipedias ordered by hourly page views. [Click to expand]Again Arabic does well, and German Wikipedia becomes more impressive as it less visited than it has total articles.&lt;/p&gt;
&lt;p&gt;Now let's go back to our definition of uniqueness - an item with just one language link. We can ask the question, how much of Wikidata only has 1 language link? And how much has 2, 3, 4, 5, 6 language links, .... all the way to 286, the maximum since there are 286 Wikipedia languages. In fact there is only one page in all the 286 items, the &lt;a href="https://www.wikidata.org/wiki/Q5296"&gt;"Main Page"&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here's the composition of Wikidata by the number of language links of each item. I've broken it up into 4 plots of increasing zoom, and coloured it to show its fractal nature.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://notconfusing.com/images/uploads/2013/06/Composition_zoom-effect.png"&gt;&lt;img alt="" class="wp-image-257" src="https://notconfusing.com/images/uploads/2013/06/Composition_zoom-effect.png" style="width:512px !important" title="Composition_zoom effect"&gt;&lt;/a&gt; Composition of Wikidata by the number of language links of each item. [Click to enlarge]I define an &lt;strong&gt;n-cluster&lt;/strong&gt; to be the set of items that have &lt;strong&gt;n language links&lt;/strong&gt;. Two-thirds of Wikidata items are in the 1-cluster. 13.3% of Wikidata exists as a 2-cluster, 6.2% are a 3-cluster, and 3.4% are a 4-cluster. The lowest n-cluster to have no items is the 193-cluster, for comparison the 192-cluster has 10 items, and the 194-cluster has 4.&lt;/p&gt;
&lt;p&gt;All of this goes to show that if you picked a random article on a random Wikipedia, statistically it probably doesn't have an equivalent article in another language. However from empirical evidence, it seems otherwise, normally there is an interlanguage link. One hypothesis to explain this is that the articles we tend to read are usually more general interest. That suggests another more difficult question to answer, comparing n-clusters to page views. (Maybe a good topic for my next research.)&lt;/p&gt;
&lt;p&gt;For now lets turn our attention to another part of the composition graph. Since we've already looked at unique items, let's inspect the 2- and 3-clusters: "pairs" and "triples".&lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="alignnone size-medium wp-image-256" src="https://notconfusing.com/images/uploads/2013/06/Composition_focus-effect.png" style="width:300px !important" title="Composition_focus effect"&gt;&lt;/p&gt;
&lt;p&gt;There are 1,578,043 items in the 2-cluster, and 735,573 in the 3-cluster. Each of those items are are cross of two or three languages. For now we'll focus on the the top 20 pairs and triples. Here they are, coloured to show whether English is in the cluster.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://notconfusing.com/images/uploads/2013/06/Pairs-And-Triples.png"&gt;&lt;img alt="" class="wp-image-260" src="https://notconfusing.com/images/uploads/2013/06/Pairs-And-Triples.png" style="width:512px !important" title="Pairs And Triples"&gt;&lt;/a&gt; Comparison of 20 highest occurring pairs and triples in Wikidata. [Click to expand]A lot of these pairs intuitively make sense from a cultural standpoint: English-German, Russian-Ukranian, Japanese-Chinese. As do some of the triples: English-German-French, English-French-Italian, Russian-Kazak-Bashkir.&lt;/p&gt;
&lt;p&gt;Some are more perplexing, especially the high prominence of Vietnamese, Cebuano, and Waray permuted with Swedish and Dutch. Were there some prolific translators? Machine translation bots? Is Wikidata not reflecting Wikipedia fully? Perhaps you can explain those correlations? In fact here is the data accurate as of 1 June 2013, and &lt;a href="https://github.com/notconfusing/langlink-analysis"&gt;code available on github.&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;UPDATE: User Zolo on Wikidata wrote to me "I'll point out here that the Vietnamese-Dutch (and perhaps Cebuano-Waray-Swedish) cluster is most probably due to the high number of items about taxons. That also explains why they have relatively few "unique items"."&lt;/p&gt;
&lt;/blockquote&gt;</content><category term="hacking"></category></entry><entry><title>Just enought Sketchup II</title><link href="https://notconfusing.com/just-enought-sketchup-ii.html" rel="alternate"></link><published>2013-04-24T01:19:00-07:00</published><updated>2013-04-24T01:19:00-07:00</updated><author><name>max</name></author><id>tag:notconfusing.com,2013-04-24:/just-enought-sketchup-ii.html</id><summary type="html">&lt;p&gt;&lt;a href="https://notconfusing.com/images/uploads/2013/04/justenough1.jpg"&gt;&lt;img alt="" class="alignnone size-medium wp-image-235" src="https://notconfusing.com/images/uploads/2013/04/justenough1.jpg" style="width:300px !important" title="justenough1"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;My follow up class to Just Enough Sketchup (creatively entitle &lt;a href="https://sudoroom.org/wiki/page/Just_enough_sketchup_to_pretend_you_can_3d_model_II_%26_Just_enough_slic3r_to_pretend_you_can_extrude_I"&gt;Just Enough Sketchup II&lt;/a&gt; ) was on Saturday April 20th 2013. As you can see, the turnout doesn't still hasn't failed to impress. I wonder how elastic the audience would be if the price wasn't free? Nevermind, we had 4 glorious …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://notconfusing.com/images/uploads/2013/04/justenough1.jpg"&gt;&lt;img alt="" class="alignnone size-medium wp-image-235" src="https://notconfusing.com/images/uploads/2013/04/justenough1.jpg" style="width:300px !important" title="justenough1"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;My follow up class to Just Enough Sketchup (creatively entitle &lt;a href="https://sudoroom.org/wiki/page/Just_enough_sketchup_to_pretend_you_can_3d_model_II_%26_Just_enough_slic3r_to_pretend_you_can_extrude_I"&gt;Just Enough Sketchup II&lt;/a&gt; ) was on Saturday April 20th 2013. As you can see, the turnout doesn't still hasn't failed to impress. I wonder how elastic the audience would be if the price wasn't free? Nevermind, we had 4 glorious hours of learning Sketchup, two of which we spent printing. (Regretfully uncaptured by camera).&lt;/p&gt;
&lt;p&gt;&lt;a href="https://notconfusing.com/images/uploads/2013/04/justenough2.jpg"&gt;&lt;img alt="" class="alignnone size-medium wp-image-236" src="https://notconfusing.com/images/uploads/2013/04/justenough2.jpg" style="width:300px !important" title="justenough2"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The most notable aspect of this round of 3D printing extravaganzing, was that we were graced by two Middle School Teachers and two Middle School Students. In their capacity to learn Sketchup, I was highly impressed. Children, I'm now recalling are almost humans in their own right?! Only with the plasticity of mind that allows you to download an entire set of program commands in to their brian in a single sitting, have them creatively recombine.&lt;/p&gt;
&lt;p&gt;Another item of note was that I was able to question these teachers for feedback on the intructional design of my class. My inability to lead everyone together had worried me, because I must have understood from school that everyone must strictly follow the teacher at their pace. But the two teachers complimented me on supporting free learning by having a central tutorial that students were free to abide, or leave at their own pace.&lt;/p&gt;
&lt;p&gt;Are there more 3D printing classes in the works? I'm inclined to set up a more casual weekly, or fortnightly print club. Any takers?&lt;/p&gt;</content><category term="hacking"></category></entry><entry><title>Just enough Sketchup to pretend you can 3D print</title><link href="https://notconfusing.com/just-enough-sketchup-to-pretend-you-can-3d-print.html" rel="alternate"></link><published>2013-03-21T20:25:00-07:00</published><updated>2013-03-21T20:25:00-07:00</updated><author><name>max</name></author><id>tag:notconfusing.com,2013-03-21:/just-enough-sketchup-to-pretend-you-can-3d-print.html</id><summary type="html">&lt;p&gt;As part of the Sudo Room's &lt;a href="https://sudoroom.org/wiki/Today_I_Learned"&gt;&lt;em&gt;Today I learned&lt;/em&gt;&lt;/a&gt; series one-off classes, workshops, and talks led by members of the Sudo Room community, I threw in my cap to teach &lt;em&gt;&lt;a href="https://sudoroom.org/wiki/Just_enough_Sketch-up_to_pretend_you_can_3d_model"&gt;Just enough Sketchup to pretend you can 3D print. This is how I capitulated it in advertising:&lt;br&gt;
&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Glance&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;WHEN&lt;/strong&gt; 2pm …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;As part of the Sudo Room's &lt;a href="https://sudoroom.org/wiki/Today_I_Learned"&gt;&lt;em&gt;Today I learned&lt;/em&gt;&lt;/a&gt; series one-off classes, workshops, and talks led by members of the Sudo Room community, I threw in my cap to teach &lt;em&gt;&lt;a href="https://sudoroom.org/wiki/Just_enough_Sketch-up_to_pretend_you_can_3d_model"&gt;Just enough Sketchup to pretend you can 3D print. This is how I capitulated it in advertising:&lt;br&gt;
&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Glance&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;WHEN&lt;/strong&gt; 2pm on Saturday the 16th of March 2013.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DURATION&lt;/strong&gt; 2 hours&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;LOCATION&lt;/strong&gt; &lt;a href="https://sudoroom.org/wiki/Getting_there" title="Getting there"&gt;sudo room&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PRICE&lt;/strong&gt; \$0&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NUTSHELL&lt;/strong&gt; Live Sketchup and print tutorial&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://notconfusing.com/images/uploads/2013/03/workshop1.jpg"&gt;&lt;img alt="" class="alignnone size-medium wp-image-223" src="https://notconfusing.com/images/uploads/2013/03/workshop1-e1363716962353.jpg" style="width:225px !important" title="workshop1"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Participation&lt;/h2&gt;
&lt;p&gt;Owing to a combination of recent hype on the subject, relative ignorance of the process, and unbeatable price-point, the workshop was well attended at 15 mouse-clickers. Happily, those computer users, were in large part not preexisting sudo room members, which is precisely the point of &lt;em&gt;Today I learned&lt;/em&gt; - a hands-dirty introduction to the hackerspace. The participants were also diverse in age, and sex which is either reassuring to the activity or persona that sudo room is trying to project.&lt;br&gt;
[pic from vicky]&lt;/p&gt;
&lt;h2&gt;Results&lt;/h2&gt;
&lt;p&gt;It was ambitious to try move the group from varying states of noob-ness to Shape Sheikhs. After the basic and abstract tutorial, I planned to make a put those skills to work, and make a model to print. A variant on the laugh-inducing &lt;a href="http://www.thingiverse.com/image:142834"&gt;fingerplate&lt;/a&gt;. Yet in spur of confidence I took suggestions from the participants for a shape, and then we had a vote. We were equally split between the adventurers in fingerplate, and those who wanted to hone on simpler objects first. Thus two factions were made, and Miloh from &lt;a href="http://typeamachines.com/"&gt;Type A Machines&lt;/a&gt; who was attempting to master his pedagogy of 3D printing, took some leadership. Even with the split, the natural effective attention span and fatigue limits of the average human meant that most left without moving from design to print. Strong parting encouragement from the pupils means that a second installment will be on the way.&lt;/p&gt;
&lt;p&gt;Yet one particularly dedicated &lt;a href="https://twitter.com/hbergeronx"&gt;hbergronx&lt;/a&gt; saw the process through, past the setting of the sun. In what may be a rite of passage, that mimics my own fascination with understanding the machine at an early stage, he made an interlocking torus (tori?) design. "Reproducing offspring" is a concept famous in the 3D world, where 3D printers can print themselves. Its original meaning is apposite here too where a generation of humans can teach one another to 3D print.&lt;br&gt;
&lt;a href="https://notconfusing.com/images/uploads/2013/03/Sudoroom_prints_tori.jpg"&gt;&lt;img alt="" class="alignnone size-medium wp-image-219" src="https://notconfusing.com/images/uploads/2013/03/Sudoroom_prints_tori.jpg" style="width:223px !important" title="Sudoroom_prints_tori"&gt;&lt;/a&gt;&lt;br&gt;
&lt;a href="https://notconfusing.com/images/uploads/2013/03/Sudoroom_prints_tori2.jpg"&gt;&lt;img alt="" class="alignnone size-medium wp-image-220" src="https://notconfusing.com/images/uploads/2013/03/Sudoroom_prints_tori2.jpg" style="width:223px !important" title="Sudoroom_prints_tori2"&gt;&lt;/a&gt;&lt;/p&gt;</content><category term="hacking"></category></entry><entry><title>Haklab Belgrade Report</title><link href="https://notconfusing.com/haklab-belgrade-report.html" rel="alternate"></link><published>2013-02-09T20:30:00-08:00</published><updated>2013-02-09T20:30:00-08:00</updated><author><name>max</name></author><id>tag:notconfusing.com,2013-02-09:/haklab-belgrade-report.html</id><summary type="html">&lt;p&gt;As a self-described 'post-Lonely Planet' traveller, I've been searching for new praxes of global adventure that make good excuses to run amok. For the months of January and February I toured Lisbon, Madrid, Modena, Bologna, Split, Belgrade, and Berlin, trying to understand their local hackerspaces. An example I was particulary …&lt;/p&gt;</summary><content type="html">&lt;p&gt;As a self-described 'post-Lonely Planet' traveller, I've been searching for new praxes of global adventure that make good excuses to run amok. For the months of January and February I toured Lisbon, Madrid, Modena, Bologna, Split, Belgrade, and Berlin, trying to understand their local hackerspaces. An example I was particulary fond of was Haklab Belgrade. Lessons to bring back to my budding Sudo Room hackerspace were abound there. I most thoroughly enjoyed  the recounting and education about the former Yugoslavia and pontifications about that history affects modern organizing.&lt;/p&gt;
&lt;p&gt;.&lt;a href="https://notconfusing.com/images/uploads/2013/03/hacklabbelgrade1.jpg"&gt;&lt;img alt="" class="alignnone size-large wp-image-208" src="https://notconfusing.com/images/uploads/2013/03/hacklabbelgrade1.jpg" style="width:1024px !important" title="hacklabbelgrade"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Existing not in commercial space at all, but a donated first-floor apartment, Hacklab, if used for it's orginal intention, would be warming, showering, and dishwashing the everyday lives of at most 2 Serbian citizens. As it is, the living room and bedroom house the main hack area, which when I arrived was the combined chamber for 6 serbs attending the the weekly python workshop. In the middle of git-pushing their work and rearranging the furniture for that night's screening of the torrented "The Pirate Bay, Away From Keyboard," I managed to strike up a few interviews with the ragamuffins in exchange for some swigs of the 2.5L bottle of lager, and local wax-topped 'Rakija'.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Read the full report at the &lt;a href="http://sudoroom.org/wiki/Hacklab_Belgrade_2013_Report"&gt;sudo room wiki&lt;/a&gt;&lt;/p&gt;</content><category term="hacking"></category></entry><entry><title>Open Access Wikipedia Challenge on P2PU</title><link href="https://notconfusing.com/open-access-wikipedia-challenge-on-p2pu.html" rel="alternate"></link><published>2012-10-22T18:40:00-07:00</published><updated>2012-10-22T18:40:00-07:00</updated><author><name>max</name></author><id>tag:notconfusing.com,2012-10-22:/open-access-wikipedia-challenge-on-p2pu.html</id><summary type="html">&lt;p&gt;It's been traditional recently to hold &lt;a href="http://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Loves_Libraries"&gt;Wikipedia Loves Libraries&lt;/a&gt; events during &lt;a href="www.openaccessweek.org/"&gt;Open Access Week&lt;/a&gt;, and I fully support the practice. What's also been traditional in a way that I wanted to change was the &lt;em&gt;editathon &lt;/em&gt;format for those events. After scrunching my mind to brainstorm new experimental ways of holding …&lt;/p&gt;</summary><content type="html">&lt;p&gt;It's been traditional recently to hold &lt;a href="http://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Loves_Libraries"&gt;Wikipedia Loves Libraries&lt;/a&gt; events during &lt;a href="www.openaccessweek.org/"&gt;Open Access Week&lt;/a&gt;, and I fully support the practice. What's also been traditional in a way that I wanted to change was the &lt;em&gt;editathon &lt;/em&gt;format for those events. After scrunching my mind to brainstorm new experimental ways of holding these trainings and celebrations, I came up with &lt;a href="https://p2pu.org/en/groups/open-access-wikipedia-challenge/"&gt;&lt;strong&gt;Open Access Wikipedia Challenge&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;. &lt;/strong&gt;  The challenge is to embed media that was harvested from Open Access journals in Wikipedia, and I created a &lt;a href="http://en.wikipedia.org/wiki/Template:Open_Access_Wikipedia_Challenge_Barnstar"&gt;special edition barnstar&lt;/a&gt; for completing it. This challenge is totally friendly to newbies and librarians as it includes over 1 hour total of six screencast tutorial videos that explain every detail right from the account creation, to transclusion, and each module has waypoint challenges.&lt;/p&gt;
&lt;p&gt;Happy Open Access Week and Good Luck on the Challenge,&lt;br&gt;
&lt;iframe src="http://www.youtube-nocookie.com/embed/wpeOgWOGl1Y" style="width:640px !important" frameborder="0"&gt;&lt;/iframe&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" class="alignnone" src="http://upload.wikimedia.org/wikipedia/commons/4/4a/Oaw2.png" style="width:173px !important" title="Open Access Wikipedia Challenge Barnstar"&gt;&lt;/p&gt;</content><category term="hacking"></category></entry><entry><title>Wikipedia Edits Statistics by Category: hacking</title><link href="https://notconfusing.com/wikipedia-edits-statistics-by-category.html" rel="alternate"></link><published>2012-10-02T04:44:00-07:00</published><updated>2012-10-02T04:44:00-07:00</updated><author><name>max</name></author><id>tag:notconfusing.com,2012-10-02:/wikipedia-edits-statistics-by-category.html</id><summary type="html">&lt;p&gt;We've heard that &lt;a href="http://www.cnn.com/2012/08/07/tech/web/romney-vp-pick-wikipedia/index.html"&gt;Wikipedia might be able to predict the Vice President&lt;/a&gt;, but what about the honcho himself?  Using a combination of Python, R, and Wikipedia version histroy, I've delved inside the hivemind and emerged with some graphs that might have the answer. So without further ado, let's try and …&lt;/p&gt;</summary><content type="html">&lt;p&gt;We've heard that &lt;a href="http://www.cnn.com/2012/08/07/tech/web/romney-vp-pick-wikipedia/index.html"&gt;Wikipedia might be able to predict the Vice President&lt;/a&gt;, but what about the honcho himself?  Using a combination of Python, R, and Wikipedia version histroy, I've delved inside the hivemind and emerged with some graphs that might have the answer. So without further ado, let's try and predict the upcoming U.S. election.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2000&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://notconfusing.com/images/uploads/2012/10/PresidentsAlGoreGeorgeBush.png"&gt;&lt;img alt="" class="alignnone size-medium wp-image-128" src="https://notconfusing.com/images/uploads/2012/10/PresidentsAlGoreGeorgeBush.png" style="width:300px !important" title="PresidentsAlGoreGeorgeBush"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Wikipedia didn't come to be until after the 2000 elections, so we can't glean a lot of good data here. And although Bush has basically always outpaced Gore in editor interest, presidents seem to always get a lot more attention &lt;em&gt;after&lt;/em&gt; they're elected as you'll see.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2004&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://notconfusing.com/images/uploads/2012/10/PresidentsJohnKerryGeorgeBush.png"&gt;&lt;img alt="" class="alignnone wp-image-129" src="https://notconfusing.com/images/uploads/2012/10/PresidentsJohnKerryGeorgeBush.png" style="width:300px !important" title="PresidentsJohnKerryGeorgeBush"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kerry and Bush battled on the encyclopedia in a close Roman-chariot-esque way in the year leading up to November 2004. But after a summer-surge by Kerry, the Bush pundits (or possibly even Bush himself) took to the keyboard and found glory.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2008&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://notconfusing.com/images/uploads/2012/10/PresidentsJohnMcCainBarackObama1.png"&gt;&lt;img alt="" class="alignnone wp-image-132" src="https://notconfusing.com/images/uploads/2012/10/PresidentsJohnMcCainBarackObama1.png" style="width:300px !important" title="PresidentsJohnMcCainBarackObama"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Obama editors trampled McCain writers in most of the year before the 2008 election, yet in dramatic fashion the silver-coiffed eagle made a late Oct-Nov-December push. Still the bulk of the pre-elections editing seems to have made its point.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2012&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://notconfusing.com/images/uploads/2012/10/PresidentsMittRomneyBarackObama.png"&gt;&lt;img alt="" class="alignnone wp-image-131" src="https://notconfusing.com/images/uploads/2012/10/PresidentsMittRomneyBarackObama.png" style="width:300px !important" title="PresidentsMittRomneyBarackObama"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Here we really see the effects that the new-white-house-smell has on Wikignomes. Yet once that odour becomes ever less fresh, it gives way to Mitt Romney clearly pipping Obama at the moment to the virtual post. Ominous? You decide.&lt;/p&gt;
&lt;p&gt;Notconfusingly yours,&lt;/p&gt;
&lt;p&gt;Max&lt;/p&gt;
&lt;p&gt;And of course the code is all open source and you can get it on &lt;a href="https://github.com/notconfusing/statsFromCategory"&gt;github.&lt;/a&gt; The next steps? Look at templates usages over time?&lt;/p&gt;</content><category term="hacking"></category></entry><entry><title>GLAM Camp: Wikipedians in Residence in London in The British Library</title><link href="https://notconfusing.com/glam-camp-wikipedians-in-residence-in-london-in-the-british-library.html" rel="alternate"></link><published>2012-09-15T14:03:00-07:00</published><updated>2012-09-15T14:03:00-07:00</updated><author><name>max</name></author><id>tag:notconfusing.com,2012-09-15:/glam-camp-wikipedians-in-residence-in-london-in-the-british-library.html</id><summary type="html">&lt;p&gt;I am a Wikipedian in Residence which, unsurprisingly, I often have to explain to people what that actually means. Yet, the occupation is on the rise, and I wonder when the day will come when such an explanation is not necessary. I investigated the history of the number of people …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I am a Wikipedian in Residence which, unsurprisingly, I often have to explain to people what that actually means. Yet, the occupation is on the rise, and I wonder when the day will come when such an explanation is not necessary. I investigated the history of the number of people in this employ while musing at &lt;a href="uk.wikimedia.org/wiki/GLAMcamp_London/September_2012"&gt;GLAM Camp London&lt;/a&gt;, and had this output.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://notconfusing.com/images/uploads/2012/09/WIRSep121.png"&gt;&lt;img alt="A Timeline of Wikipedians in Residence" class="wp-image-109" src="https://notconfusing.com/images/uploads/2012/09/WIRSep121.png" style="width:640px !important" title="Wikipedians in Residence Genesis to September 2012"&gt;&lt;/a&gt; CC-BY-SA Maximilian Klein&lt;/p&gt;</content><category term="hacking"></category></entry><entry><title>Island Hopping the Data Archipelago</title><link href="https://notconfusing.com/island-hopping-the-data-archipelago.html" rel="alternate"></link><published>2012-09-01T14:00:00-07:00</published><updated>2012-09-01T14:00:00-07:00</updated><author><name>max</name></author><id>tag:notconfusing.com,2012-09-01:/island-hopping-the-data-archipelago.html</id><summary type="html">&lt;p&gt;iSchool at Berkeley was a department I revered during my time there lurking in the backs of lecture rooms for special events and timidly knocking on Professor doors, which is why I was so honoured when they &lt;a href="http://www.ischool.berkeley.edu/newsandevents/events/ias/20120824"&gt;invited me to guest-speak at the infamous Friday Seminar&lt;/a&gt;. On August 24th 2012 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;iSchool at Berkeley was a department I revered during my time there lurking in the backs of lecture rooms for special events and timidly knocking on Professor doors, which is why I was so honoured when they &lt;a href="http://www.ischool.berkeley.edu/newsandevents/events/ias/20120824"&gt;invited me to guest-speak at the infamous Friday Seminar&lt;/a&gt;. On August 24th 2012, in front of 20 attendees I talked about my latest passion, creating bidirectional links between Wikipedia and other online databases by *link reciprocation. * &lt;a href="http://www.ibi.hu-berlin.de/institut/personen/petras"&gt;Vivian Petras&lt;/a&gt; gave a suitable opening talk about Europeana language problems form a half-technical perspective, which made a suitable segue into my talk, "Data Archipelago."&lt;/p&gt;
&lt;p&gt;&lt;a href="https://notconfusing.com/images/uploads/2012/09/islandhoppingstructurethumb.jpg"&gt;&lt;img alt="" class="alignnone size-medium wp-image-66" src="https://notconfusing.com/images/uploads/2012/09/islandhoppingstructurethumb.jpg" style="width:300px !important" title="islandhoppingstructurethumb"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Unknowing on how much to delve into details of still-&lt;a href="http://en.wikipedia.org/wiki/Wikipedia:Bots/Requests_for_approval/VIAFbot"&gt;in-review&lt;/a&gt; &lt;a href="https://github.com/notconfusing/VIAFbot"&gt;VIAFbot&lt;/a&gt;, I attempted to strike a middle ground by describing how VIAFbot would work, and subsequently what that could mean for the future of connecting what I call &lt;em&gt;Data Islands&lt;/em&gt;. The method that I propose in this talk is the &lt;em&gt;link reciprocation&lt;/em&gt; technique, whereby an algorithm follows links going into Wikipedia then reflects and reciprocates those links back out to the initiating site. This imitates a &lt;a href="http://www.w3.org/DesignIssues/Topology.html"&gt;bidirectional linking structure&lt;/a&gt;, which the ultimate loser in a hot debate in the creation of the web circa 1990.  A stand-out, Wikipedia always kept &lt;a href="http://en.wikipedia.org/wiki/Help:What_links_here"&gt;bidirectional linking in its internal structure&lt;/a&gt;, but could not by itself  implement this type of link between the exterior Data Islands.  Given VIAFbot's hacked-way of making these cross-site bidirectional links we could create new ways to determine not only our interests, but topics in the intersection of our interests of which we were previously unaware (slides 40-46). As an analogy, I aimed to show that there were practical ways to create informational seaplanes to island-hop what I am creating and terming the &lt;em&gt;Data Archipelago&lt;/em&gt;.&lt;/p&gt;
&lt;iframe src="http://www.slideshare.net/slideshow/embed_code/14138305?hostedIn=slideshare&amp;amp;page=upload" style="width:476px !important" frameborder="0" marginwidth="0" marginheight="0" scrolling="no"&gt;&lt;/iframe&gt;</content><category term="hacking"></category></entry><entry><title>What First Monday is missing.</title><link href="https://notconfusing.com/what-first-monday-is-missing.html" rel="alternate"></link><published>2012-06-20T23:38:00-07:00</published><updated>2012-06-20T23:38:00-07:00</updated><author><name>max</name></author><id>tag:notconfusing.com,2012-06-20:/what-first-monday-is-missing.html</id><summary type="html">&lt;p&gt;Killer Features: The Vitality of Piracy Cultures&lt;/p&gt;
&lt;p&gt;University of California, Berkeley&lt;/p&gt;
&lt;p&gt;Abstract&lt;/p&gt;
&lt;p&gt;As experienced facilitators of an undergraduate discussion course on The Politics of Piracy, the authors are uniquely positioned to contribute analyses of several, specific Piracy Cultures. Questions about the relationships between content creators and consumers are posed to …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Killer Features: The Vitality of Piracy Cultures&lt;/p&gt;
&lt;p&gt;University of California, Berkeley&lt;/p&gt;
&lt;p&gt;Abstract&lt;/p&gt;
&lt;p&gt;As experienced facilitators of an undergraduate discussion course on The Politics of Piracy, the authors are uniquely positioned to contribute analyses of several, specific Piracy Cultures. Questions about the relationships between content creators and consumers are posed to two self-identified pirates, each members of piracy groups. The results indicate creator-consumer relationships are present, but are merely one in a set of crucial motivations for piracy--elements herein called killer features. One case study delves into the world of LUElinks, a private linksharing community that is as much social network as media provider. The other explores the world of BakaBT and anime communities which exist in large part through shared “fansubs”--peer-produced, superior translations of sometimes unlicensed Japanese animation.&lt;/p&gt;
&lt;p&gt;Keywords: Media Piracy, Piracy Cultures, Social Networks, Anime, Fansub, Killer Features, Creator-Consumer Relationship, Communities of Practice&lt;/p&gt;
&lt;p&gt;Introduction&lt;/p&gt;
&lt;p&gt;Consider piracy. Consider the pirates. They are the digital citizens and deviants that operate outside the set confines of institutional rules. While their actions are unbound, they bond together in outsider status, inevitably creating their own cultures as they go. These cultures are not typically inspected. There is a boundary between the Piracy Cultures and the corporate cultures of the media industry. Typically we inspect the shape and movements of the institutionalized industry to see its influence on the gate between the two camps. Yet symmetrically, there exists another vantage point on this divide: the shape and movement of Piracy Cultures. Focusing on Piracy Cultures is an opportunity to dust away questions of legality and examine the piracy--and pirates--for what they are worth, for what they might tell us about the future condition of that border between the media organizations’ domain, and the space exterior.&lt;/p&gt;
&lt;p&gt;At UC Berkeley the border of the student-instructor dynamic has been breached. Undergraduates, through a long-running program, have the opportunity to initiate and facilitate credit-bearing courses. One such course is Politics of Piracy, through which the authors have critically examined piracy as discussion leaders. Author one facilitated the course for four semesters until Fall 2010 and Author two facilitated for the second time this Spring 2011, a total of four years between them. The authors hold unique positions as student-scholars. They are close to many piracy practitioners: some students enrolled in the course and some student-pirates outside the classroom. Additionally they have immersed themselves in the academic field, steeped in piracy research and current events.&lt;/p&gt;
&lt;p&gt;A recent landmark event was the release of Media Piracy in Emerging Economies a report by the Social Sciences Research Council (2011). Summarily they found that “piracy is better conceived as a failure of affordable access to media in legal markets.” While claiming that affordability stands as one of the main tenets to subvert legality in acquiring media, they fail to complete the thought. The Social Sciences Research Council (2011) declares that “increasingly, commercial pirates face the same dilemma as the legal industry: how to compete with free” (p. 53) in their analysis of organized crime’s role in piracy. This would suggest that resellers of pirated content are contending with legitimate distribution on the linear factor of pricing, that the end consumer’s choice is simply a function of money. Yet this curiously arrives at a contradiction, as resellers have not yet become extinct everywhere. As a distribution channel, they must add some incentive to purchase the media in order for their business to exist, even if only geographic convenience. Likewise, the authors of this paper contend “piracy as distribution” possesses (in a re-branding of the popular marketing buzzwords) killer features that differentiate piracy in the spectrum of all available options. Some of those distinguishing features have been analyzed through the lens of the economic concept of commons-based peer-production.&lt;/p&gt;
&lt;p&gt;In The Wealth of Networks, Yochai Benkler (2006) attempts to understand disparate practices of production that challenge canonical economic precepts.  Benkler gives an engineering view of peer-to-peer file-sharing (including piracy) as, “a highly efficient system for storing and accessing data in a computer network.” This is a useful interpretation since it too shaves away legality to approach more essential qualities of a dynamic, rich set of solutions. Further, it encourages an approach Benkler purports throughout his work--to examine critically an unfamiliar or seemingly inexplicable system (with respect to networks) as a creative innovation. The creative innovation while lauded by forward-thinkers, is not an instantly accepted salvation for all, it is subject to what regulators may think of it. They are less likely to be impressed by clever coding alone.&lt;/p&gt;
&lt;p&gt;The regulatory literature comprehends piracy in its relation to copyright expansion as the bolstering of exclusive rights regimes made successful through policy change enabled by industry lobbying. Lawrence Lessig’s Free Culture includes an apt metaphor of the narrative of a long-term, unsuccessful foray into both federal lawsuits and policy reform on copyright (2004). Being 2011 today, seven years distant, one witnesses significant changes in piracy and social use of the internet. A more recent legal work by Annemarie Bridy (2009) expresses the further compounding frustration of both public and private regulatory practices. Bridy analyzes the issues of regulation and enforcement up until today, emphasizing a failure of the regulation of goods, networks, and user behavior in a manifest self-regulation in the user.&lt;/p&gt;
&lt;p&gt;Burdensome regulation has meant that in order to make distribution work for digital goods, large, influential organizations are needed. The complexity of regulation dictates that any satisfying solution will have large overhead, and the organizations have looked to create design and user experience attractions to make palatable the passing of that overhead. As an example, Blythe, Mark and Wright recommend the music industry drop intimidation and embrace enchantment (2008)--a distinctly affective strategy reminiscent of Apple’s “environment” model, and of course, iTunes, the generally lauded “gatekeeper” of music (Wingfield 2007). Other examples include Google Books, which provides an equivalent analogy for the publishing industry. An example closer to our research is the praised Steam, a stand-alone distribution software for computer games (Gambotto-Burke 2007).&lt;/p&gt;
&lt;p&gt;With a growing number of increasingly elaborate digital gatekeepers, one of the internet’s original promises of disintermediation is broken. As outlined by Brown and Duguid (2000) in The Social Life of Information, disintermediation in an increasingly networked society is far from clear. It is argued that a future of disintermediation is a myth. The institutions that lay between the creator and the consumer are not shrinking, and are perhaps expanding. Even if institutions were disintermediating, Brown and Duguid (2000) explain that “it requires a profoundly naive belief in disitermediation to assume that all links that fall between […] are somehow interference in the information channel” (p. 6). The chains of connections that are the information channel from creator to consumer are not simply resistors with each additional link contributing more impedance in the institutional frameworks. But is the same true for information channels outside of the official media intermediaries, for those in Piracy Cultures?&lt;/p&gt;
&lt;p&gt;Given the various approaches to piracy in the academic literature, the authors have discerned that few take on the perspective of understanding the relationship at the core of this issue of Piracy. The creation and exchange of goods, examined as a black box, takes place between creator and consumer. There is some relationship that exists, however mediated, between the originators of some good, and the consumer or end-user. Investigating Piracy Cultures from the hinterland of the pirate’s perspective, to which the authors have a rare point of entry, elucidates valuable understandings of this underlying relationship. On the other hand, research revealed the relationship between creators and consumers, while evident, is merely one in a range of crucial, killer features existing in and of Piracy Cultures.&lt;/p&gt;
&lt;p&gt;Methods&lt;/p&gt;
&lt;p&gt;The research questions at the outset of this paper were: In Piracy Cultures, how important is the relationship between content creator and consumer? What does this relationship look like? How does this play out through intermediaries? How do pirates consider or deal with the issue of compensation?&lt;/p&gt;
&lt;p&gt;Through posing these questions, the suggestion is made that there is value in understanding the nature of the relationship (or lack thereof) between content creators and consumers since it is the fundamental, underlying issue of all forms of piracy. Understanding more fully how some Piracy Cultures view and practice this relationship will thus help inform our general understandings of piracy, which has implications for the role of institutionalized markets, suggests insights into perceptions of policy, and begins to address the question of whether piracy will remain a perpetual counter-culture.&lt;/p&gt;
&lt;p&gt;The hypothesis held prior to research was that in Piracy Cultures, pirates feel a closer connection to content creators than they do through institutionalized markets. It is put forth that piracy could bring creators and consumers together rather than obscure or destroy obligations between them; they form a closer peer-to-peer relationship, so to speak.&lt;/p&gt;
&lt;p&gt;The research was conducted through separate, semi-structured interviews with two self-described pirates who currently maintain membership to piracy groups. Each was asked a common set of questions and asked additional follow up questions as topics arose out of their responses. Questions focused on three main areas, in order: the facts of their content consumption, details and practices concerning Piracy Cultures around content consumption, and their relationship with content creators.&lt;/p&gt;
&lt;p&gt;The interviewees had not previously shared their personal experiences with the authors. Hence, our interviews were the first time they had discussed explicitly (in any amount of detail) the culture and practices of themselves and their Piracy Cultures with respect to academic research.&lt;/p&gt;
&lt;p&gt;Case Studies&lt;/p&gt;
&lt;p&gt;Case Study #1 - Melinda&lt;/p&gt;
&lt;p&gt;Within the last few years, Melinda joined LUElinks, a private social networking website with an active message board, a member-created site-wide poll each day, and a healthy database of links to material (including software, games, video, photos, and audio), aggregated so members of the board can locate and reproduce the content, often without the permission of copyright holder(s). According to Melinda, these links typically come from cloud storage services, most prominently MegaUpload. Melinda is active on LUElinks to this day, reading and posting on the various message boards and downloading content located through the site’s links, especially movies and documentaries. Additionally, she has come to develop an interest in the masses of unlicensed images and pictures posted on the message boards. However, she rarely uploads content, citing a few cases in which she discovered some content had not already been shared. She proceeded to pirate the material through a torrent, upload to MegaUpload, and, finally, enter a new link in the site database.&lt;/p&gt;
&lt;p&gt;Though Melinda is a woman, on LUElinks she pretends (often implicitly) to be a man. This is a mechanism to protect herself from harassment, solicitations for pictures or “e-pussy,” unwanted attention through the creation of message board threads about her, and general denigration as an object with inferior intellect and worth. Melinda’s intentions are both to be free of harassment and to hold a reasonable amount of credibility through other posters’ respect--something she believes she would lose if she were to identify as female. Interestingly, Melinda claims not to be the only woman who is incognito. To this effect, she references the results of a site poll as well as a confirming message on a thread she started on the “anonymous” message board asking whether there are other women who pretend to be men on LUElinks.&lt;/p&gt;
&lt;p&gt;LUElinks is a portmanteau of the acronym LUE, which stands for “Life, the Universe, and Everything,” and the word “links,” referring to the links to media content for which the site exists. The expansion of LUE is a phrase popularized by Douglas Adams’ comic radio show cum book cummovie, The Hitchhiker*fs Guide to the Galaxy. According to Melinda, and long before her time, LUElinks originated from a “Life, the Universe, and Everything” themed message board on the GameFAQs website forum. According to LUElink mythology, this corner of the forum received much attention from moderators due to conduct that violated the terms of service agreement, including supposedly lewd posting and, of course, alleged piracy. At some point, this discussion reached a critical mass at which a user, known by the handle Llamaguy, broke off from GameFAQs and created an independent message board that was private to only original LUE users and whose purpose was to support freely the file-sharing that took place on the board. Thus LUElinks was born.&lt;/p&gt;
&lt;p&gt;Later, during Melinda’s tenure on LUElinks, the community began to express concern about the US government enforcing copyright by cracking down on media sharing sites similar to LUElinks. It is unclear what motivated the final decision, but eventually Llamaguy decided to migrate the site to a new domain. The site was moved to &lt;a href="http://endoftheinter.net/"&gt;endoftheinter.net&lt;/a&gt;, where it currently remains. However, members of the site still refer to themselves and each other as LUElinkers. Additionally, LUElinkers actually meet in the physical world. Melinda reports meet-ups are usually organized to play games on a local area network. She does not attend partly due to lack of interest and partly because she does not want to reveal her identity.&lt;/p&gt;
&lt;p&gt;Case-Study #2 - William&lt;/p&gt;
&lt;p&gt;Years ago, William attended a University summer program created to introduce high school students to college-level work in computer science and mathematics. He met a student there who gave him a different introduction--an introduction to piracy. Within days William had filled his 60 gigabyte hard drive to the brim with all sorts of media, and in the moment he was unable to download further, he realized something. He needed more space. In High School, a classmate introduced William to Japanese animation, or anime. His interest in pirating anime (as well as other media) blossomed as he entered college. In the school dormitories, William became a sizable file-sharing node on their internal network. He hosted and permitted the duplication of content through a peer-to-peer software called DC++, their version of which was tweaked by students in William’s dorm.&lt;/p&gt;
&lt;p&gt;Since he no longer lives in the dorms, William largely pirates through other means. He is a member of a campus anime club that has regular meetings, hosts events such as an annual anime conference, and holds weekly screenings. There are physical exchanges of anime through USB drives, as well as independent piracy through other avenues. Among those other avenues, William uses BakaBT, a torrent tracking site that provides access to an array of torrents of complete anime series, listed after meeting quality requirements. Access to BakaBT is not restricted to registered users, however, registration is open to the public and carries perks like better search and the ability to upload. Another torrent site William uses for general downloading purposes is Demonoid, a popular, public, “everything and the kitchen sink” type of tracker. Many anime series that are not licensed in the United States, and thus do not have any supported distribution method, are only available through piracy. Sometimes these series are dubbed over in other languages, such as English, but most often they are sub-titled by fans, aptly named “fansubs.”&lt;/p&gt;
&lt;p&gt;William is a computer gamer, and in some circumstances he pirates games he likes to play regularly or simply wants to try. He also pirates music, but since they are oldies from the 70s and 80s, he believes his piracy is justified because those songs’ copyright should have expired. William argues copyright law is supposed to support creativity, so making profit from old songs, to him, is unreasonable. On the other hand, William only pirates software that he believes to be unreasonably priced based on his expected use and what he considers a fair charge. William will pay for Windows 7 at the student price of \$40, but refuses to pay even the student price of the Adobe Creative Suite since the amount he will use the software is very minimal--only every once in a while--and not for commercial purposes. As a computer scientist, he intends to practice his software price ideals in his future work.&lt;/p&gt;
&lt;p&gt;Summary&lt;/p&gt;
&lt;p&gt;In the case of Melinda, piracy is largely a backdrop, an implicit assumption of a practicing community of content consumers. LUElinks is a place she goes for stand-alone entertainment and socializing as much as it is a reliable place to locate media she desires. As for William, the type of content is a clear, major factor as to how he performs piracy and why. For example, William’s anime piracy takes place between several different groups, including physical and virtual interactions--and in the case of a demonoid torrent, there is no human-to-human interaction at all other than the tacit transfer of the content itself. The value of these case studies lies in, the particularities and minutia of their cultures and behaviors, on which the following results will focus.&lt;/p&gt;
&lt;p&gt;Results&lt;/p&gt;
&lt;p&gt;Douglas Adams seemed to be the first port-of-call for a possible fandom subculture. We asked Melinda if people talk about Douglass Adams on LUElinks, to which she clarified, “I imagine on the original board they did, but no. It's gone, [...] it is its own entity [...] it doesn't really bring up images of Hitchhicker's Guide anymore.” And fair enough, perhaps one can read too much into a namesake. What then, were some of Melinda’s favored creators? Louis Theroux was the only name shortlisted in her mind, so we prodded on what discussions there were about Theroux on LUElinks? “There was a topic recently that just listed all of his stuff on Youtube so you can go watch it, [...] people giving their thoughts, now let's see how many pages this went to?” She clicks on her laptop, “Average, I mean, the big ones stay on the front board a lot longer.” Melinda did have something to say about the man behind the documentaries, but it was unenthusiastic and the other LUElinkers corroborated by their relative absence of opinion.&lt;/p&gt;
&lt;p&gt;The front board is the section where the site’s hottest discussion occurs, and Melinda even once herself started a topic about a creator that “got stickied” there for a time--Michael Cera. Yet the purpose was decidedly not fan-like. There was an official online poll of where Michael Cera should tour in support of his new film, “I just happened to notice in one of these metropolitan areas there was an ITT Tech. So I thought it would be kind of cool to make Michael Cera go on a national tour of ITT Techs.” LUElinks mobilized to skew the vote, “and we were leading in Boston and SF Bay area until the last few days.” Sending this celebrity actor astray highlights the LUElink mentality towards the origins of the media: unimportant, and even laughable. The artists here are at best shrugged off, and at worst humiliated. These figures for Melinda are not deserving of any figure other than zero when the topic of compensation arises. “I'm too poor,” she says, and that mindset along with creator irrelevance means she need not think any further about indemnity. The relationship for this Piracy Culture towards creators varies between uninterested and satirical.&lt;/p&gt;
&lt;p&gt;On the existence of Anime Club, William explains, “we’re a group of people because of our interest, not because of any prior planning, we sort of pirate the same things.” The scope of the pirated media for them is narrow and it’s only specific media that brings them together. William acts as a bridge between Anime Club and BakaBT “because I pirated anime a lot before I joined, I was an ideal candidate to continue pirating anime for them.” Creators play a crucial role in both of these Piracy Cultures; they are not substitutable. “It's something that, that we would, that the rest of America would never see unless it was [fan]subtitled.” When asked which anime William enjoyed, the titles and creators were forthcoming. “Studios that come to mind are Kyoto animation, and also Studio Shaft, which is one of my favorites, just because they produce really random off-the-wall stuff. The thing with Shaft is that, anything Shaft produces is never going to get licensed in the United States, it's too crazy. There’s no market for it. The problem is that when they do license stuff, its stuff that’s airable on TV, that's mass-marketable, not niche stuff that I would like.” For William much of his anime piracy is about acquiring content made by Studio Shaft by way of fansubs. Anime pirates’ enjoyment is indebted in large part to the labor and work of these creators, and William, at least, is well aware of this relation.&lt;/p&gt;
&lt;p&gt;The appreciation of this work and knowledge of the law is displayed through a sensitivity and discrimination in piracy. As a representative William clarifies they are “careful when picking the shows showing in Anime Club next season because, our policy is that we don't show something if it gets licensed.” Pirating anime shows yet to be licensed in the U.S. is okay, but if the anime becomes licensed then it is strictly hands off, “because of our industry friends that sponsor us, we don't want to step on their toes.” Anime Club is sponsored by some official distribution intermediaries who support their conventions and events. Cognizant of the formal process of copyright and licensing, the Anime Club stays within important self-imposed bounds because they recognize the mutual benefits of symbiosis with  distributors and producers. To illustrate the extent of respect of this process, fans will comply regardless of their own appreciation of a given work. William relates that some in Anime Club “that watch fansubs were unhappy [when some popular anime showings were cancelled] because [companies] have been so trigger-happy with their licensing.” However, the club agrees to cease showings nonetheless, when the shows are licensed.&lt;/p&gt;
&lt;p&gt;On downloading from BakaBT we ignorantly asked why those on the tracker did not “vote with their dollars?” The responding bombast is telling: “There's no way we can! We have no means. If I purchase, let's say a boxset of the Japanese version, it’s useless to me. I don't know enough Japanese for it to be useful to me, there's no way to vote with our money.” Then he displays some affection “I know Studio Shaft for a fact went through some really hard times last season and I would have loved to provide them with some kind of monetary compensation. I feel helpless sometimes. I want to help these guys out, and there's no way of doing it.” An understanding is evident that consumption must be a dialogue where consumers support the creator; else the creator may not be able to supply the consumer. This Piracy Culture may not have created any material profit in an institutional sense, but it has created a connection between creator and consumer, an empathy which is a perfect stage for potential capitalization. “I really wish there was some way I could show how much I like, I adore their material.” This is a feeling made possible through Piracy.&lt;/p&gt;
&lt;p&gt;No direct correlation is clear between the act of piracy and the relation between creator and consumer. The case studies presented here are disparate stories of both apathy and great care from the consumer to the creator. Rejecting the hypothesis, the evidence suggests Piracy Cultures do not necessarily promote a common a desire or motivation to relate to their artists. But Piracy Cultures can offer a closer connection as a feature, such as in William’s case. In Melinda’s case the closeness, not to creator, but to other consumers is a major aspect of LUElinks. In the research Piracy Cultures produce not merely free content distribution services, but importantly imbued goods with significant additional value.&lt;/p&gt;
&lt;p&gt;LUElinks is a showcase of a mastery in a user-generated user experience. With only simple message board software, a thriving community shines as the main attraction. In a wondrous state, on the topic of the image-posting threads on LUElinks, Melinda told us, “I've gotten actually drawn in to just looking at them for hours at a time,  I don't know... they're just nice pictures to look at.” Her difficulty in explanation is around a hypnotic phenomenon, akin to that of the enigmatic and legendary imageboard, &lt;a href="http://4chan.org/"&gt;4chan.org&lt;/a&gt;. The imageboards of photoshopped humour are a type of “sharing” themselves though more focused around discussion of topics than exchange of images, and serve more as a vessel for social networking. Like aPlayboy reader, but in earnest, she admits “honestly, it’s just interesting, I enjoy reading the topics. I mean look at my tabs (she points to a browser tab bar of closely nestled LUElinks icons), it’s all fucking LUElinks.” On other boards users answer questions, share fitness tips, discuss commonplace grievances, and give advice on life. The community at LUElinks is its killer feature. LUElinks is entertainment and a secure, vibrant community in Melinda’s life, infinitely more than a dumb pipe for media.&lt;/p&gt;
&lt;p&gt;BakaBT isn’t just a torrent tracker, it’s purveyor of luxury and hard-to-find items. They offer only entire seasons of anime, a collector’s pleasure, and crucially (for non-Japanese speakers) fansubbed anime. Carrying only complete collections of anime highlights the serious and dedicated nature of BakaBT ideology. Yet, the most killer feature for BakaBT is the access to fansubbed anime. BakaBT gives the users what they want, William relays that institutional distributors “don't expect people to want subtitles they expect them to want dubs.” Anime lovers and researchers attest that fansubs have major upshots (Mäntylä 2010). The first and most obvious is shared with dubbing, that you can watch a Japanese-only anime even if you are an English-only consumer. The second is a trophy of commons-based peer production. Fans are collaboratively translating their favorite anime to make subtitles that are, as William opines, “in a lot of ways  better than professional subtitling. Fansbbers are actually superior because they actually care about what they're translating.” But it’s not just William that holds fansubbing in high regard, in an ironic twist of fate, the U.S. distributors do too. William shared this anecdote with us:&lt;/p&gt;
&lt;p&gt;just recently there was a show on TV called “Greatest Otaku” or something like that ... basically they took a tour of Funimation studios where they do the dubbing, and they had a shot, the shot that eveyone’s talking about, a shot into the ‘dub’ room. They had an anime on screen that they were dubbing. The strange thing about the anime on screen that they were dubbing was, the subtitles. Looking at the anime it looks very normal, except the subtitles looked nice, you thought ‘the subtitles look nice, what's going on here?’ [...] It's a fan sub […] they've been crusading against fansubbers, and they're just hypocrites!&lt;/p&gt;
&lt;p&gt;A favored scandal by the anime community, the debacle serves as evidence for the legitimacy and quality of fansubbing (“Downloaded … ” 2011). High speed downloads of almost professionally archived anime, with better than professional subtitling, of locally unavailble content are the killer features that bring William BakaBT. The price is free as well, but to hold that as the main feature would miss the point.&lt;/p&gt;
&lt;p&gt;Major and direct market indicators came from our research, evident in the piracy of games. The first and most powerful was a trope that both Melinda and William gave with verbatim duplicate phrasing: “Try before you buy.” The concept is self explanatory but meaningful. On LUElinks there is a practice: “it's a pretty common sentiment that if you really really like something, you'll pay money for it. You'll even see people in the link page if they really liked it, they'll come back and post ‘I'm buying this.’ [...] but generally, the rule of thumb is you try it out first.” On LUElinks, buying is the greatest flattery for a game on your hard disk, “you already have it obviously, that’s how you show support for the [developer].” Post-play purchase was also a sentiment echoed by William who says he likes to, “follow the try before you buy mentality, I’ll pirate a game, try it out, and if I like it enough I'll buy it from the company.” For whatever it is worth gamers seem to be united in this modus operandi, a signal the market could well heed.&lt;/p&gt;
&lt;p&gt;A second related signal comes from seeing Piracy as a pricing problem in the same way as the Social Sciences Research Council does (2011). When a gamer sees a price as reasonable, sometimes try before you buy is too cumbersome. William chimes, “if it's on Steam and it's cheap, I won't even pirate it in the first place. There are so many games in my steam library that I haven't even played because they were only five dollars.” Steam is already a well heralded market solution but the admission coming from an active pirate is one not to be ignored. Although it may be the case that free cannot be beaten, the monetary price is not the only cost in acquiring a game; there are levers that the institutional markets could pull to draw more pirates to their services.&lt;/p&gt;
&lt;p&gt;Conclusion&lt;/p&gt;
&lt;p&gt;Piracy Cultures allow their users to be the lever-operators and cog-designers of their own engines. In being their own engineers they have developedkiller features. A glimpse of this is evident in the research. From Melinda it is seen that any market solution offered to her must have strong social networking components among the consumer base: the country-club style community. From William it is seen that to be drawn to any market solution it must provide him access to the niche long tail content he desires (Anderson 2004). Also he wants assurance the distributors and subtitlers care as much about the content as himself. Some of the killer features here have clear conversion potential for creator compensation. William regrets that he is powerless to give his beloved content creators any innovation stipend. Realistically though, not all Piracy Culture’s killer features could be directly employed in the task of reimbursing the creators. Whether or not these features could be cashed in on indirectly is another question that will require creativity to answer in the positive. However, in all cases, the killer features do signal auxiliary desires fulfilled by their distribution mechanism. You need not be a behavioral economist to calculate difference in appeal between institutionalized markets and Piracy Cultures; when faced with the choice, no one says “give me the more expensive, less convenient, less valuable one.”&lt;/p&gt;
&lt;p&gt;The research shows some Piracy Cultures have evolved to provide built-in supplementary and value-addition services to suit their users’ desires. Institutional Markets may view these services as necessary additions in order to compete. Of course when viewing killer features as market demands, they aren't accompanied with a clear solution on compensating creators; there is not much negotiation forthcoming on that front from pirates. In order to know whether the future holds these Piracy Cultures as perpetual counter-cultures or not will be reliant on whether institutions shift to accommodate the above-outlined consumer demands, or effectively declare an impasse. Therefore until either one is solved, a conundrum is problematized: is society more content with the unsettled issue of creator compensation or unmet consumer demands?&lt;/p&gt;
&lt;p&gt;Bibliography&lt;/p&gt;
&lt;p&gt;Benkler, Y. (2006). The wealth of networks: how social production transforms markets and freedom. New Haven, Conn.: Yale University Press.&lt;/p&gt;
&lt;p&gt;Bridy, A. (2009). Why Pirates (Still) Won't Behave: Regulating P2P in the Decade after Napster (April 23, 2009). Rutgers Law Journal, 40(3), 565 - 612.&lt;/p&gt;
&lt;p&gt;Brown, J. S., &amp;amp; Paul, D. (2000). The Social Life of Information. Boston: Harvard Business School Press.&lt;/p&gt;
&lt;p&gt;Chris, A. (2004, October). The Long Tail. Wired. Retrieved March 30, 2011, from &lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.wired.com%2Fwired%2Farchive%2F12.10%2Ftail.html&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNF5DeZfV2br-SdJRwuzIOlfLeFSog"&gt;http&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.wired.com%2Fwired%2Farchive%2F12.10%2Ftail.html&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNF5DeZfV2br-SdJRwuzIOlfLeFSog"&gt;://&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.wired.com%2Fwired%2Farchive%2F12.10%2Ftail.html&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNF5DeZfV2br-SdJRwuzIOlfLeFSog"&gt;www&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.wired.com%2Fwired%2Farchive%2F12.10%2Ftail.html&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNF5DeZfV2br-SdJRwuzIOlfLeFSog"&gt;.&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.wired.com%2Fwired%2Farchive%2F12.10%2Ftail.html&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNF5DeZfV2br-SdJRwuzIOlfLeFSog"&gt;wired&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.wired.com%2Fwired%2Farchive%2F12.10%2Ftail.html&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNF5DeZfV2br-SdJRwuzIOlfLeFSog"&gt;.&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.wired.com%2Fwired%2Farchive%2F12.10%2Ftail.html&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNF5DeZfV2br-SdJRwuzIOlfLeFSog"&gt;com&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.wired.com%2Fwired%2Farchive%2F12.10%2Ftail.html&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNF5DeZfV2br-SdJRwuzIOlfLeFSog"&gt;/&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.wired.com%2Fwired%2Farchive%2F12.10%2Ftail.html&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNF5DeZfV2br-SdJRwuzIOlfLeFSog"&gt;wired&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.wired.com%2Fwired%2Farchive%2F12.10%2Ftail.html&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNF5DeZfV2br-SdJRwuzIOlfLeFSog"&gt;/&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.wired.com%2Fwired%2Farchive%2F12.10%2Ftail.html&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNF5DeZfV2br-SdJRwuzIOlfLeFSog"&gt;archive&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.wired.com%2Fwired%2Farchive%2F12.10%2Ftail.html&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNF5DeZfV2br-SdJRwuzIOlfLeFSog"&gt;/12.10/&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.wired.com%2Fwired%2Farchive%2F12.10%2Ftail.html&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNF5DeZfV2br-SdJRwuzIOlfLeFSog"&gt;tail&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.wired.com%2Fwired%2Farchive%2F12.10%2Ftail.html&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNF5DeZfV2br-SdJRwuzIOlfLeFSog"&gt;.&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.wired.com%2Fwired%2Farchive%2F12.10%2Ftail.html&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNF5DeZfV2br-SdJRwuzIOlfLeFSog"&gt;html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;(2011, March 22). Downloaded Sora no Otoshimono Copy Shown at Funimation Studio. Anime News Network. Retrieved March 27, 2011, from &lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.animenewsnetwork.com%2Fnews%2F2011-03-22%2Fdownloaded-sora-no-otoshimono-copy-shown-at-funimation-studio&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGzvS1REHIG_LhGtvW3WJTrKvRXUA"&gt;http&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.animenewsnetwork.com%2Fnews%2F2011-03-22%2Fdownloaded-sora-no-otoshimono-copy-shown-at-funimation-studio&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGzvS1REHIG_LhGtvW3WJTrKvRXUA"&gt;://&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.animenewsnetwork.com%2Fnews%2F2011-03-22%2Fdownloaded-sora-no-otoshimono-copy-shown-at-funimation-studio&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGzvS1REHIG_LhGtvW3WJTrKvRXUA"&gt;www&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.animenewsnetwork.com%2Fnews%2F2011-03-22%2Fdownloaded-sora-no-otoshimono-copy-shown-at-funimation-studio&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGzvS1REHIG_LhGtvW3WJTrKvRXUA"&gt;.&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.animenewsnetwork.com%2Fnews%2F2011-03-22%2Fdownloaded-sora-no-otoshimono-copy-shown-at-funimation-studio&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGzvS1REHIG_LhGtvW3WJTrKvRXUA"&gt;animenewsnetwork&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.animenewsnetwork.com%2Fnews%2F2011-03-22%2Fdownloaded-sora-no-otoshimono-copy-shown-at-funimation-studio&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGzvS1REHIG_LhGtvW3WJTrKvRXUA"&gt;.&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.animenewsnetwork.com%2Fnews%2F2011-03-22%2Fdownloaded-sora-no-otoshimono-copy-shown-at-funimation-studio&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGzvS1REHIG_LhGtvW3WJTrKvRXUA"&gt;com&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.animenewsnetwork.com%2Fnews%2F2011-03-22%2Fdownloaded-sora-no-otoshimono-copy-shown-at-funimation-studio&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGzvS1REHIG_LhGtvW3WJTrKvRXUA"&gt;/&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.animenewsnetwork.com%2Fnews%2F2011-03-22%2Fdownloaded-sora-no-otoshimono-copy-shown-at-funimation-studio&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGzvS1REHIG_LhGtvW3WJTrKvRXUA"&gt;news&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.animenewsnetwork.com%2Fnews%2F2011-03-22%2Fdownloaded-sora-no-otoshimono-copy-shown-at-funimation-studio&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGzvS1REHIG_LhGtvW3WJTrKvRXUA"&gt;/2011-03-22/&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.animenewsnetwork.com%2Fnews%2F2011-03-22%2Fdownloaded-sora-no-otoshimono-copy-shown-at-funimation-studio&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGzvS1REHIG_LhGtvW3WJTrKvRXUA"&gt;downloaded&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.animenewsnetwork.com%2Fnews%2F2011-03-22%2Fdownloaded-sora-no-otoshimono-copy-shown-at-funimation-studio&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGzvS1REHIG_LhGtvW3WJTrKvRXUA"&gt;-&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.animenewsnetwork.com%2Fnews%2F2011-03-22%2Fdownloaded-sora-no-otoshimono-copy-shown-at-funimation-studio&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGzvS1REHIG_LhGtvW3WJTrKvRXUA"&gt;sora&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.animenewsnetwork.com%2Fnews%2F2011-03-22%2Fdownloaded-sora-no-otoshimono-copy-shown-at-funimation-studio&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGzvS1REHIG_LhGtvW3WJTrKvRXUA"&gt;-&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.animenewsnetwork.com%2Fnews%2F2011-03-22%2Fdownloaded-sora-no-otoshimono-copy-shown-at-funimation-studio&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGzvS1REHIG_LhGtvW3WJTrKvRXUA"&gt;no&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.animenewsnetwork.com%2Fnews%2F2011-03-22%2Fdownloaded-sora-no-otoshimono-copy-shown-at-funimation-studio&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGzvS1REHIG_LhGtvW3WJTrKvRXUA"&gt;-&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.animenewsnetwork.com%2Fnews%2F2011-03-22%2Fdownloaded-sora-no-otoshimono-copy-shown-at-funimation-studio&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGzvS1REHIG_LhGtvW3WJTrKvRXUA"&gt;otoshimono&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.animenewsnetwork.com%2Fnews%2F2011-03-22%2Fdownloaded-sora-no-otoshimono-copy-shown-at-funimation-studio&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGzvS1REHIG_LhGtvW3WJTrKvRXUA"&gt;-&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.animenewsnetwork.com%2Fnews%2F2011-03-22%2Fdownloaded-sora-no-otoshimono-copy-shown-at-funimation-studio&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGzvS1REHIG_LhGtvW3WJTrKvRXUA"&gt;copy&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.animenewsnetwork.com%2Fnews%2F2011-03-22%2Fdownloaded-sora-no-otoshimono-copy-shown-at-funimation-studio&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGzvS1REHIG_LhGtvW3WJTrKvRXUA"&gt;-&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.animenewsnetwork.com%2Fnews%2F2011-03-22%2Fdownloaded-sora-no-otoshimono-copy-shown-at-funimation-studio&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGzvS1REHIG_LhGtvW3WJTrKvRXUA"&gt;shown&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.animenewsnetwork.com%2Fnews%2F2011-03-22%2Fdownloaded-sora-no-otoshimono-copy-shown-at-funimation-studio&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGzvS1REHIG_LhGtvW3WJTrKvRXUA"&gt;-&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.animenewsnetwork.com%2Fnews%2F2011-03-22%2Fdownloaded-sora-no-otoshimono-copy-shown-at-funimation-studio&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGzvS1REHIG_LhGtvW3WJTrKvRXUA"&gt;at&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.animenewsnetwork.com%2Fnews%2F2011-03-22%2Fdownloaded-sora-no-otoshimono-copy-shown-at-funimation-studio&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGzvS1REHIG_LhGtvW3WJTrKvRXUA"&gt;-&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.animenewsnetwork.com%2Fnews%2F2011-03-22%2Fdownloaded-sora-no-otoshimono-copy-shown-at-funimation-studio&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGzvS1REHIG_LhGtvW3WJTrKvRXUA"&gt;funimation&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.animenewsnetwork.com%2Fnews%2F2011-03-22%2Fdownloaded-sora-no-otoshimono-copy-shown-at-funimation-studio&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGzvS1REHIG_LhGtvW3WJTrKvRXUA"&gt;-&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.animenewsnetwork.com%2Fnews%2F2011-03-22%2Fdownloaded-sora-no-otoshimono-copy-shown-at-funimation-studio&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGzvS1REHIG_LhGtvW3WJTrKvRXUA"&gt;studio&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Gambotto-Burke, Alexander. (2007, August). Will online distribution overtake the boxed game? &lt;a href="http://guardian.co.uk/"&gt;Guardian.co.uk&lt;/a&gt;. Retrieved March 31, 2011, from &lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.guardian.co.uk%2Ftechnology%2F2007%2Faug%2F30%2Fguardianweeklytechnologysection.it&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNFrPdV9cOZTcJoFS9CawbCFgBh68Q"&gt;http&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.guardian.co.uk%2Ftechnology%2F2007%2Faug%2F30%2Fguardianweeklytechnologysection.it&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNFrPdV9cOZTcJoFS9CawbCFgBh68Q"&gt;://&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.guardian.co.uk%2Ftechnology%2F2007%2Faug%2F30%2Fguardianweeklytechnologysection.it&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNFrPdV9cOZTcJoFS9CawbCFgBh68Q"&gt;www&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.guardian.co.uk%2Ftechnology%2F2007%2Faug%2F30%2Fguardianweeklytechnologysection.it&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNFrPdV9cOZTcJoFS9CawbCFgBh68Q"&gt;.&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.guardian.co.uk%2Ftechnology%2F2007%2Faug%2F30%2Fguardianweeklytechnologysection.it&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNFrPdV9cOZTcJoFS9CawbCFgBh68Q"&gt;guardian&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.guardian.co.uk%2Ftechnology%2F2007%2Faug%2F30%2Fguardianweeklytechnologysection.it&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNFrPdV9cOZTcJoFS9CawbCFgBh68Q"&gt;.&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.guardian.co.uk%2Ftechnology%2F2007%2Faug%2F30%2Fguardianweeklytechnologysection.it&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNFrPdV9cOZTcJoFS9CawbCFgBh68Q"&gt;co&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.guardian.co.uk%2Ftechnology%2F2007%2Faug%2F30%2Fguardianweeklytechnologysection.it&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNFrPdV9cOZTcJoFS9CawbCFgBh68Q"&gt;.&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.guardian.co.uk%2Ftechnology%2F2007%2Faug%2F30%2Fguardianweeklytechnologysection.it&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNFrPdV9cOZTcJoFS9CawbCFgBh68Q"&gt;uk&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.guardian.co.uk%2Ftechnology%2F2007%2Faug%2F30%2Fguardianweeklytechnologysection.it&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNFrPdV9cOZTcJoFS9CawbCFgBh68Q"&gt;/&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.guardian.co.uk%2Ftechnology%2F2007%2Faug%2F30%2Fguardianweeklytechnologysection.it&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNFrPdV9cOZTcJoFS9CawbCFgBh68Q"&gt;technology&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.guardian.co.uk%2Ftechnology%2F2007%2Faug%2F30%2Fguardianweeklytechnologysection.it&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNFrPdV9cOZTcJoFS9CawbCFgBh68Q"&gt;/2007/&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.guardian.co.uk%2Ftechnology%2F2007%2Faug%2F30%2Fguardianweeklytechnologysection.it&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNFrPdV9cOZTcJoFS9CawbCFgBh68Q"&gt;aug&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.guardian.co.uk%2Ftechnology%2F2007%2Faug%2F30%2Fguardianweeklytechnologysection.it&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNFrPdV9cOZTcJoFS9CawbCFgBh68Q"&gt;/30/&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.guardian.co.uk%2Ftechnology%2F2007%2Faug%2F30%2Fguardianweeklytechnologysection.it&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNFrPdV9cOZTcJoFS9CawbCFgBh68Q"&gt;guardianweeklytechnologysection&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.guardian.co.uk%2Ftechnology%2F2007%2Faug%2F30%2Fguardianweeklytechnologysection.it&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNFrPdV9cOZTcJoFS9CawbCFgBh68Q"&gt;.&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fwww.guardian.co.uk%2Ftechnology%2F2007%2Faug%2F30%2Fguardianweeklytechnologysection.it&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNFrPdV9cOZTcJoFS9CawbCFgBh68Q"&gt;it&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Lessig, L. (2005). Free culture: the nature and future of creativity. New York: Penguin Books.&lt;/p&gt;
&lt;p&gt;Mäntylä, Teemu.  Piracy or Productivity: Unlawful Practices in Anime Fansubbing. MA/MS Thesis. Aalto University School of Science and Technology, 2010. Print.&lt;/p&gt;
&lt;p&gt;Social Science Research Council. (2011). Media Piracy in Emerging Economies. New York, NY: Author&lt;/p&gt;
&lt;p&gt;Wingfield, N. (2007, March 9). Music's New Gatekeeper. The Wall Street Journal - WSJ.com. Retrieved March 28, 2011, from&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fonline.wsj.com%2Fpublic%2Farticle%2FSB117340340327331757-aqC21fdikWTyQ_8Wq0nkNv4l5j4_20070407.html%3Fmod%3Dtff_main_tff_top&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGVI6bARgn8OA2GAk3_vpeF8biYmA"&gt;http&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fonline.wsj.com%2Fpublic%2Farticle%2FSB117340340327331757-aqC21fdikWTyQ_8Wq0nkNv4l5j4_20070407.html%3Fmod%3Dtff_main_tff_top&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGVI6bARgn8OA2GAk3_vpeF8biYmA"&gt;://&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fonline.wsj.com%2Fpublic%2Farticle%2FSB117340340327331757-aqC21fdikWTyQ_8Wq0nkNv4l5j4_20070407.html%3Fmod%3Dtff_main_tff_top&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGVI6bARgn8OA2GAk3_vpeF8biYmA"&gt;online&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fonline.wsj.com%2Fpublic%2Farticle%2FSB117340340327331757-aqC21fdikWTyQ_8Wq0nkNv4l5j4_20070407.html%3Fmod%3Dtff_main_tff_top&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGVI6bARgn8OA2GAk3_vpeF8biYmA"&gt;.&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fonline.wsj.com%2Fpublic%2Farticle%2FSB117340340327331757-aqC21fdikWTyQ_8Wq0nkNv4l5j4_20070407.html%3Fmod%3Dtff_main_tff_top&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGVI6bARgn8OA2GAk3_vpeF8biYmA"&gt;wsj&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fonline.wsj.com%2Fpublic%2Farticle%2FSB117340340327331757-aqC21fdikWTyQ_8Wq0nkNv4l5j4_20070407.html%3Fmod%3Dtff_main_tff_top&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGVI6bARgn8OA2GAk3_vpeF8biYmA"&gt;.&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fonline.wsj.com%2Fpublic%2Farticle%2FSB117340340327331757-aqC21fdikWTyQ_8Wq0nkNv4l5j4_20070407.html%3Fmod%3Dtff_main_tff_top&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGVI6bARgn8OA2GAk3_vpeF8biYmA"&gt;com&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fonline.wsj.com%2Fpublic%2Farticle%2FSB117340340327331757-aqC21fdikWTyQ_8Wq0nkNv4l5j4_20070407.html%3Fmod%3Dtff_main_tff_top&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGVI6bARgn8OA2GAk3_vpeF8biYmA"&gt;/&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fonline.wsj.com%2Fpublic%2Farticle%2FSB117340340327331757-aqC21fdikWTyQ_8Wq0nkNv4l5j4_20070407.html%3Fmod%3Dtff_main_tff_top&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGVI6bARgn8OA2GAk3_vpeF8biYmA"&gt;public&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fonline.wsj.com%2Fpublic%2Farticle%2FSB117340340327331757-aqC21fdikWTyQ_8Wq0nkNv4l5j4_20070407.html%3Fmod%3Dtff_main_tff_top&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGVI6bARgn8OA2GAk3_vpeF8biYmA"&gt;/&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fonline.wsj.com%2Fpublic%2Farticle%2FSB117340340327331757-aqC21fdikWTyQ_8Wq0nkNv4l5j4_20070407.html%3Fmod%3Dtff_main_tff_top&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGVI6bARgn8OA2GAk3_vpeF8biYmA"&gt;article&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fonline.wsj.com%2Fpublic%2Farticle%2FSB117340340327331757-aqC21fdikWTyQ_8Wq0nkNv4l5j4_20070407.html%3Fmod%3Dtff_main_tff_top&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGVI6bARgn8OA2GAk3_vpeF8biYmA"&gt;/&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fonline.wsj.com%2Fpublic%2Farticle%2FSB117340340327331757-aqC21fdikWTyQ_8Wq0nkNv4l5j4_20070407.html%3Fmod%3Dtff_main_tff_top&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGVI6bARgn8OA2GAk3_vpeF8biYmA"&gt;SB&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fonline.wsj.com%2Fpublic%2Farticle%2FSB117340340327331757-aqC21fdikWTyQ_8Wq0nkNv4l5j4_20070407.html%3Fmod%3Dtff_main_tff_top&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGVI6bARgn8OA2GAk3_vpeF8biYmA"&gt;117340340327331757-&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fonline.wsj.com%2Fpublic%2Farticle%2FSB117340340327331757-aqC21fdikWTyQ_8Wq0nkNv4l5j4_20070407.html%3Fmod%3Dtff_main_tff_top&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGVI6bARgn8OA2GAk3_vpeF8biYmA"&gt;aqC&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fonline.wsj.com%2Fpublic%2Farticle%2FSB117340340327331757-aqC21fdikWTyQ_8Wq0nkNv4l5j4_20070407.html%3Fmod%3Dtff_main_tff_top&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGVI6bARgn8OA2GAk3_vpeF8biYmA"&gt;21&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fonline.wsj.com%2Fpublic%2Farticle%2FSB117340340327331757-aqC21fdikWTyQ_8Wq0nkNv4l5j4_20070407.html%3Fmod%3Dtff_main_tff_top&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGVI6bARgn8OA2GAk3_vpeF8biYmA"&gt;fdikWTyQ&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fonline.wsj.com%2Fpublic%2Farticle%2FSB117340340327331757-aqC21fdikWTyQ_8Wq0nkNv4l5j4_20070407.html%3Fmod%3Dtff_main_tff_top&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGVI6bARgn8OA2GAk3_vpeF8biYmA"&gt;_8&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fonline.wsj.com%2Fpublic%2Farticle%2FSB117340340327331757-aqC21fdikWTyQ_8Wq0nkNv4l5j4_20070407.html%3Fmod%3Dtff_main_tff_top&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGVI6bARgn8OA2GAk3_vpeF8biYmA"&gt;Wq&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fonline.wsj.com%2Fpublic%2Farticle%2FSB117340340327331757-aqC21fdikWTyQ_8Wq0nkNv4l5j4_20070407.html%3Fmod%3Dtff_main_tff_top&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGVI6bARgn8OA2GAk3_vpeF8biYmA"&gt;0&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fonline.wsj.com%2Fpublic%2Farticle%2FSB117340340327331757-aqC21fdikWTyQ_8Wq0nkNv4l5j4_20070407.html%3Fmod%3Dtff_main_tff_top&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGVI6bARgn8OA2GAk3_vpeF8biYmA"&gt;nkNv&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fonline.wsj.com%2Fpublic%2Farticle%2FSB117340340327331757-aqC21fdikWTyQ_8Wq0nkNv4l5j4_20070407.html%3Fmod%3Dtff_main_tff_top&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGVI6bARgn8OA2GAk3_vpeF8biYmA"&gt;4&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fonline.wsj.com%2Fpublic%2Farticle%2FSB117340340327331757-aqC21fdikWTyQ_8Wq0nkNv4l5j4_20070407.html%3Fmod%3Dtff_main_tff_top&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGVI6bARgn8OA2GAk3_vpeF8biYmA"&gt;l&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fonline.wsj.com%2Fpublic%2Farticle%2FSB117340340327331757-aqC21fdikWTyQ_8Wq0nkNv4l5j4_20070407.html%3Fmod%3Dtff_main_tff_top&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGVI6bARgn8OA2GAk3_vpeF8biYmA"&gt;5&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fonline.wsj.com%2Fpublic%2Farticle%2FSB117340340327331757-aqC21fdikWTyQ_8Wq0nkNv4l5j4_20070407.html%3Fmod%3Dtff_main_tff_top&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGVI6bARgn8OA2GAk3_vpeF8biYmA"&gt;j&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fonline.wsj.com%2Fpublic%2Farticle%2FSB117340340327331757-aqC21fdikWTyQ_8Wq0nkNv4l5j4_20070407.html%3Fmod%3Dtff_main_tff_top&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGVI6bARgn8OA2GAk3_vpeF8biYmA"&gt;4_20070407.&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fonline.wsj.com%2Fpublic%2Farticle%2FSB117340340327331757-aqC21fdikWTyQ_8Wq0nkNv4l5j4_20070407.html%3Fmod%3Dtff_main_tff_top&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGVI6bARgn8OA2GAk3_vpeF8biYmA"&gt;html&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fonline.wsj.com%2Fpublic%2Farticle%2FSB117340340327331757-aqC21fdikWTyQ_8Wq0nkNv4l5j4_20070407.html%3Fmod%3Dtff_main_tff_top&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGVI6bARgn8OA2GAk3_vpeF8biYmA"&gt;?&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fonline.wsj.com%2Fpublic%2Farticle%2FSB117340340327331757-aqC21fdikWTyQ_8Wq0nkNv4l5j4_20070407.html%3Fmod%3Dtff_main_tff_top&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGVI6bARgn8OA2GAk3_vpeF8biYmA"&gt;mod&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fonline.wsj.com%2Fpublic%2Farticle%2FSB117340340327331757-aqC21fdikWTyQ_8Wq0nkNv4l5j4_20070407.html%3Fmod%3Dtff_main_tff_top&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGVI6bARgn8OA2GAk3_vpeF8biYmA"&gt;=&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fonline.wsj.com%2Fpublic%2Farticle%2FSB117340340327331757-aqC21fdikWTyQ_8Wq0nkNv4l5j4_20070407.html%3Fmod%3Dtff_main_tff_top&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGVI6bARgn8OA2GAk3_vpeF8biYmA"&gt;tff&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fonline.wsj.com%2Fpublic%2Farticle%2FSB117340340327331757-aqC21fdikWTyQ_8Wq0nkNv4l5j4_20070407.html%3Fmod%3Dtff_main_tff_top&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGVI6bARgn8OA2GAk3_vpeF8biYmA"&gt;_&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fonline.wsj.com%2Fpublic%2Farticle%2FSB117340340327331757-aqC21fdikWTyQ_8Wq0nkNv4l5j4_20070407.html%3Fmod%3Dtff_main_tff_top&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGVI6bARgn8OA2GAk3_vpeF8biYmA"&gt;main&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fonline.wsj.com%2Fpublic%2Farticle%2FSB117340340327331757-aqC21fdikWTyQ_8Wq0nkNv4l5j4_20070407.html%3Fmod%3Dtff_main_tff_top&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGVI6bARgn8OA2GAk3_vpeF8biYmA"&gt;_&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fonline.wsj.com%2Fpublic%2Farticle%2FSB117340340327331757-aqC21fdikWTyQ_8Wq0nkNv4l5j4_20070407.html%3Fmod%3Dtff_main_tff_top&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGVI6bARgn8OA2GAk3_vpeF8biYmA"&gt;tff&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fonline.wsj.com%2Fpublic%2Farticle%2FSB117340340327331757-aqC21fdikWTyQ_8Wq0nkNv4l5j4_20070407.html%3Fmod%3Dtff_main_tff_top&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGVI6bARgn8OA2GAk3_vpeF8biYmA"&gt;_&lt;/a&gt;&lt;a href="http://www.google.com/url?q=http%3A%2F%2Fonline.wsj.com%2Fpublic%2Farticle%2FSB117340340327331757-aqC21fdikWTyQ_8Wq0nkNv4l5j4_20070407.html%3Fmod%3Dtff_main_tff_top&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AFQjCNGVI6bARgn8OA2GAk3_vpeF8biYmA"&gt;top&lt;/a&gt;&lt;/p&gt;</content><category term="hacking"></category></entry></feed>